{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:12:07.808665Z",
     "iopub.status.busy": "2025-05-23T05:12:07.808206Z",
     "iopub.status.idle": "2025-05-23T05:12:12.984582Z",
     "shell.execute_reply": "2025-05-23T05:12:12.983251Z",
     "shell.execute_reply.started": "2025-05-23T05:12:07.808621Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:12:12.986440Z",
     "iopub.status.busy": "2025-05-23T05:12:12.986059Z",
     "iopub.status.idle": "2025-05-23T05:12:17.478973Z",
     "shell.execute_reply": "2025-05-23T05:12:17.477790Z",
     "shell.execute_reply.started": "2025-05-23T05:12:12.986400Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:18:25.432575Z",
     "iopub.status.busy": "2025-05-22T20:18:25.432287Z",
     "iopub.status.idle": "2025-05-22T20:18:25.466083Z",
     "shell.execute_reply": "2025-05-22T20:18:25.465249Z",
     "shell.execute_reply.started": "2025-05-22T20:18:25.432537Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-23T05:12:52.471732Z",
     "iopub.status.busy": "2025-05-23T05:12:52.471383Z",
     "iopub.status.idle": "2025-05-23T05:13:20.262458Z",
     "shell.execute_reply": "2025-05-23T05:13:20.261115Z",
     "shell.execute_reply.started": "2025-05-23T05:12:52.471706Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import model_eval\n",
    "from model_eval import *\n",
    "from spider_utils_py import load_csv_database\n",
    "#from process_transformations import split_code_into_blocks, generate_nl_description\n",
    "\n",
    "# required for spider2 csvs\n",
    "CSV_DBS_BASE_PATH = \"/kaggle/input/spider-dbs-csv\"\n",
    "os.environ[\"DB_CSVS_BASE_PATH\"] = CSV_DBS_BASE_PATH\n",
    "\n",
    "# enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Suppress FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# set pandas to display in 2 decimal places\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)  \n",
    "\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:20.264392Z",
     "iopub.status.busy": "2025-05-23T05:13:20.263791Z",
     "iopub.status.idle": "2025-05-23T05:13:20.268920Z",
     "shell.execute_reply": "2025-05-23T05:13:20.267787Z",
     "shell.execute_reply.started": "2025-05-23T05:13:20.264360Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_COL = 'model'\n",
    "ORIGINAL_NOTEBOOK_COL = 'nb_source'\n",
    "ACTUAL_CODE_COL = 'code'\n",
    "INPUT_DATA_COL = 'inputs'\n",
    "OUTPUT_DATA_COL = 'outputs'\n",
    "TRANSFORMATION_DESCRIPTION_COL = 'intent'\n",
    "GENERATED_CODE_COL = 'generated_intent_code'\n",
    "GENERATED_INPUT_DATA_COL = 'gen_inputs'\n",
    "GENERATED_OUTPUT_DATA_COL = 'generated_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:12:20.058247Z",
     "iopub.status.busy": "2025-05-23T05:12:20.057911Z",
     "iopub.status.idle": "2025-05-23T05:12:20.062370Z",
     "shell.execute_reply": "2025-05-23T05:12:20.061234Z",
     "shell.execute_reply.started": "2025-05-23T05:12:20.058220Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_GEN_INTENTS = None # limit how many to generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m=n=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:20.273061Z",
     "iopub.status.busy": "2025-05-23T05:13:20.272683Z",
     "iopub.status.idle": "2025-05-23T05:13:29.521496Z",
     "shell.execute_reply": "2025-05-23T05:13:29.520476Z",
     "shell.execute_reply.started": "2025-05-23T05:13:20.273022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Arcade existing\n",
    "\n",
    "# Gemini completions\n",
    "arcade_existing_gemini_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_existing_transformed_generated_google_ALL_notebooks_m_1_n_1.csv')\n",
    "arcade_existing_gemini_df['model'] = 'Gemini'\n",
    "arcade_existing_gemini_df['benchmark'] = 'ARCADE existing'\n",
    "\n",
    "# Claude completions\n",
    "arcade_existing_claude_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_existing_transformed_generated_anthropic_ALL_notebooks_m_1_n_1.csv')\n",
    "arcade_existing_claude_df['model'] = 'Claude'\n",
    "arcade_existing_claude_df['benchmark'] = 'ARCADE existing'\n",
    "\n",
    "# ChatGPT completions\n",
    "arcade_existing_chatgpt_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_existing_transformed_generated_openai_ALL_notebooks_m_1_n_1.csv')\n",
    "arcade_existing_chatgpt_df['model'] = 'ChatGPT'\n",
    "arcade_existing_chatgpt_df['benchmark'] = 'ARCADE existing'\n",
    "\n",
    "# Arcade new\n",
    "\n",
    "# Gemini completions\n",
    "arcade_new_gemini_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_new_transformed_generated_google_ALL_notebooks_m_1_n_1.csv')\n",
    "arcade_new_gemini_df['model'] = 'Gemini'\n",
    "arcade_new_gemini_df['benchmark'] = 'ARCADE new'\n",
    "\n",
    "# Claude completions\n",
    "arcade_new_claude_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_new_transformed_generated_anthropic_ALL_notebooks_m_1_n_1.csv')\n",
    "arcade_new_claude_df['model'] = 'Claude'\n",
    "arcade_new_claude_df['benchmark'] = 'ARCADE new'\n",
    "\n",
    "# ChatGPT completions\n",
    "arcade_new_chatgpt_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_new_transformed_generated_openai_ALL_notebooks_m_1_n_1.csv')\n",
    "arcade_new_chatgpt_df['model'] = 'ChatGPT'\n",
    "arcade_new_chatgpt_df['benchmark'] = 'ARCADE new'\n",
    "\n",
    "\n",
    "# Spider2-intents\n",
    "\n",
    "# Gemini completions\n",
    "spider2_intents_gemini_df = pd.read_csv('/kaggle/input/llm-etl-data-set/spider2_intents_transformed_generated_google_ALL_notebooks_m_1_n_1.csv')\n",
    "spider2_intents_gemini_df['model'] = 'Gemini'\n",
    "spider2_intents_gemini_df['benchmark'] = 'Spider2-intents'\n",
    "\n",
    "# Claude completions\n",
    "spider2_intents_claude_df = pd.read_csv('/kaggle/input/llm-etl-data-set/spider2_intents_transformed_generated_anthropic_ALL_notebooks_m_1_n_1.csv')\n",
    "spider2_intents_claude_df['model'] = 'Claude'\n",
    "spider2_intents_claude_df['benchmark'] = 'Spider2-intents'\n",
    "\n",
    "# ChatGPT completions\n",
    "spider2_intents_chatgpt_df = pd.read_csv('/kaggle/input/llm-etl-data-set/spider2_intents_transformed_generated_openai_ALL_notebooks_m_1_n_1.csv')\n",
    "spider2_intents_chatgpt_df['model'] = 'ChatGPT'\n",
    "spider2_intents_chatgpt_df['benchmark'] = 'Spider2-intents'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:29.523099Z",
     "iopub.status.busy": "2025-05-23T05:13:29.522801Z",
     "iopub.status.idle": "2025-05-23T05:13:29.542985Z",
     "shell.execute_reply": "2025-05-23T05:13:29.542122Z",
     "shell.execute_reply.started": "2025-05-23T05:13:29.523075Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.concat([\n",
    "    arcade_existing_gemini_df, \n",
    "    arcade_existing_claude_df, \n",
    "    arcade_existing_chatgpt_df,\n",
    "    arcade_new_gemini_df, \n",
    "    arcade_new_claude_df, \n",
    "    arcade_new_chatgpt_df,\n",
    "    spider2_intents_gemini_df, \n",
    "    spider2_intents_claude_df, \n",
    "    spider2_intents_chatgpt_df,\n",
    "])\n",
    "test_df['m'] = 1\n",
    "test_df['n'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:29.544284Z",
     "iopub.status.busy": "2025-05-23T05:13:29.543955Z",
     "iopub.status.idle": "2025-05-23T05:13:29.567846Z",
     "shell.execute_reply": "2025-05-23T05:13:29.566992Z",
     "shell.execute_reply.started": "2025-05-23T05:13:29.544250Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df.groupby([\"benchmark\", 'model']).intent_number.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m, n > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:29.569011Z",
     "iopub.status.busy": "2025-05-23T05:13:29.568749Z",
     "iopub.status.idle": "2025-05-23T05:13:30.831672Z",
     "shell.execute_reply": "2025-05-23T05:13:30.830368Z",
     "shell.execute_reply.started": "2025-05-23T05:13:29.568989Z"
    }
   },
   "outputs": [],
   "source": [
    "# chatgpt 10 arcade existing notebooks \n",
    "chatgpt_m1n1_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_transformed_generated_openai_ten_notebooks_m_1_n_1.csv')\n",
    "chatgpt_m1n2_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_transformed_generated_openai_ten_notebooks_m_1_n_2.csv')\n",
    "chatgpt_m1n3_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_transformed_generated_openai_ten_notebooks_m_1_n_3.csv')\n",
    "chatgpt_m2n1_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_transformed_generated_openai_ten_notebooks_m_2_n_1.csv')\n",
    "chatgpt_m2n2_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_transformed_generated_openai_ten_notebooks_m_2_n_2.csv')\n",
    "chatgpt_m3n3_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_transformed_generated_openai_ten_notebooks_m_3_n_3.csv')\n",
    "chatgpt_m3n1_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_transformed_generated_openai_ten_notebooks_m_3_n_1.csv')\n",
    "chatgpt_m3n2_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_transformed_generated_openai_ten_notebooks_m_3_n_2.csv')\n",
    "chatgpt_m2n3_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_transformed_generated_openai_ten_notebooks_m_2_n_3.csv')\n",
    "\n",
    "chatgpt_m1n1_df['model'] = 'ChatGPT'\n",
    "chatgpt_m1n2_df['model'] = 'ChatGPT'\n",
    "chatgpt_m1n3_df['model'] = 'ChatGPT'\n",
    "chatgpt_m2n1_df['model'] = 'ChatGPT'\n",
    "chatgpt_m2n2_df['model'] = 'ChatGPT'\n",
    "chatgpt_m2n3_df['model'] = 'ChatGPT'\n",
    "chatgpt_m3n1_df['model'] = 'ChatGPT'\n",
    "chatgpt_m3n2_df['model'] = 'ChatGPT'\n",
    "chatgpt_m3n3_df['model'] = 'ChatGPT'\n",
    "\n",
    "chatgpt_m1n1_df['benchmark'] = 'ARCADE existing'\n",
    "chatgpt_m1n2_df['benchmark'] = 'ARCADE existing'\n",
    "chatgpt_m1n3_df['benchmark'] = 'ARCADE existing'\n",
    "chatgpt_m2n1_df['benchmark'] = 'ARCADE existing'\n",
    "chatgpt_m2n2_df['benchmark'] = 'ARCADE existing'\n",
    "chatgpt_m2n3_df['benchmark'] = 'ARCADE existing'\n",
    "chatgpt_m3n1_df['benchmark'] = 'ARCADE existing'\n",
    "chatgpt_m3n2_df['benchmark'] = 'ARCADE existing'\n",
    "chatgpt_m3n3_df['benchmark'] = 'ARCADE existing'\n",
    "\n",
    "chatgpt_m1n1_df['m'] = 1\n",
    "chatgpt_m1n2_df['m'] = 1\n",
    "chatgpt_m1n3_df['m'] = 1\n",
    "chatgpt_m2n1_df['m'] = 2\n",
    "chatgpt_m2n2_df['m'] = 2\n",
    "chatgpt_m2n3_df['m'] = 2\n",
    "chatgpt_m3n1_df['m'] = 3\n",
    "chatgpt_m3n2_df['m'] = 3\n",
    "chatgpt_m3n3_df['m'] = 3\n",
    "\n",
    "chatgpt_m1n1_df['n'] = 1\n",
    "chatgpt_m1n2_df['n'] = 2\n",
    "chatgpt_m1n3_df['n'] = 3\n",
    "chatgpt_m2n1_df['n'] = 1\n",
    "chatgpt_m2n2_df['n'] = 2\n",
    "chatgpt_m2n3_df['n'] = 3\n",
    "chatgpt_m3n1_df['n'] = 1\n",
    "chatgpt_m3n2_df['n'] = 2\n",
    "chatgpt_m3n3_df['n'] = 3\n",
    "\n",
    "chatgpt_10_test_mn_df = pd.concat([\n",
    "    chatgpt_m1n1_df,\n",
    "    chatgpt_m1n2_df,\n",
    "    chatgpt_m1n3_df,\n",
    "    chatgpt_m2n1_df,\n",
    "    chatgpt_m2n2_df,\n",
    "    chatgpt_m2n3_df,\n",
    "    chatgpt_m3n1_df,\n",
    "    chatgpt_m3n2_df,\n",
    "    chatgpt_m3n3_df    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:30.833997Z",
     "iopub.status.busy": "2025-05-23T05:13:30.833580Z",
     "iopub.status.idle": "2025-05-23T05:13:50.559950Z",
     "shell.execute_reply": "2025-05-23T05:13:50.559149Z",
     "shell.execute_reply.started": "2025-05-23T05:13:30.833959Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemini all arcade new notebooks; note some only have 20 notebooks\n",
    "gemini_m1n1_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_new_transformed_generated_google_ALL_notebooks_m_1_n_1.csv')\n",
    "gemini_m1n2_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_new_transformed_generated_google_ALL_notebooks_m_1_n_2.csv')\n",
    "gemini_m1n3_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_new_transformed_generated_google_ALL_notebooks_m_1_n_3.csv')\n",
    "gemini_m2n1_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_new_transformed_generated_google_ALL_notebooks_m_2_n_1.csv')\n",
    "gemini_m2n2_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_new_transformed_generated_google_ALL_notebooks_m_2_n_2.csv')\n",
    "gemini_m3n3_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_new_transformed_generated_google_ALL_notebooks_m_3_n_3.csv')\n",
    "gemini_m3n1_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_20_new_transformed_generated_google_ALL_notebooks_m_3_n_1.csv')\n",
    "gemini_m3n2_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_20_new_transformed_generated_google_ALL_notebooks_m_3_n_2.csv')\n",
    "gemini_m2n3_df = pd.read_csv('/kaggle/input/llm-etl-data-set/arcade_20_new_transformed_generated_google_ALL_notebooks_m_2_n_3.csv')\n",
    "\n",
    "gemini_m1n1_df['model'] = 'Gemini'\n",
    "gemini_m1n2_df['model'] = 'Gemini'\n",
    "gemini_m1n3_df['model'] = 'Gemini'\n",
    "gemini_m2n1_df['model'] = 'Gemini'\n",
    "gemini_m2n2_df['model'] = 'Gemini'\n",
    "gemini_m2n3_df['model'] = 'Gemini'\n",
    "gemini_m3n1_df['model'] = 'Gemini'\n",
    "gemini_m3n2_df['model'] = 'Gemini'\n",
    "gemini_m3n3_df['model'] = 'Gemini'\n",
    "\n",
    "gemini_m1n1_df['benchmark'] = 'ARCADE new'\n",
    "gemini_m1n2_df['benchmark'] = 'ARCADE new'\n",
    "gemini_m1n3_df['benchmark'] = 'ARCADE new'\n",
    "gemini_m2n1_df['benchmark'] = 'ARCADE new'\n",
    "gemini_m2n2_df['benchmark'] = 'ARCADE new'\n",
    "gemini_m2n3_df['benchmark'] = 'ARCADE new'\n",
    "gemini_m3n1_df['benchmark'] = 'ARCADE new'\n",
    "gemini_m3n2_df['benchmark'] = 'ARCADE new'\n",
    "gemini_m3n3_df['benchmark'] = 'ARCADE new'\n",
    "\n",
    "gemini_m1n1_df['m'] = 1\n",
    "gemini_m1n2_df['m'] = 1\n",
    "gemini_m1n3_df['m'] = 1\n",
    "gemini_m2n1_df['m'] = 2\n",
    "gemini_m2n2_df['m'] = 2\n",
    "gemini_m2n3_df['m'] = 2\n",
    "gemini_m3n1_df['m'] = 3\n",
    "gemini_m3n2_df['m'] = 3\n",
    "gemini_m3n3_df['m'] = 3\n",
    "\n",
    "gemini_m1n1_df['n'] = 1\n",
    "gemini_m1n2_df['n'] = 2\n",
    "gemini_m1n3_df['n'] = 3\n",
    "gemini_m2n1_df['n'] = 1\n",
    "gemini_m2n2_df['n'] = 2\n",
    "gemini_m2n3_df['n'] = 3\n",
    "gemini_m3n1_df['n'] = 1\n",
    "gemini_m3n2_df['n'] = 2\n",
    "gemini_m3n3_df['n'] = 3\n",
    "\n",
    "\n",
    "test_mn_df = pd.concat([\n",
    "    gemini_m1n1_df,\n",
    "    gemini_m1n2_df,\n",
    "    gemini_m1n3_df,\n",
    "    gemini_m2n1_df,\n",
    "    gemini_m2n2_df,\n",
    "    gemini_m2n3_df,\n",
    "    gemini_m3n1_df,\n",
    "    gemini_m3n2_df,\n",
    "    gemini_m3n3_df    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:50.561753Z",
     "iopub.status.busy": "2025-05-23T05:13:50.561387Z",
     "iopub.status.idle": "2025-05-23T05:13:50.578433Z",
     "shell.execute_reply": "2025-05-23T05:13:50.577534Z",
     "shell.execute_reply.started": "2025-05-23T05:13:50.561721Z"
    }
   },
   "outputs": [],
   "source": [
    "# intents with predictions\n",
    "test_df = test_df[~test_df.generated_intent_code.isna()]\n",
    "test_mn_df = test_mn_df[~test_mn_df.generated_intent_code.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:50.580279Z",
     "iopub.status.busy": "2025-05-23T05:13:50.580018Z",
     "iopub.status.idle": "2025-05-23T05:13:50.605133Z",
     "shell.execute_reply": "2025-05-23T05:13:50.604075Z",
     "shell.execute_reply.started": "2025-05-23T05:13:50.580250Z"
    }
   },
   "outputs": [],
   "source": [
    "# no execution errors\n",
    "test_df = test_df[~test_df.execute_error]\n",
    "test_mn_df = test_mn_df[~test_mn_df.execute_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:50.606985Z",
     "iopub.status.busy": "2025-05-23T05:13:50.606645Z",
     "iopub.status.idle": "2025-05-23T05:13:50.620829Z",
     "shell.execute_reply": "2025-05-23T05:13:50.619968Z",
     "shell.execute_reply.started": "2025-05-23T05:13:50.606955Z"
    }
   },
   "outputs": [],
   "source": [
    "# mxn notebooks that are common across all experiments\n",
    "# Create sets of `nb_name` for each dataset\n",
    "nb_name_sets = [\n",
    "    set(gemini_m1n1_df['nb_name']),\n",
    "    set(gemini_m1n2_df['nb_name']),\n",
    "    set(gemini_m1n3_df['nb_name']),\n",
    "    set(gemini_m2n1_df['nb_name']),\n",
    "    set(gemini_m2n2_df['nb_name']),\n",
    "    set(gemini_m2n3_df['nb_name']),\n",
    "    set(gemini_m3n1_df['nb_name']),\n",
    "    set(gemini_m3n2_df['nb_name']),\n",
    "    set(gemini_m3n3_df['nb_name'])\n",
    "]\n",
    "\n",
    "# Find the intersection of all `nb_name` sets\n",
    "common_nb_names = set.intersection(*nb_name_sets)\n",
    "\n",
    "# Filter the combined DataFrame to keep only rows with `nb_name` in the intersection\n",
    "filtered_test_mn_df = test_mn_df[test_mn_df['nb_name'].isin(common_nb_names)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(\"Number of notebooks common to all mxn experiments:\", len(filtered_test_mn_df.nb_name.unique()))\n",
    "print(filtered_test_mn_df.nb_name.unique())\n",
    "\n",
    "test_mn_df = filtered_test_mn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:50.622058Z",
     "iopub.status.busy": "2025-05-23T05:13:50.621713Z",
     "iopub.status.idle": "2025-05-23T05:13:50.634873Z",
     "shell.execute_reply": "2025-05-23T05:13:50.633843Z",
     "shell.execute_reply.started": "2025-05-23T05:13:50.622024Z"
    }
   },
   "outputs": [],
   "source": [
    "# make sure known notebook with header error has been removed\n",
    "assert(test_df[test_df.nb_name == 'dataset_chipotle/notebook_1/annotated.ipynb'].empty)\n",
    "assert(test_mn_df[test_mn_df.nb_name == 'dataset_chipotle/notebook_1/annotated.ipynb'].empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:50.636030Z",
     "iopub.status.busy": "2025-05-23T05:13:50.635715Z",
     "iopub.status.idle": "2025-05-23T05:13:50.650639Z",
     "shell.execute_reply": "2025-05-23T05:13:50.649606Z",
     "shell.execute_reply.started": "2025-05-23T05:13:50.636006Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Filtered test_df shape:\", test_df.shape)\n",
    "print(\"Filtered test_mn_df shape:\", test_mn_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Eval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:13:50.652090Z",
     "iopub.status.busy": "2025-05-23T05:13:50.651754Z",
     "iopub.status.idle": "2025-05-23T05:14:13.633140Z",
     "shell.execute_reply": "2025-05-23T05:14:13.632123Z",
     "shell.execute_reply.started": "2025-05-23T05:13:50.652058Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import tracemalloc\n",
    "from threading import Thread\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "import difflib\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "#import nltk\n",
    "#from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import evaluate\n",
    "\n",
    "from process_transformations import split_code_into_blocks, generate_nl_description \n",
    "\n",
    "def compare_code(generated_code, actual_code):\n",
    "    '''Compares code using the diff library similar to version control'''\n",
    "    generated_code = filter_comments(generated_code)\n",
    "    actual_code = filter_comments(actual_code)\n",
    "    \n",
    "    generated_lines = generated_code.split('\\n')\n",
    "    actual_lines = actual_code.split('\\n')\n",
    "\n",
    "    diff = difflib.unified_diff(\n",
    "        generated_lines, actual_lines,\n",
    "        fromfile='Generated Code', tofile='Actual Code',\n",
    "        lineterm=''\n",
    "    )\n",
    "    \n",
    "    for line in diff:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            print(f'\\033[1;34m{line}\\033[0m')  # Blue header\n",
    "        elif line.startswith('-'):\n",
    "            print(f'\\033[1;31m{line}\\033[0m')  # Red for deletions\n",
    "        elif line.startswith('+'):\n",
    "            print(f'\\033[1;32m{line}\\033[0m')  # Green for additions\n",
    "        else:\n",
    "            print(line)  # Normal text\n",
    "\n",
    "def clean_up_llm_gen_code(generated_code):\n",
    "    '''Applies clean up to code generate by LLMs eg Markdown code ``` delimiters'''\n",
    "    generated_code = generated_code.str.removeprefix(\"```python\").str.lstrip()\n",
    "    generated_code = generated_code.str.removesuffix(\"```\").str.rstrip()\n",
    "    return generated_code\n",
    "\n",
    "# Function to calculate BLEU-3 score using simple n-gram matching\n",
    "def calculate_bleu_3(reference, candidate):\n",
    "    \"\"\"\n",
    "    Computes a simple BLEU-3-like score based on n-gram overlap.\n",
    "    \"\"\"\n",
    "    # Check for empty reference or candidate\n",
    "    if not reference or not candidate:\n",
    "        return 0.0  # Return a BLEU score of 0 for empty inputs\n",
    "    \n",
    "    # Load the BLEU metric\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "     \n",
    "    # Compute BLEU-3 score\n",
    "    results = bleu.compute(predictions=[candidate], references=[[reference]], max_order=3)\n",
    "    #print(results['bleu'])    \n",
    "    return results['bleu']\n",
    "\n",
    "# Function to calculate percentage structure and data correctness\n",
    "def compare_objects(obj_actual, obj_gen):\n",
    "    \"\"\"\n",
    "    Compare two objects (DataFrame or Series) and calculate structure and content correctness scores.\n",
    "    \"\"\"\n",
    "    if obj_actual is None or obj_gen is None:\n",
    "        return (0.0, 0.0)\n",
    "\n",
    "    # Handle DataFrame comparison\n",
    "    if isinstance(obj_actual, pd.DataFrame) and isinstance(obj_gen, pd.DataFrame):\n",
    "        # Structural similarity: Check matching columns\n",
    "        common_columns = set(obj_gen.columns).intersection(set(obj_actual.columns))\n",
    "        structure_score = len(common_columns) / max(len(obj_gen.columns), len(obj_actual.columns))\n",
    "        \n",
    "        # Align data by reindexing to the same number of rows\n",
    "        max_rows = max(len(obj_gen), len(obj_actual))\n",
    "        obj_gen = obj_gen.reindex(range(max_rows))\n",
    "        obj_actual = obj_actual.reindex(range(max_rows))\n",
    "        \n",
    "        # Content correctness: Count exact matches including missing values\n",
    "        exact_matches = 0\n",
    "        total_values = 0\n",
    "        \n",
    "        for col in common_columns:\n",
    "            matches = (obj_gen[col] == obj_actual[col]) & obj_gen[col].notna() & obj_actual[col].notna()\n",
    "            exact_matches += matches.sum()\n",
    "            total_values += max(len(obj_gen[col].dropna()), len(obj_actual[col].dropna()))\n",
    "        \n",
    "        content_score = exact_matches / total_values if total_values > 0 else 0\n",
    "        return (structure_score, content_score)\n",
    "\n",
    "    # Handle Series comparison\n",
    "    elif isinstance(obj_actual, pd.Series) and isinstance(obj_gen, pd.Series):\n",
    "        # Structural similarity: Check if indices match\n",
    "        common_indices = set(obj_gen.index).intersection(set(obj_actual.index))\n",
    "        structure_score = len(common_indices) / max(len(obj_gen.index), len(obj_actual.index))\n",
    "        \n",
    "        # Align data by reindexing to the same indices\n",
    "        obj_gen = obj_gen.reindex(obj_actual.index)\n",
    "        \n",
    "        # Content correctness: Count exact matches including missing values\n",
    "        matches = (obj_gen == obj_actual) & obj_gen.notna() & obj_actual.notna()\n",
    "        exact_matches = matches.sum()\n",
    "        total_values = max(len(obj_gen.dropna()), len(obj_actual.dropna()))\n",
    "        \n",
    "        content_score = exact_matches / total_values if total_values > 0 else 0\n",
    "        return (structure_score, content_score)\n",
    "\n",
    "    # If types don't match, return 0 scores\n",
    "    else:\n",
    "        return (0.0, 0.0)\n",
    "\n",
    "\n",
    "def measure_performance(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Measures execution time, memory usage, and peak CPU utilization of a function.\n",
    "    \"\"\"\n",
    "    # Start tracking memory and time\n",
    "    tracemalloc.start()\n",
    "    start_time = time.perf_counter()  # High-resolution start time\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_memory = process.memory_info().rss\n",
    "\n",
    "    # Variable to store peak CPU utilization\n",
    "    peak_cpu = 0\n",
    "\n",
    "    # Function to monitor CPU usage in a separate thread\n",
    "    def monitor_cpu():\n",
    "        nonlocal peak_cpu\n",
    "        while not stop_monitoring:\n",
    "            cpu_usage = process.cpu_percent(interval=0.01)  # Measure CPU usage every 0.1 seconds\n",
    "            peak_cpu = max(peak_cpu, cpu_usage)\n",
    "\n",
    "    # Start monitoring CPU usage\n",
    "    stop_monitoring = False\n",
    "    cpu_thread = Thread(target=monitor_cpu)\n",
    "    cpu_thread.start()\n",
    "\n",
    "    # Execute the function\n",
    "    result = func(*args, **kwargs)\n",
    "\n",
    "    # Stop monitoring CPU usage\n",
    "    stop_monitoring = True\n",
    "    cpu_thread.join()\n",
    "\n",
    "    # Stop tracking memory and time\n",
    "    end_time = time.perf_counter()  # High-resolution end time\n",
    "    peak_memory = tracemalloc.get_traced_memory()[1]  # Peak memory usage\n",
    "    end_memory = process.memory_info().rss\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    # Calculate elapsed time and memory difference\n",
    "    elapsed_time = (end_time - start_time)*1000\n",
    "    memory_difference = end_memory - start_memory  # Memory difference from baseline\n",
    "\n",
    "    return result, elapsed_time, peak_memory, peak_cpu\n",
    "    \n",
    "\n",
    "def evaluate_completions(test_df, repeat_runs=3):\n",
    "    \"\"\"\n",
    "    Evaluates generated code and stores metrics directly in the original DataFrame.\n",
    "    Captures metrics for every run and adds a run number to the output.\n",
    "\n",
    "    Params:\n",
    "        test_df: DataFrame containing test cases.\n",
    "        repeat_runs: Number of times to repeat each intent for performance measurement.\n",
    "    \"\"\"\n",
    "    # Initialize metrics\n",
    "    test_df[\"bleu_3_exact_code\"] = float(\"nan\")\n",
    "    test_df[\"output_structure_score\"] = float(\"nan\")\n",
    "    test_df[\"output_data_score\"] = float(\"nan\")\n",
    "    test_df[\"original_execution_time\"] = float(\"nan\")\n",
    "    test_df[\"original_peak_memory\"] = float(\"nan\")\n",
    "    test_df[\"original_peak_cpu\"] = float(\"nan\")\n",
    "    test_df[\"generated_execution_time\"] = float(\"nan\")\n",
    "    test_df[\"generated_peak_memory\"] = float(\"nan\")\n",
    "    test_df[\"generated_peak_cpu\"] = float(\"nan\")\n",
    "    test_df[\"generated_error\"] = bool(\"nan\")\n",
    "    test_df[\"original_error\"] = bool(\"nan\")\n",
    "    test_df[\"generated_error_message\"] = ''\n",
    "    test_df[\"original_error_message\"] = ''\n",
    "    test_df[\"original_code_lines\"] = float(\"nan\") \n",
    "    test_df[\"generated_code_lines\"] = float(\"nan\") \n",
    "    test_df['repeat_runs'] = repeat_runs\n",
    "  \n",
    "    df_nbs = test_df.groupby([\"benchmark\", \"model\", \"nb_name\", \"m\", \"n\"])\n",
    "\n",
    "    df_nbs = test_df.groupby([\"benchmark\", \"model\", \"nb_name\", \"m\", \"n\"])\n",
    "\n",
    "    for (benchmark, model, nb_name, m, n), intents in tqdm(df_nbs):\n",
    "        print(f\"Evaluating (model, notebook, m, n): {benchmark, model, nb_name, m, n}\")\n",
    "\n",
    "        # Step 1: Execute the notebook header\n",
    "        nb_header = intents.iloc[0][\"nb_setup_code\"]  # Assuming the header code is in the first row\n",
    "\n",
    "        # Initialize execution states for original and generated\n",
    "        exec_state_org = {\"pd\": pd, \"load_csv_database\": load_csv_database, \"np\": np}\n",
    "        exec_state_gen = {\"pd\": pd, \"load_csv_database\": load_csv_database, \"np\": np}\n",
    "\n",
    "        # Execute the notebook setup\n",
    "        try:\n",
    "            outputs, exec_state_org = execute_intent_code(exec_state_org, nb_header, verbose=False)\n",
    "            inputs = outputs  # Initialize inputs with the header execution outputs\n",
    "            exec_state_gen = exec_state_org.copy()  # The state after executing the same header is the same for org and gen\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing notebook header for {nb_name}: {e}\")\n",
    "            continue  # Skip this notebook if the header fails\n",
    "\n",
    "        # Step 2: Process each intent in the notebook\n",
    "        for _, row in intents.iterrows():\n",
    "            model = row[MODEL_COL]\n",
    "            nb_name = row[\"nb_name\"]\n",
    "            intent = row[\"intent\"]\n",
    "            input_data = row[INPUT_DATA_COL]\n",
    "            description = row[TRANSFORMATION_DESCRIPTION_COL]\n",
    "            actual_code = filter_comments(row[ACTUAL_CODE_COL])\n",
    "            output_data = row[OUTPUT_DATA_COL]\n",
    "            generated_code = filter_comments(row[GENERATED_CODE_COL])\n",
    "\n",
    "            # Calculate number of lines of code for original and generated code\n",
    "            original_code_lines = len(actual_code.splitlines()) if actual_code else 0\n",
    "            generated_code_lines = len(generated_code.splitlines()) if generated_code else 0\n",
    "\n",
    "            # First execution: Update exec_states and capture outputs\n",
    "            try:\n",
    "                print(\"Executing original code (first run)...\")\n",
    "                (original_outputs, exec_state_org), original_elapsed_time, original_peak_memory, original_peak_cpu = measure_performance(\n",
    "                    execute_intent_code, exec_state_org, actual_code, verbose=False\n",
    "                )\n",
    "                original_error = 'error' in original_outputs\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing original code: {e}\")\n",
    "                original_outputs = {'error': str(e)}\n",
    "                original_elapsed_time = None\n",
    "                original_peak_memory = None\n",
    "                original_peak_cpu = None\n",
    "                original_error = True\n",
    "\n",
    "            try:\n",
    "                print(\"Executing generated code (first run)...\")\n",
    "                (generated_outputs, exec_state_gen), generated_elapsed_time, generated_peak_memory, generated_peak_cpu = measure_performance(\n",
    "                    execute_intent_code, exec_state_gen, generated_code, verbose=False\n",
    "                )\n",
    "                generated_error = 'error' in generated_outputs\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing generated code: {e}\")\n",
    "                generated_outputs = {'error': str(e)}\n",
    "                generated_elapsed_time = None\n",
    "                generated_peak_memory = None\n",
    "                generated_peak_cpu = None\n",
    "                generated_error = True\n",
    "\n",
    "            # Subsequent executions: Only measure performance\n",
    "            original_times, original_memories, original_cpus = [original_elapsed_time], [original_peak_memory], [original_peak_cpu]\n",
    "            generated_times, generated_memories, generated_cpus = [generated_elapsed_time], [generated_peak_memory], [generated_peak_cpu]\n",
    "\n",
    "            for run_number in range(2, repeat_runs + 1):\n",
    "                print(f\"Run {run_number} for intent {intent} (performance measurement only)\")\n",
    "\n",
    "                # Measure performance of original code execution\n",
    "                try:\n",
    "                    _, elapsed_time, peak_memory, peak_cpu = measure_performance(\n",
    "                        execute_intent_code, exec_state_org.copy(), actual_code, verbose=False\n",
    "                    )\n",
    "                    original_times.append(elapsed_time)\n",
    "                    original_memories.append(peak_memory)\n",
    "                    original_cpus.append(peak_cpu)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error executing original code: {e}\")\n",
    "                    original_times.append(None)\n",
    "                    original_memories.append(None)\n",
    "                    original_cpus.append(None)\n",
    "\n",
    "                # Measure performance of generated code execution\n",
    "                try:\n",
    "                    _, elapsed_time, peak_memory, peak_cpu = measure_performance(\n",
    "                        execute_intent_code, exec_state_gen.copy(), generated_code, verbose=False\n",
    "                    )\n",
    "                    generated_times.append(elapsed_time)\n",
    "                    generated_memories.append(peak_memory)\n",
    "                    generated_cpus.append(peak_cpu)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error executing generated code: {e}\")\n",
    "                    generated_times.append(None)\n",
    "                    generated_memories.append(None)\n",
    "                    generated_cpus.append(None)\n",
    "\n",
    "            # Calculate averages\n",
    "            avg_original_time = sum(filter(None, original_times)) / len(list(filter(None, original_times))) if any(original_times) else None\n",
    "            avg_original_memory = sum(filter(None, original_memories)) / len(list(filter(None, original_memories))) if any(original_memories) else None\n",
    "            avg_original_cpu = sum(filter(None, original_cpus)) / len(list(filter(None, original_cpus))) if any(original_cpus) else None\n",
    "\n",
    "            avg_generated_time = sum(filter(None, generated_times)) / len(list(filter(None, generated_times))) if any(generated_times) else None\n",
    "            avg_generated_memory = sum(filter(None, generated_memories)) / len(list(filter(None, generated_memories))) if any(generated_memories) else None\n",
    "            avg_generated_cpu = sum(filter(None, generated_cpus)) / len(list(filter(None, generated_cpus))) if any(generated_cpus) else None\n",
    "\n",
    "            # Compute BLEU-3 score for overall code correctness\n",
    "            bleu_score = calculate_bleu_3(actual_code, generated_code)\n",
    "\n",
    "            # Compute BLEU-3 score for overall code correctness\n",
    "            bleu_score = calculate_bleu_3(actual_code, generated_code)\n",
    "\n",
    "            # Compare DataFrames\n",
    "            try:\n",
    "                # Capture both DataFrames and Series\n",
    "                original_objects = [\n",
    "                    pd.DataFrame(json.loads(value)) if value.startswith(\"[{\") and value.endswith(\"}]\") else pd.Series(json.loads(value))\n",
    "                    for key, value in original_outputs.items()\n",
    "                    if isinstance(value, str) and (value.startswith(\"[{\") and value.endswith(\"}]\") or value.startswith(\"{\") and value.endswith(\"}\"))\n",
    "                ]\n",
    "                \n",
    "                generated_objects = [\n",
    "                    pd.DataFrame(json.loads(value)) if value.startswith(\"[{\") and value.endswith(\"}]\") else pd.Series(json.loads(value))\n",
    "                    for key, value in generated_outputs.items()\n",
    "                    if isinstance(value, str) and (value.startswith(\"[{\") and value.endswith(\"}]\") or value.startswith(\"{\") and value.endswith(\"}\"))\n",
    "                ]\n",
    "                \n",
    "                # Compare each original DataFrame with every generated DataFrame\n",
    "                max_structure_scores = []\n",
    "                max_data_scores = []\n",
    "\n",
    "                for original_obj in original_objects:\n",
    "                    structure_scores = []\n",
    "                    data_scores = []\n",
    "                    for generated_obj in generated_objects:\n",
    "                        output_structure_score, output_data_score = compare_objects(original_obj, generated_obj)\n",
    "                        structure_scores.append(output_structure_score)\n",
    "                        data_scores.append(output_data_score)\n",
    "\n",
    "                    # Store the maximum scores for this original DataFrame\n",
    "                    if structure_scores:\n",
    "                        max_structure_scores.append(max(structure_scores))\n",
    "                    if data_scores:\n",
    "                        max_data_scores.append(max(data_scores))\n",
    "\n",
    "                # Compute the average of the maximum scores for this intent\n",
    "                avg_max_structure_score = sum(max_structure_scores) / len(max_structure_scores) if max_structure_scores else 0\n",
    "                avg_max_data_score = sum(max_data_scores) / len(max_data_scores) if max_data_scores else 0\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error comparing DataFrames: {e}\")\n",
    "                avg_max_structure_score, avg_max_data_score = 0, 0\n",
    "            \n",
    "            # Store metrics in the DataFrame\n",
    "            test_df.loc[\n",
    "                (test_df[\"benchmark\"] == benchmark) & \n",
    "                (test_df[\"model\"] == model) & \n",
    "                (test_df[\"nb_name\"] == nb_name) & \n",
    "                (test_df[\"intent\"] == intent) &\n",
    "                (test_df['m'] == m) &\n",
    "                (test_df['n'] == n), \n",
    "                [   GENERATED_OUTPUT_DATA_COL,\n",
    "                    \"bleu_3_exact_code\", \n",
    "                    \"output_structure_score\", \"output_data_score\",\n",
    "                    \"original_execution_time\", \"original_peak_memory\", \"original_peak_cpu\",\n",
    "                    \"generated_execution_time\", \"generated_peak_memory\", \"generated_peak_cpu\",\n",
    "                    \"original_error\", \"generated_error\", \n",
    "                    \"original_code_lines\", \"generated_code_lines\"\n",
    "                ]\n",
    "            ] = [\n",
    "                str(generated_outputs),\n",
    "                round(bleu_score, 3), \n",
    "                avg_max_structure_score, avg_max_data_score,\n",
    "                avg_original_time, avg_original_memory, avg_original_cpu,\n",
    "                avg_generated_time, avg_generated_memory, avg_generated_cpu,\n",
    "                original_error, generated_error,\n",
    "                original_code_lines, generated_code_lines\n",
    "            ]\n",
    "\n",
    "    return test_df\n",
    "\n",
    "\n",
    "def perfect_gen(df):\n",
    "    '''\n",
    "    Generates \"perfect\" predictions by copying original code and output.\n",
    "    Used for testing of evaluation functions.\n",
    "    '''\n",
    "    df[GENERATED_CODE_COL] = df[ACTUAL_CODE_COL]\n",
    "    df[GENERATED_INPUT_DATA_COL] = df[INPUT_DATA_COL]\n",
    "    df[GENERATED_OUTPUT_DATA_COL] = df[OUTPUT_DATA_COL]\n",
    "\n",
    "    # hypothetical perfect model\n",
    "    df[MODEL_COL] = 'perfect'   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1: How do current LLMs perform on Arcade?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T17:41:00.975471Z",
     "iopub.status.busy": "2025-05-22T17:41:00.975147Z",
     "iopub.status.idle": "2025-05-22T17:41:01.077370Z",
     "shell.execute_reply": "2025-05-22T17:41:01.075986Z",
     "shell.execute_reply.started": "2025-05-22T17:41:00.975444Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_models_df = perfect_gen(test_df)\n",
    "#test_models_df = test_df[test_df['benchmark']=='Spider2-intents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T17:41:27.450880Z",
     "iopub.status.busy": "2025-05-22T17:41:27.450458Z",
     "iopub.status.idle": "2025-05-22T18:40:32.255455Z",
     "shell.execute_reply": "2025-05-22T18:40:32.254101Z",
     "shell.execute_reply.started": "2025-05-22T17:41:27.450849Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#metrics = evaluate_completions(test_models_df)\n",
    "#metrics.to_csv(\"/kaggle/working/rq1_results.csv\")\n",
    "#metrics.to_csv(\"/kaggle/working/rq1_results_spider2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:14:52.805672Z",
     "iopub.status.busy": "2025-05-23T05:14:52.805321Z",
     "iopub.status.idle": "2025-05-23T05:14:58.993662Z",
     "shell.execute_reply": "2025-05-23T05:14:58.992680Z",
     "shell.execute_reply.started": "2025-05-23T05:14:52.805644Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_arcade = pd.read_csv(\"/kaggle/input/llm-etl-data-set/rq1_results.csv\")\n",
    "metrics_spider2 = pd.read_csv(\"/kaggle/input/llm-etl-data-set/rq1_results_spider2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:18:28.688048Z",
     "iopub.status.busy": "2025-05-23T05:18:28.687649Z",
     "iopub.status.idle": "2025-05-23T05:18:28.697061Z",
     "shell.execute_reply": "2025-05-23T05:18:28.696105Z",
     "shell.execute_reply.started": "2025-05-23T05:18:28.688015Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = pd.concat([metrics_arcade, metrics_spider2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:18:34.761708Z",
     "iopub.status.busy": "2025-05-23T05:18:34.761383Z",
     "iopub.status.idle": "2025-05-23T05:18:34.797234Z",
     "shell.execute_reply": "2025-05-23T05:18:34.796269Z",
     "shell.execute_reply.started": "2025-05-23T05:18:34.761682Z"
    }
   },
   "outputs": [],
   "source": [
    "rq1_correctness_scores = metrics[[\n",
    "    'benchmark',\n",
    "    'model',\n",
    "    'bleu_3_exact_code',\n",
    "    'output_structure_score',\n",
    "    'output_data_score',\n",
    "    'original_error', \n",
    "    'generated_error',\n",
    "    'original_code_lines', 'generated_code_lines']].groupby(['benchmark','model']).mean().round(2).transpose()\n",
    "\n",
    "rq1_correctness_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T05:24:31.921658Z",
     "iopub.status.busy": "2025-05-23T05:24:31.921285Z",
     "iopub.status.idle": "2025-05-23T05:24:31.932099Z",
     "shell.execute_reply": "2025-05-23T05:24:31.930790Z",
     "shell.execute_reply.started": "2025-05-23T05:24:31.921631Z"
    }
   },
   "outputs": [],
   "source": [
    "print(rq1_correctness_scores.to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:44:07.800409Z",
     "iopub.status.busy": "2025-05-21T22:44:07.800019Z",
     "iopub.status.idle": "2025-05-21T22:44:07.887880Z",
     "shell.execute_reply": "2025-05-21T22:44:07.886460Z",
     "shell.execute_reply.started": "2025-05-21T22:44:07.800378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Switch back to an interactive backend\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall charts (across benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:44:12.166575Z",
     "iopub.status.busy": "2025-05-21T22:44:12.166227Z",
     "iopub.status.idle": "2025-05-21T22:44:12.257333Z",
     "shell.execute_reply": "2025-05-21T22:44:12.255908Z",
     "shell.execute_reply.started": "2025-05-21T22:44:12.166544Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_metrics(metrics, metric_groups):\n",
    "    \"\"\"\n",
    "    Visualize metrics as histograms grouped by metric groups, with each row corresponding to a metric group\n",
    "    and each metric in the group displayed side-by-side. Bars for different models are separated.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics (pd.DataFrame): DataFrame containing the metrics to visualize.\n",
    "    - metric_groups (list of list): A list of metric groups, where each group is a list of metric names.\n",
    "    \"\"\"\n",
    "    # Replace infinite values with NaN\n",
    "    metrics = metrics.replace([float('inf'), -float('inf')], pd.NA)\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    n_rows = len(metric_groups)\n",
    "    max_cols = max(len(group) for group in metric_groups)  # Max number of metrics in any group\n",
    "    fig, axes = plt.subplots(n_rows, max_cols, figsize=(6 * max_cols, 6 * n_rows), constrained_layout=True)\n",
    "\n",
    "    # Ensure axes is always a 2D array for consistent indexing\n",
    "    if n_rows == 1:\n",
    "        axes = [axes]  # Wrap single row in a list\n",
    "    if max_cols == 1:\n",
    "        axes = [[ax] for ax in axes]  # Wrap single column in a list\n",
    "\n",
    "    # Iterate through each metric group and plot\n",
    "    for row_idx, group in enumerate(metric_groups):\n",
    "        for col_idx, metric in enumerate(group):\n",
    "            ax = axes[row_idx][col_idx]\n",
    "            sns.histplot(\n",
    "                data=metrics,\n",
    "                x=metric,\n",
    "                hue='model',\n",
    "                kde=False,\n",
    "                bins=20,\n",
    "                alpha=0.6,\n",
    "                multiple='dodge',  # Ensure bars for different models don't overlap\n",
    "                ax=ax\n",
    "            )\n",
    "            ax.set_title(f\"{metric} Histogram\")\n",
    "            ax.set_xlabel(\"Value\")\n",
    "            ax.set_ylabel(\"Frequency\")\n",
    "         \n",
    "        # Hide unused subplots in the row\n",
    "        for col_idx in range(len(group), max_cols):\n",
    "            axes[row_idx][col_idx].axis(\"off\")\n",
    "\n",
    "    # Show the grid of plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:44:14.375528Z",
     "iopub.status.busy": "2025-05-21T22:44:14.375231Z",
     "iopub.status.idle": "2025-05-21T22:44:17.425100Z",
     "shell.execute_reply": "2025-05-21T22:44:17.424067Z",
     "shell.execute_reply.started": "2025-05-21T22:44:14.375507Z"
    }
   },
   "outputs": [],
   "source": [
    "metric_groups = [\n",
    "    ['bleu_3_exact_code'],\n",
    "    ['output_structure_score', 'output_data_score'],   \n",
    "    ['original_execution_time', 'generated_execution_time'],\n",
    "    ['original_peak_memory', 'generated_peak_memory'],\n",
    "    ['original_peak_cpu', 'generated_peak_cpu'],\n",
    "    ['original_error', 'generated_error'],\n",
    "    ['original_code_lines', 'generated_code_lines']\n",
    "]\n",
    "\n",
    "visualize_metrics(metrics, metric_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By benchmark charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:44:51.458546Z",
     "iopub.status.busy": "2025-05-21T22:44:51.458207Z",
     "iopub.status.idle": "2025-05-21T22:44:52.336508Z",
     "shell.execute_reply": "2025-05-21T22:44:52.335315Z",
     "shell.execute_reply.started": "2025-05-21T22:44:51.458515Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "metrics = metrics.replace([float('inf'), -float('inf')], pd.NA)\n",
    "\n",
    "all_metrics = [\n",
    "    'bleu_3_exact_code',\n",
    "    'generated_error',\n",
    "    'output_structure_score',\n",
    "    'output_data_score',\n",
    "    'original_error',\n",
    "    'generated_error',\n",
    "    'original_code_lines',\n",
    "    'generated_code_lines'\n",
    "]\n",
    "\n",
    "# Get unique benchmark values\n",
    "benchmarks = metrics['benchmark'].unique()\n",
    "\n",
    "# Calculate global x-axis limits for each metric\n",
    "x_limits = {}\n",
    "for metric in all_metrics:\n",
    "    x_min = metrics[metric].min()\n",
    "    x_max = metrics[metric].max()\n",
    "    x_limits[metric] = (x_min, x_max)\n",
    "\n",
    "# Create a grid of subplots\n",
    "n_metrics = len(all_metrics)\n",
    "n_benchmarks = len(benchmarks)\n",
    "\n",
    "fig, axes = plt.subplots(n_metrics, n_benchmarks, figsize=(5 * n_benchmarks, 3 * n_metrics), sharex=False, sharey=False)\n",
    "\n",
    "# Iterate through each metric (row) and benchmark (column)\n",
    "for i, metric in enumerate(all_metrics):\n",
    "    for j, benchmark in enumerate(benchmarks):\n",
    "        ax = axes[i, j] if n_metrics > 1 and n_benchmarks > 1 else axes  # Handle single-row/column cases\n",
    "        group = metrics[metrics['benchmark'] == benchmark]\n",
    "        \n",
    "        sns.histplot(\n",
    "            data=group,\n",
    "            x=metric,\n",
    "            hue='model',\n",
    "            kde=False,\n",
    "            bins=20,\n",
    "            alpha=0.6,\n",
    "            multiple='dodge',\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        # Set x-axis limits\n",
    "        ax.set_xlim(x_limits[metric])\n",
    "        \n",
    "        # Set titles and labels\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Benchmark: {benchmark}\", fontsize=12)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Value\", fontsize=8)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:44:56.163737Z",
     "iopub.status.busy": "2025-05-21T22:44:56.163399Z",
     "iopub.status.idle": "2025-05-21T22:44:56.253517Z",
     "shell.execute_reply": "2025-05-21T22:44:56.252408Z",
     "shell.execute_reply.started": "2025-05-21T22:44:56.163707Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the performance deltas\n",
    "metrics['diff_execution_time'] = metrics['generated_execution_time'] - metrics['original_execution_time']\n",
    "metrics['diff_peak_memory'] = metrics['generated_peak_memory'] - metrics['original_peak_memory']\n",
    "metrics['diff_peak_cpu'] = metrics['generated_peak_cpu'] - metrics['original_peak_cpu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:44:58.947409Z",
     "iopub.status.busy": "2025-05-21T22:44:58.946983Z",
     "iopub.status.idle": "2025-05-21T22:44:59.036485Z",
     "shell.execute_reply": "2025-05-21T22:44:59.035367Z",
     "shell.execute_reply.started": "2025-05-21T22:44:58.947368Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert memory to MBs\n",
    "metrics['diff_peak_memory_mb'] = metrics['diff_peak_memory']/1e6 # convert to MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:45:00.131165Z",
     "iopub.status.busy": "2025-05-21T22:45:00.130831Z",
     "iopub.status.idle": "2025-05-21T22:45:00.227210Z",
     "shell.execute_reply": "2025-05-21T22:45:00.226227Z",
     "shell.execute_reply.started": "2025-05-21T22:45:00.131100Z"
    }
   },
   "outputs": [],
   "source": [
    "rq1_performance_scores = metrics[[\n",
    "    'benchmark',\n",
    "    'model',\n",
    "    'original_execution_time', 'generated_execution_time',\n",
    "    'original_peak_memory', 'generated_peak_memory',\n",
    "    'original_peak_cpu', 'generated_peak_cpu'\n",
    "]].groupby(['benchmark','model']).mean().round(2).transpose()\n",
    "\n",
    "rq1_performance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:45:04.202729Z",
     "iopub.status.busy": "2025-05-21T22:45:04.202440Z",
     "iopub.status.idle": "2025-05-21T22:45:04.293803Z",
     "shell.execute_reply": "2025-05-21T22:45:04.292856Z",
     "shell.execute_reply.started": "2025-05-21T22:45:04.202705Z"
    }
   },
   "outputs": [],
   "source": [
    "print(rq1_performance_scores.to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original to Generated Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:45:06.230919Z",
     "iopub.status.busy": "2025-05-21T22:45:06.230585Z",
     "iopub.status.idle": "2025-05-21T22:45:06.322253Z",
     "shell.execute_reply": "2025-05-21T22:45:06.321260Z",
     "shell.execute_reply.started": "2025-05-21T22:45:06.230887Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics['diff_peak_cpu'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:45:11.241366Z",
     "iopub.status.busy": "2025-05-21T22:45:11.240988Z",
     "iopub.status.idle": "2025-05-21T22:45:11.361125Z",
     "shell.execute_reply": "2025-05-21T22:45:11.360139Z",
     "shell.execute_reply.started": "2025-05-21T22:45:11.241335Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_1samp, sem, t\n",
    "\n",
    "# Function to calculate 95% confidence intervals\n",
    "def calculate_confidence_interval(data):\n",
    "    data = data.dropna()\n",
    "    n = len(data)\n",
    "    if n < 2:\n",
    "        return (np.nan, np.nan)  # Not enough data for confidence interval\n",
    "    mean = np.mean(data)\n",
    "    margin_of_error = t.ppf(0.975, n - 1) * sem(data)\n",
    "    lower_bound = mean - margin_of_error\n",
    "    upper_bound = mean + margin_of_error\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Function to perform a one-sample t-test\n",
    "def perform_t_test(data):\n",
    "    data = data.dropna()\n",
    "    if len(data) < 2:\n",
    "        return np.nan, np.nan  # Not enough data for t-test\n",
    "    t_stat, p_value = ttest_1samp(data, 0)  # Test if the mean difference is significantly different from 0\n",
    "    return t_stat, p_value\n",
    "\n",
    "# Step 1: Group by `benchmark` and `model` and calculate stats for each delta\n",
    "grouped = metrics.groupby(['benchmark', 'model'])\n",
    "\n",
    "# Calculate mean, std, confidence intervals, and t-test results for each delta\n",
    "ci_stats = grouped['diff_execution_time'].apply(calculate_confidence_interval).apply(pd.Series)\n",
    "ci_stats.columns = ['execution_time_ci_lower', 'execution_time_ci_upper']\n",
    "\n",
    "ci_stats['mean_diff_execution_time'] = grouped['diff_execution_time'].mean()\n",
    "ci_stats['std_diff_execution_time'] = grouped['diff_execution_time'].std()\n",
    "\n",
    "ci_stats['execution_time_t_stat'], ci_stats['execution_time_p_value'] = zip(\n",
    "    *grouped['diff_execution_time'].apply(perform_t_test)\n",
    ")\n",
    "\n",
    "ci_stats['peak_memory_ci_lower'], ci_stats['peak_memory_ci_upper'] = zip(\n",
    "    *grouped['diff_peak_memory_mb'].apply(calculate_confidence_interval)\n",
    ")\n",
    "ci_stats['mean_diff_peak_memory'] = grouped['diff_peak_memory_mb'].mean()\n",
    "ci_stats['std_diff_peak_memory'] = grouped['diff_peak_memory_mb'].std()\n",
    "\n",
    "ci_stats['peak_memory_t_stat'], ci_stats['peak_memory_p_value'] = zip(\n",
    "    *grouped['diff_peak_memory_mb'].apply(perform_t_test)\n",
    ")\n",
    "\n",
    "ci_stats['peak_cpu_ci_lower'], ci_stats['peak_cpu_ci_upper'] = zip(\n",
    "    *grouped['diff_peak_cpu'].apply(calculate_confidence_interval)\n",
    ")\n",
    "ci_stats['mean_diff_peak_cpu'] = grouped['diff_peak_cpu'].mean()\n",
    "ci_stats['std_diff_peak_cpu'] = grouped['diff_peak_cpu'].std()\n",
    "\n",
    "ci_stats['peak_cpu_t_stat'], ci_stats['peak_cpu_p_value'] = zip(\n",
    "    *grouped['diff_peak_cpu'].apply(perform_t_test)\n",
    ")\n",
    "\n",
    "# Reset index for better readability\n",
    "#ci_stats = ci_stats.reset_index()\n",
    "\n",
    "# Step 2: Display results\n",
    "print(\"\\nConfidence Intervals and T-Test Results by `benchmark` and `model`:\")\n",
    "ci_stats.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:45:19.933578Z",
     "iopub.status.busy": "2025-05-21T22:45:19.933263Z",
     "iopub.status.idle": "2025-05-21T22:45:20.024676Z",
     "shell.execute_reply": "2025-05-21T22:45:20.023690Z",
     "shell.execute_reply.started": "2025-05-21T22:45:19.933551Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ci_stats.transpose().to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By benchmark charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T22:45:23.805298Z",
     "iopub.status.busy": "2025-05-21T22:45:23.804949Z",
     "iopub.status.idle": "2025-05-21T22:45:24.213495Z",
     "shell.execute_reply": "2025-05-21T22:45:24.212247Z",
     "shell.execute_reply.started": "2025-05-21T22:45:23.805272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace infinite values with NaN\n",
    "metrics = metrics.replace([float('inf'), -float('inf')], pd.NA)\n",
    "\n",
    "all_metrics = [\n",
    "    'diff_execution_time',\n",
    "    'diff_peak_memory',\n",
    "    'diff_peak_cpu'\n",
    "]\n",
    "\n",
    "# Get unique benchmark values\n",
    "benchmarks = metrics['benchmark'].unique()\n",
    "\n",
    "# Calculate global x-axis limits for each metric\n",
    "x_limits = {}\n",
    "for metric in all_metrics:\n",
    "    x_min = metrics[metric].min()\n",
    "    x_max = metrics[metric].max()\n",
    "    x_limits[metric] = (x_min, x_max)\n",
    "\n",
    "# Create a grid of subplots\n",
    "n_metrics = len(all_metrics)\n",
    "n_benchmarks = len(benchmarks)\n",
    "\n",
    "fig, axes = plt.subplots(n_metrics, n_benchmarks, figsize=(5 * n_benchmarks, 3 * n_metrics), sharex=False, sharey=False)\n",
    "\n",
    "# Iterate through each metric (row) and benchmark (column)\n",
    "for i, metric in enumerate(all_metrics):\n",
    "    for j, benchmark in enumerate(benchmarks):\n",
    "        ax = axes[i, j] if n_metrics > 1 and n_benchmarks > 1 else axes  # Handle single-row/column cases\n",
    "        group = metrics[metrics['benchmark'] == benchmark]\n",
    "        \n",
    "        sns.histplot(\n",
    "            data=group,\n",
    "            x=metric,\n",
    "            hue='model',\n",
    "            kde=False,\n",
    "            bins=20,\n",
    "            alpha=0.6,\n",
    "            multiple='dodge',\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        # Set x-axis limits\n",
    "        # ax.set_xlim(x_limits[metric])\n",
    "        \n",
    "        # Set titles and labels\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Benchmark: {benchmark}\", fontsize=12)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(metric, fontsize=10)\n",
    "        ax.set_xlabel(\"Value\", fontsize=8)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2: What types of issues do we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:44:11.923803Z",
     "iopub.status.busy": "2025-04-17T20:44:11.923503Z",
     "iopub.status.idle": "2025-04-17T20:44:12.024313Z",
     "shell.execute_reply": "2025-04-17T20:44:12.023042Z",
     "shell.execute_reply.started": "2025-04-17T20:44:11.923777Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_results(metrics, i, show_prompt=False, show_dfs=True):\n",
    "    \"\"\"\n",
    "    Prints evaluation results for a specific row in the metrics DataFrame.\n",
    "    \"\"\"\n",
    "    row = metrics.iloc[i]  # Extract the row once to avoid repetitive indexing\n",
    "\n",
    "    print(\"Model:\", row['model'])\n",
    "    print(\"Notebook:\", row['nb_name'])\n",
    "    print(\"Original error:\", row['original_error'])\n",
    "    print(\"Generated error:\", row['generated_error'])\n",
    "    if show_prompt:\n",
    "        print(\"Prompt:\", row['evolved_prompt'])\n",
    "    print(\"Intent:\", row['intent'])\n",
    "    #print(\"Header:\", row['nb_setup_code'])\n",
    "    print(\"Bleu 3 score:\", row['bleu_3_exact_code'])\n",
    "    print(\"Output structure score:\", row['output_structure_score'])\n",
    "    print(\"Output data score:\", row['output_structure_score'])\n",
    "    print(\"Code:\")\n",
    "    compare_code(row['generated_intent_code'], row['code'])\n",
    "    print(\"Output:\")\n",
    "\n",
    "    try:\n",
    "        original_outputs = eval(row[OUTPUT_DATA_COL])\n",
    "        generated_outputs = eval(row[GENERATED_OUTPUT_DATA_COL])\n",
    "    \n",
    "        # Extract DataFrames and Series from original outputs\n",
    "        original_objects = {\n",
    "            key: pd.DataFrame(json.loads(value)) if value.startswith(\"[{\") and value.endswith(\"}]\") \n",
    "            else pd.Series(json.loads(value))\n",
    "            for key, value in original_outputs.items()\n",
    "            if isinstance(value, str) and (\n",
    "                value.startswith(\"[{\") and value.endswith(\"}]\") or value.startswith(\"{\") and value.endswith(\"}\")\n",
    "            )\n",
    "        }\n",
    "    \n",
    "        # Extract DataFrames and Series from generated outputs\n",
    "        generated_objects = {\n",
    "            key: pd.DataFrame(json.loads(value)) if value.startswith(\"[{\") and value.endswith(\"}]\") \n",
    "            else pd.Series(json.loads(value))\n",
    "            for key, value in generated_outputs.items()\n",
    "            if isinstance(value, str) and (\n",
    "                value.startswith(\"[{\") and value.endswith(\"}]\") or value.startswith(\"{\") and value.endswith(\"}\")\n",
    "            )\n",
    "        }\n",
    "    \n",
    "        # Display original objects\n",
    "        print(\"Original objects:\", list(original_objects.keys()))\n",
    "        if show_dfs:\n",
    "            for obj_name, original_obj in original_objects.items():\n",
    "                print(obj_name)\n",
    "                if isinstance(original_obj, pd.DataFrame):\n",
    "                    print(\"DataFrame:\")\n",
    "                    display(original_obj)\n",
    "                elif isinstance(original_obj, pd.Series):\n",
    "                    print(\"Series:\")\n",
    "                    display(original_obj)\n",
    "    \n",
    "        # Display generated objects\n",
    "        print(\"Generated objects:\", list(generated_objects.keys()))\n",
    "        if show_dfs:\n",
    "            for obj_name, generated_obj in generated_objects.items():\n",
    "                print(obj_name)\n",
    "                if isinstance(generated_obj, pd.DataFrame):\n",
    "                    print(\"DataFrame:\")\n",
    "                    display(generated_obj)\n",
    "                elif isinstance(generated_obj, pd.Series):\n",
    "                    print(\"Series:\")\n",
    "                    display(generated_obj)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing objects: {e}\")\n",
    "        avg_max_structure_score, avg_max_data_score = 0, 0\n",
    "                \n",
    "    print('_' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:44:12.025748Z",
     "iopub.status.busy": "2025-04-17T20:44:12.025339Z",
     "iopub.status.idle": "2025-04-17T20:44:12.125035Z",
     "shell.execute_reply": "2025-04-17T20:44:12.123987Z",
     "shell.execute_reply.started": "2025-04-17T20:44:12.025701Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics[metrics.benchmark=='ARCADE new'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:44:12.126653Z",
     "iopub.status.busy": "2025-04-17T20:44:12.126290Z",
     "iopub.status.idle": "2025-04-17T20:44:12.222930Z",
     "shell.execute_reply": "2025-04-17T20:44:12.221729Z",
     "shell.execute_reply.started": "2025-04-17T20:44:12.126622Z"
    }
   },
   "outputs": [],
   "source": [
    "outliers = metrics[metrics.generated_code_lines > 50]\n",
    "print(outliers.model.value_counts())\n",
    "print(outliers.nb_name.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:44:12.224535Z",
     "iopub.status.busy": "2025-04-17T20:44:12.224152Z",
     "iopub.status.idle": "2025-04-17T20:44:12.361737Z",
     "shell.execute_reply": "2025-04-17T20:44:12.360686Z",
     "shell.execute_reply.started": "2025-04-17T20:44:12.224500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gemini tends to repeat the entire code at each iteration; ChatGPT and Claude do not for the same prompt\n",
    "\n",
    "print_results(metrics[(metrics.nb_name == 'dataset_batting/notebook_0/annotated.ipynb') & (metrics.model == 'Gemini')], 0, show_prompt=False, show_dfs=False)\n",
    "print_results(metrics[(metrics.nb_name == 'dataset_batting/notebook_0/annotated.ipynb') & (metrics.model == 'ChatGPT')], 0, show_prompt=False, show_dfs=False)\n",
    "print_results(metrics[(metrics.nb_name == 'dataset_batting/notebook_0/annotated.ipynb') & (metrics.model == 'Claude')], 0, show_prompt=False, show_dfs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:44:12.363335Z",
     "iopub.status.busy": "2025-04-17T20:44:12.362935Z",
     "iopub.status.idle": "2025-04-17T20:44:12.569894Z",
     "shell.execute_reply": "2025-04-17T20:44:12.568390Z",
     "shell.execute_reply.started": "2025-04-17T20:44:12.363298Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Function to clean and fix invalid JSON strings\n",
    "def clean_json_string(output):\n",
    "    if isinstance(output, str):\n",
    "        # Replace single quotes with double quotes for the outermost JSON object\n",
    "        output = re.sub(r\"(?<!\\\\)'\", '\"', output)\n",
    "        # Fix improperly escaped characters (e.g., \\/ -> /)\n",
    "        output = output.replace(r\"\\/\", \"/\")\n",
    "    return output\n",
    "\n",
    "# Function to extract error types from the generated_outputs column\n",
    "def extract_error_str(output):\n",
    "    try:\n",
    "        # Clean the JSON string\n",
    "        output = clean_json_string(output)\n",
    "        # Parse the JSON string into a dictionary\n",
    "        output_dict = json.loads(output)\n",
    "        if 'error' in output_dict and isinstance(output_dict['error'], str):\n",
    "            return output_dict['error'].split(\":\")[0].strip()  # Extract the part before the colon (error type)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        pass    \n",
    "    return \"Malformed output\"\n",
    "\n",
    "\n",
    "def categorize_error(error_string):\n",
    "    \"\"\"\n",
    "    Categorize an error string into broader categories based on keywords or patterns.\n",
    "\n",
    "    Args:\n",
    "        error_string (str): The error string to categorize.\n",
    "\n",
    "    Returns:\n",
    "        str: The category of the error.\n",
    "    \"\"\"\n",
    "    if not isinstance(error_string, str):\n",
    "        return \"Uncategorized\"\n",
    "\n",
    "    # Syntax Errors\n",
    "    if \"syntax\" in error_string.lower() or \"unterminated string literal\" in error_string.lower():\n",
    "        return \"Syntax Errors\"\n",
    "\n",
    "    # Runtime Errors\n",
    "    if \"recursion\" in error_string.lower() or \"truth value\" in error_string.lower():\n",
    "        return \"Runtime Errors\"\n",
    "\n",
    "    # DataFrame Errors\n",
    "    if \"dataframe\" in error_string.lower() or \"mask\" in error_string.lower() or \"column\" in error_string.lower():\n",
    "        return \"DataFrame Errors\"\n",
    "\n",
    "    # Index Errors\n",
    "    if \"index\" in error_string.lower() or \"list index\" in error_string.lower():\n",
    "        return \"Index Errors\"\n",
    "\n",
    "    # Type Errors\n",
    "    if \"type\" in error_string.lower() or \"comparison\" in error_string.lower():\n",
    "        return \"Type Errors\"\n",
    "\n",
    "    # Output Errors\n",
    "    if \"malformed\" in error_string.lower() or \"output\" in error_string.lower():\n",
    "        return \"Malformed Output Errors\"\n",
    "\n",
    "    # Other Errors\n",
    "    if \"nan\" in error_string.lower() or \"expected\" in error_string.lower():\n",
    "        return \"Other Errors\"\n",
    "\n",
    "    # Uncategorized\n",
    "    return \"Uncategorized\"\n",
    "\n",
    "\n",
    "# Find the first errors in each notebook\n",
    "errors = metrics[metrics.generated_error == True].groupby(\n",
    "    ['model', 'benchmark', 'nb_name']\n",
    ").apply(lambda group: group.loc[group.intent.idxmin()]).reset_index(drop=True)\n",
    "\n",
    "# Extract error types from the generated_outputs column\n",
    "errors['error_type'] = errors['generated_outputs'].apply(extract_error_str).apply(categorize_error)\n",
    "\n",
    "# Group by model, benchmark, and error category, and count occurrences\n",
    "error_summary = errors.groupby(['model', 'benchmark', 'error_type']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot the table for better readability\n",
    "error_summary_pivot = error_summary.pivot_table(\n",
    "    index='error_type', \n",
    "    columns=['model', 'benchmark'], \n",
    "    values='count', \n",
    "    fill_value=0,\n",
    "    aggfunc='sum',\n",
    "    margins=True, \n",
    "    margins_name='Total'\n",
    ")\n",
    "\n",
    "# Separate the Total row\n",
    "total_row = error_summary_pivot.loc['Total']\n",
    "error_summary_pivot = error_summary_pivot.drop('Total').sort_values('Total', ascending=False)\n",
    "\n",
    "# Append the Total row back at the bottom\n",
    "error_summary_pivot.loc['Total'] = total_row\n",
    "\n",
    "# Display the summarized table\n",
    "error_summary_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:44:12.571265Z",
     "iopub.status.busy": "2025-04-17T20:44:12.570918Z",
     "iopub.status.idle": "2025-04-17T20:44:12.663547Z",
     "shell.execute_reply": "2025-04-17T20:44:12.662568Z",
     "shell.execute_reply.started": "2025-04-17T20:44:12.571234Z"
    }
   },
   "outputs": [],
   "source": [
    "# # generated intents with syntax errors\n",
    "# metrics_errors = metrics[metrics.generated_error]\n",
    "# for i in range(metrics_errors.shape[0]):\n",
    "#     print('*'*80)\n",
    "#     print_results(metrics_errors, i, show_prompt=False, show_dfs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:44:12.665145Z",
     "iopub.status.busy": "2025-04-17T20:44:12.664780Z",
     "iopub.status.idle": "2025-04-17T20:44:12.757424Z",
     "shell.execute_reply": "2025-04-17T20:44:12.756094Z",
     "shell.execute_reply.started": "2025-04-17T20:44:12.665112Z"
    }
   },
   "outputs": [],
   "source": [
    "# # all output\n",
    "# for i in range(metrics.shape[0]):\n",
    "#     print('*'*80)\n",
    "#     print_results(metrics,i, show_prompt=False, show_dfs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T23:05:01.077584Z",
     "iopub.status.busy": "2025-04-03T23:05:01.077181Z",
     "iopub.status.idle": "2025-04-03T23:05:01.162667Z",
     "shell.execute_reply": "2025-04-03T23:05:01.161658Z",
     "shell.execute_reply.started": "2025-04-03T23:05:01.077552Z"
    }
   },
   "source": [
    "## Controllable Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:44:12.758915Z",
     "iopub.status.busy": "2025-04-17T20:44:12.758560Z",
     "iopub.status.idle": "2025-04-17T20:44:12.850139Z",
     "shell.execute_reply": "2025-04-17T20:44:12.848883Z",
     "shell.execute_reply.started": "2025-04-17T20:44:12.758876Z"
    }
   },
   "outputs": [],
   "source": [
    "# # example notebook with original execution errors\n",
    "# print(metrics[metrics.original_error].iloc[0]['nb_setup_code'])\n",
    "\n",
    "# #Error processing 'first_n_rows': Expecting ',' delimiter: line 1 column 757 (char 756)\n",
    "# print(process_first_n_rows(eval(metrics[metrics.original_error].iloc[0]['inputs'])))\n",
    "\n",
    "# # something is wrong with original formatting of data\n",
    "# pprint(metrics[metrics.original_error].iloc[0]['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3: Does more time \"thinking\" by the LLM help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:44:12.851612Z",
     "iopub.status.busy": "2025-04-17T20:44:12.851293Z",
     "iopub.status.idle": "2025-04-17T20:44:12.944878Z",
     "shell.execute_reply": "2025-04-17T20:44:12.943785Z",
     "shell.execute_reply.started": "2025-04-17T20:44:12.851584Z"
    }
   },
   "outputs": [],
   "source": [
    "#metrics_mn = evaluate_completions(test_mn_df)\n",
    "#metrics_mn.to_csv(\"/kaggle/working/rq3_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:45:08.804215Z",
     "iopub.status.busy": "2025-04-17T20:45:08.803803Z",
     "iopub.status.idle": "2025-04-17T20:45:14.964925Z",
     "shell.execute_reply": "2025-04-17T20:45:14.963681Z",
     "shell.execute_reply.started": "2025-04-17T20:45:08.804182Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_mn = pd.read_csv(\"/kaggle/input/llm-etl-data-set/rq3_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:45:14.966681Z",
     "iopub.status.busy": "2025-04-17T20:45:14.966376Z",
     "iopub.status.idle": "2025-04-17T20:45:15.060764Z",
     "shell.execute_reply": "2025-04-17T20:45:15.059579Z",
     "shell.execute_reply.started": "2025-04-17T20:45:14.966655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Switch back to an interactive backend\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:45:15.062554Z",
     "iopub.status.busy": "2025-04-17T20:45:15.062089Z",
     "iopub.status.idle": "2025-04-17T20:45:15.178349Z",
     "shell.execute_reply": "2025-04-17T20:45:15.176810Z",
     "shell.execute_reply.started": "2025-04-17T20:45:15.062520Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_mn[[\n",
    "    'm',\n",
    "    'n',\n",
    "    'bleu_3_exact_code',\n",
    "    'output_structure_score',\n",
    "    'output_data_score',\n",
    "    'original_error', \n",
    "    'generated_error',\n",
    "    'original_code_lines', 'generated_code_lines']].groupby(['m','n']).mean().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T20:58:29.484545Z",
     "iopub.status.busy": "2025-04-17T20:58:29.484183Z",
     "iopub.status.idle": "2025-04-17T20:58:30.586284Z",
     "shell.execute_reply": "2025-04-17T20:58:30.585114Z",
     "shell.execute_reply.started": "2025-04-17T20:58:29.484514Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mxn_heat_map(metrics_mn_grouped):\n",
    "    # Iterate through each metric and plot\n",
    "    for metric in metrics_mn_grouped.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # Pivot the data for heatmap or line plot\n",
    "        metric_data = metrics_mn_grouped[metric].unstack(level='n')  # Reshape for plotting\n",
    "        \n",
    "        # Create a heatmap for the metric\n",
    "        sns.heatmap(metric_data, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={'label': metric})\n",
    "        plt.title(f\"Heatmap of {metric} for increasing m and n\")\n",
    "        plt.xlabel(\"n\")\n",
    "        plt.ylabel(\"m\")\n",
    "        plt.show()\n",
    "        \n",
    "# Group by 'm' and 'n', calculate the mean, and transpose the DataFrame\n",
    "metrics_mn_grouped = metrics_mn[[\n",
    "    'm',\n",
    "    'n',\n",
    "    'bleu_3_exact_code',\n",
    "    'generated_code_lines',\n",
    "    'output_data_score', \n",
    "    'output_structure_score'\n",
    "]].groupby(['m', 'n']).mean()\n",
    "\n",
    "plot_mxn_heat_map(metrics_mn_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T21:00:34.370500Z",
     "iopub.status.busy": "2025-04-17T21:00:34.370123Z",
     "iopub.status.idle": "2025-04-17T21:00:34.467387Z",
     "shell.execute_reply": "2025-04-17T21:00:34.466045Z",
     "shell.execute_reply.started": "2025-04-17T21:00:34.370470Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_mn['original_peak_memory_mb'] = metrics_mn['original_peak_memory']/1e6\n",
    "metrics_mn['generated_peak_memory_mb'] = metrics_mn['generated_peak_memory']/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T21:00:36.049111Z",
     "iopub.status.busy": "2025-04-17T21:00:36.048696Z",
     "iopub.status.idle": "2025-04-17T21:00:36.157621Z",
     "shell.execute_reply": "2025-04-17T21:00:36.156360Z",
     "shell.execute_reply.started": "2025-04-17T21:00:36.049070Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_mn[[\n",
    "    'm', 'n',\n",
    "    'original_execution_time', 'generated_execution_time',\n",
    "    'original_peak_memory_mb', 'generated_peak_memory_mb',\n",
    "    'original_peak_cpu', 'generated_peak_cpu'\n",
    "]].groupby(['m','n']).mean().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T21:00:45.202187Z",
     "iopub.status.busy": "2025-04-17T21:00:45.201639Z",
     "iopub.status.idle": "2025-04-17T21:00:45.827313Z",
     "shell.execute_reply": "2025-04-17T21:00:45.825885Z",
     "shell.execute_reply.started": "2025-04-17T21:00:45.202146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group by 'm' and 'n', calculate the mean, and transpose the DataFrame\n",
    "metrics_mn_grouped = metrics_mn[[\n",
    "    'm',\n",
    "    'n',\n",
    "    'generated_execution_time',\n",
    "    'generated_peak_memory_mb',\n",
    "]].groupby(['m', 'n']).mean()\n",
    "\n",
    "plot_mxn_heat_map(metrics_mn_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Notebook Stat. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T22:29:51.366256Z",
     "iopub.status.busy": "2025-04-17T22:29:51.365865Z",
     "iopub.status.idle": "2025-04-17T22:29:51.471085Z",
     "shell.execute_reply": "2025-04-17T22:29:51.469836Z",
     "shell.execute_reply.started": "2025-04-17T22:29:51.366225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-import necessary packages after code execution state reset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "def same_notebook_analysis(df, metric):\n",
    "    # Ensure `m` and `n` are discrete and limited to 1, 2, 3\n",
    "    df['m'] = pd.Categorical(df['m'], categories=[1, 2, 3], ordered=True)\n",
    "    df['n'] = pd.Categorical(df['n'], categories=[1, 2, 3], ordered=True)\n",
    "\n",
    "    # Mixed effects model (correctness)\n",
    "    model_correct = smf.mixedlm(metric + \" ~ C(m) * C(n)\", df, groups=df[\"subject\"])\n",
    "    result_correct = model_correct.fit()\n",
    "    \n",
    "    # Random effects per subject\n",
    "    random_effects = result_correct.random_effects\n",
    "    \n",
    "    # Tukey HSD post hoc\n",
    "    df['condition'] = df['m'].astype(str) + '-' + df['n'].astype(str)\n",
    "    tukey_correct = pairwise_tukeyhsd(df[metric], df['condition'], alpha=0.05)\n",
    "    \n",
    "    # Box plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    df['m_n'] = 'm=' + df['m'].astype(str) + ', n=' + df['n'].astype(str)\n",
    "    sorted_mn = sorted(df['m_n'].unique(), key=lambda x: (int(x[2]), int(x[-1])))\n",
    "    \n",
    "    sns.boxplot(data=df, y='m_n', x=metric, order=sorted_mn, palette='viridis', showfliers=True)\n",
    "    sns.stripplot(data=df, y='m_n', x=metric, order=sorted_mn, color='k', alpha=0.3, jitter=0.15)\n",
    "    plt.title(\"Box Plot of \" + metric + \" per (m, n) Combination\")\n",
    "    plt.xlabel(\"Correctness Score\")\n",
    "    plt.ylabel(\"(m, n)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Heatmap\n",
    "    pivot_correct = df.groupby(['m', 'n'])[metric].mean().unstack()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pivot_correct, annot=True, cmap='YlGnBu')\n",
    "    plt.title(\"Mean \" + metric + \" by (m, n)\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Confidence interval plot\n",
    "    grouped = df.groupby(['m', 'n'])[metric]\n",
    "    means = grouped.mean()\n",
    "    stds = grouped.std()\n",
    "    counts = grouped.count()\n",
    "    cis = 1.96 * stds / np.sqrt(counts)\n",
    "    \n",
    "    ci_df = means.reset_index()\n",
    "    ci_df['ci'] = cis.values\n",
    "    \n",
    "    plt.figure(figsize=(4, 6))\n",
    "    m_levels = sorted(df['m'].unique())\n",
    "    for m in m_levels:\n",
    "        subset = ci_df[ci_df['m'] == m]\n",
    "        plt.errorbar(subset['n'], subset[metric], yerr=subset['ci'], label=f'm={m}', capsize=4, marker='o', alpha=0.75)\n",
    "    \n",
    "    plt.title(\"Mean \" + metric + \" with 95% Confidence Intervals\")\n",
    "    plt.xticks(ticks=[1, 2, 3], labels=[1, 2, 3])\n",
    "    plt.xlabel(\"Intent Refinement Level (n)\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(title=\"Notebook Refinement Level (m)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract std errors manually\n",
    "    cov_matrix = result_correct.cov_params()\n",
    "    std_errors = np.sqrt(np.diag(cov_matrix))\n",
    "    se_df = pd.DataFrame({'Term': cov_matrix.columns, 'Std_Error': std_errors})\n",
    "       \n",
    "    print(result_correct.summary())\n",
    "    print(result_correct.summary().tables[0].to_latex())\n",
    "    print(result_correct.summary().tables[1].to_latex())\n",
    "    #print(tukey_correct.summary())\n",
    "    #print(random_effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T22:12:06.714344Z",
     "iopub.status.busy": "2025-04-17T22:12:06.713917Z",
     "iopub.status.idle": "2025-04-17T22:12:06.817755Z",
     "shell.execute_reply": "2025-04-17T22:12:06.816404Z",
     "shell.execute_reply.started": "2025-04-17T22:12:06.714310Z"
    }
   },
   "source": [
    "### Bleu 3 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T22:29:52.303030Z",
     "iopub.status.busy": "2025-04-17T22:29:52.302638Z",
     "iopub.status.idle": "2025-04-17T22:29:54.302470Z",
     "shell.execute_reply": "2025-04-17T22:29:54.301258Z",
     "shell.execute_reply.started": "2025-04-17T22:29:52.302997Z"
    }
   },
   "outputs": [],
   "source": [
    "df['subject'] = df['nb_name'] + '_' + df['intent_number'].astype(str)\n",
    "same_notebook_analysis(df, metric='bleu_3_exact_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output structure score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T22:30:03.492171Z",
     "iopub.status.busy": "2025-04-17T22:30:03.491733Z",
     "iopub.status.idle": "2025-04-17T22:30:05.645986Z",
     "shell.execute_reply": "2025-04-17T22:30:05.644783Z",
     "shell.execute_reply.started": "2025-04-17T22:30:03.492131Z"
    }
   },
   "outputs": [],
   "source": [
    "df['subject'] = df['nb_name'] + '_' + df['intent_number'].astype(str)\n",
    "same_notebook_analysis(df, metric='output_structure_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output data score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T22:35:10.011182Z",
     "iopub.status.busy": "2025-04-17T22:35:10.010792Z",
     "iopub.status.idle": "2025-04-17T22:35:12.010909Z",
     "shell.execute_reply": "2025-04-17T22:35:12.009824Z",
     "shell.execute_reply.started": "2025-04-17T22:35:10.011150Z"
    }
   },
   "outputs": [],
   "source": [
    "df = metrics_mn.copy()\n",
    "df['subject'] = df['nb_name'] + '_' + df['intent_number'].astype(str)\n",
    "same_notebook_analysis(df, metric='output_data_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T22:35:04.911452Z",
     "iopub.status.busy": "2025-04-17T22:35:04.911074Z",
     "iopub.status.idle": "2025-04-17T22:35:06.860276Z",
     "shell.execute_reply": "2025-04-17T22:35:06.858965Z",
     "shell.execute_reply.started": "2025-04-17T22:35:04.911419Z"
    }
   },
   "outputs": [],
   "source": [
    "df['subject'] = df['nb_name'] + '_' + df['intent_number'].astype(str)\n",
    "same_notebook_analysis(df, metric='generated_execution_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T22:34:51.692336Z",
     "iopub.status.busy": "2025-04-17T22:34:51.691988Z",
     "iopub.status.idle": "2025-04-17T22:34:53.665319Z",
     "shell.execute_reply": "2025-04-17T22:34:53.664121Z",
     "shell.execute_reply.started": "2025-04-17T22:34:51.692306Z"
    }
   },
   "outputs": [],
   "source": [
    "df['subject'] = df['nb_name'] + '_' + df['intent_number'].astype(str)\n",
    "same_notebook_analysis(df, metric='generated_peak_memory_mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T22:34:43.052338Z",
     "iopub.status.busy": "2025-04-17T22:34:43.052003Z",
     "iopub.status.idle": "2025-04-17T22:34:43.227297Z",
     "shell.execute_reply": "2025-04-17T22:34:43.225593Z",
     "shell.execute_reply.started": "2025-04-17T22:34:43.052303Z"
    }
   },
   "outputs": [],
   "source": [
    "df['subject'] = df['nb_name'] + '_' + df['intent_number'].astype(str)\n",
    "same_notebook_analysis(df, metric='generated_peak_cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7101196,
     "sourceId": 11349944,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7116834,
     "sourceId": 11369318,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7078100,
     "sourceId": 11884618,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7035263,
     "sourceId": 11917002,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 229490371,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 233954297,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 240880404,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
