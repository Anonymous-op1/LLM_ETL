{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_METADATAS_PATH=\"Spider2/spider2-lite/resource/databases/sqlite/\"\n",
    "#You need to download the sqlite dbs manually and unzip them here\n",
    "DATABASES_PATH=\"Spider2/spider2-lite/resource/databases/spider2-localdb/\" \n",
    "DB_CSVS_BASE_PATH=\"Spider2/spider2-lite/resource/databases/csv_dbs\"\n",
    "EVALUATION_SET_PATH=\"Spider2/spider2-lite/spider2-lite.jsonl\"\n",
    "SPIDER2_LOCAL_DB_LINK = \"https://drive.usercontent.google.com/download?id=1coEVsCZq-Xvj9p2TnhBFoFTsY-UoYGmG&export=download&authuser=0&confirm=t&uuid=e4894821-9b03-4a4a-b574-9e931c7f6497&at=AEz70l4CupjM1wWNkGFVtYAST2Xs%3A1743423729461\"\n",
    "GOLD_RESULT_PATH = \"Spider2/spider2-lite/evaluation_suite/gold/exec_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_tasks(tasks_jsonl=None):\n",
    "    tasks_jsonl = tasks_jsonl or EVALUATION_SET_PATH\n",
    "    local_tasks = []\n",
    "    with open(EVALUATION_SET_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            if record.get(\"instance_id\", \"\").startswith(\"local\"):\n",
    "                local_tasks.append(record)\n",
    "    return local_tasks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_databases():\n",
    "    with open(os.path.join(DATABASES_PATH, \"local-map.jsonl\")) as f:\n",
    "        task_db_map = json.load(f)\n",
    "    return set(task_db_map.values())\n",
    "    # local_tasks = get_local_tasks()\n",
    "    # relevant_dbs = set(task.get(\"db\") for task in local_tasks)\n",
    "    # return relevant_dbs\n",
    "\n",
    "def get_task_expected_output(task_id, rows_limit=10, as_dict=False):\n",
    "    expected_output_files = []\n",
    "    result_mapping = {}\n",
    "    for file_name in os.listdir(GOLD_RESULT_PATH):\n",
    "        if file_name.startswith(task_id) and file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(GOLD_RESULT_PATH, file_name)\n",
    "            table_name = os.path.splitext(file_name)[0]\n",
    "            df = pd.read_csv(file_path)\n",
    "            expected_output_files.append(file_name)\n",
    "            if rows_limit >0:\n",
    "                result_mapping[table_name] = df.head(10)\n",
    "            if as_dict:\n",
    "                result_mapping[table_name] = result_mapping[table_name].to_dict(orient='records')\n",
    "    return result_mapping\n",
    "    # with open(EVALUATION_SET_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    #     for line in file:\n",
    "    #         record = json.loads(line)\n",
    "    #         if record.get(\"instance_id\", \"\").startswith(\"local\"):\n",
    "    #             db_set.add(record.get(\"db\"))\n",
    "    # return db_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_task_expected_output(\"local002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_tables_to_csv(folders_path, dbs_path, output_folder, force=False, db_set=None):\n",
    "    \"\"\"\n",
    "    Reads JSON files describing database tables, fetches data, and exports to CSV.\n",
    "    \n",
    "    :param folders_path: Path to folder containing JSON files.\n",
    "    :param db_path: Path to SQLite database file.\n",
    "    :param output_folder: Path to folder where CSV files will be saved.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(DATABASES_PATH, \"local-map.jsonl\")) as f:\n",
    "        task_db_map = json.load(f)\n",
    "    for db_name in set(task_db_map.values()):\n",
    "        print(db_name)\n",
    "        if db_set and db_name not in db_set:\n",
    "            continue\n",
    "        db_folder_path = os.path.join(folders_path, db_name)\n",
    "        db_output_path = os.path.join(output_folder, db_name)\n",
    "        if force and os.path.exists(db_output_path):\n",
    "            shutil.rmtree(db_output_path)\n",
    "        os.makedirs(db_output_path, exist_ok=True)\n",
    "    \n",
    "        db_path = os.path.join(dbs_path, f\"{db_name}.sqlite\")    \n",
    "        os.makedirs(db_folder_path, exist_ok=True)\n",
    "        \n",
    "        # Connect to the SQLite database\n",
    "        print(db_path)\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        try:\n",
    "            for filename in os.listdir(db_folder_path):\n",
    "                if filename.endswith(\".json\"):\n",
    "                    file_path = os.path.join(db_folder_path, filename)\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        table_metadata = json.load(f)\n",
    "                    \n",
    "                    table_name = table_metadata[\"table_name\"]\n",
    "                    output_csv = os.path.join(db_output_path, f\"{table_name}.csv\")\n",
    "                    \n",
    "                    # Fetch data from the table\n",
    "                    query = f\"SELECT * FROM {table_name};\"\n",
    "                    df = pd.read_sql_query(query, conn)\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    df.to_csv(output_csv, index=False)\n",
    "                    print(f\"Exported {table_name} to {output_csv}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data_types(folders_path):\n",
    "    datatypes_set = set()\n",
    "    db_set = get_relevant_databases()\n",
    "    for db_name in os.listdir(folders_path):\n",
    "        if db_set and db_name not in db_set:\n",
    "            continue\n",
    "        db_folder_path = os.path.join(folders_path, db_name)            \n",
    "        try:\n",
    "            for filename in os.listdir(db_folder_path):\n",
    "                if filename.endswith(\".json\"):\n",
    "                    file_path = os.path.join(db_folder_path, filename)\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        table_metadata = json.load(f)\n",
    "                    \n",
    "                    datatypes_set.update(x.lower() for x in table_metadata.get(\"column_types\", []))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return\n",
    "    return datatypes_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_metadata(database_name, table_name):\n",
    "    table_metadata_file = os.path.join(DB_METADATAS_PATH, database_name, f\"{table_name}.json\")\n",
    "    try:\n",
    "        with open(table_metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "           table_metadata = json.load(f)\n",
    "           return table_metadata\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_table_names_list(database_name):\n",
    "    db_folder_path = os.path.join(DB_METADATAS_PATH, database_name)\n",
    "    table_names = []\n",
    "    try:\n",
    "        for filename in os.listdir(db_folder_path):\n",
    "            if filename.endswith(\".json\"):\n",
    "                file_path = os.path.join(db_folder_path, filename)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    table_metadata = json.load(f)\n",
    "                    table_name = table_metadata[\"table_name\"]\n",
    "                    table_names.append(table_name)\n",
    "        return table_names\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_dtype_map(database_name, table_name, sql=False):\n",
    "    table_metadata = get_table_metadata(database_name, table_name)\n",
    "    dtype_map = dict(\n",
    "        zip(\n",
    "            table_metadata.get(\"column_names\", []),\n",
    "            table_metadata.get(\"column_types\", []),\n",
    "        )\n",
    "    )\n",
    "    if sql:\n",
    "        return dtype_map\n",
    "    else:\n",
    "        try:\n",
    "            with open(\"spider_dtype_mappings.json\", \"r\") as f:\n",
    "                mappings = json.load(f)\n",
    "                dtype_map = {k: mappings.get(v.lower(), \"object\") for k, v in dtype_map.items()}\n",
    "                return dtype_map\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "def get_database_dtype_map(database_name, sql=False):\n",
    "    table_names = get_database_table_names_list(database_name=database_name)\n",
    "    if not table_names: return {}\n",
    "    dtype_map = {\n",
    "        tname: get_table_dtype_map(database_name, tname, sql=sql) for tname in table_names\n",
    "    }\n",
    "    return dtype_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ground_truth_sql(task_id):\n",
    "    sql_path = f\"Spider2/spider2-lite/evaluation_suite/gold/sql/{task_id}.sql\"\n",
    "    if os.path.exists(sql_path):\n",
    "        print(sql_path)\n",
    "        with open(sql_path, \"r\") as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_database_dtype_map(\"E_commerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_database(database_name, rows_limit=10, as_dict=False):\n",
    "    \"\"\"\n",
    "    Load a CSV-dumped database into a dictionary where each key is a table name and the value is a pandas DataFrame.\n",
    "\n",
    "    :param database_path: Path to the directory containing the CSV files representing the database.\n",
    "    :return: A dictionary with table names as keys and pandas DataFrames as values.\n",
    "    \"\"\"\n",
    "    path1 = os.path.join(DB_CSVS_BASE_PATH, database_name)\n",
    "    path2 = path1.replace(\"-\", \"_\")\n",
    "    path3 = path1.replace(\"_\", \"-\")\n",
    "    path = [x for x in [path1, path2, path3] if os.path.exists(x)]\n",
    "    if path:\n",
    "        database_path = path[0]\n",
    "    else:\n",
    "        print(\"Failed to get database\")\n",
    "        return None\n",
    "    tables = {}\n",
    "    for file_name in os.listdir(database_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            table_name = os.path.splitext(file_name)[0]\n",
    "            file_path = os.path.join(database_path, file_name)\n",
    "            dtypes = get_table_dtype_map(database_name, table_name)\n",
    "            tables[table_name] = pd.read_csv(file_path,)# dtype=dtypes\n",
    "            if rows_limit >= 0:\n",
    "                tables[table_name] = tables[table_name].iloc[:rows_limit]\n",
    "            if as_dict:\n",
    "                tables[table_name] = tables[table_name].to_dict(orient='records')\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def make_header(db_name, work_dir=\"\"):\n",
    "    header = f\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "{db_name} = dict()\n",
    "for table, table_data in load_csv_database('{db_name}', rows_limit=-1).items():\n",
    "    {db_name}[table] = pd.DataFrame(table_data)\n",
    "OUTPUT_DIR = f\"{work_dir}/output.csv\"\n",
    "\"\"\"\n",
    "    return header\n",
    "\n",
    "def make_llm_etl_dataset_df(local_task_list):\n",
    "    \"\"\"\n",
    "    Create a DataFrame containing ETL dataset information.\n",
    "\n",
    "    :param local_task_list: List of local tasks.\n",
    "    :return: A pandas DataFrame with the ETL dataset.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(DATABASES_PATH, \"local-map.jsonl\")) as f:\n",
    "        task_db_map = json.load(f)\n",
    "    with open(\"Spider2/spider2-lite/evaluation_suite/gold/spider2lite_eval.jsonl\", \"r\", encoding=\"utf-8\") as eval_file:\n",
    "        groundtruth_data = [json.loads(line) for line in eval_file]\n",
    "    groundtruth_data = {task.get(\"instance_id\"): task for task in groundtruth_data if task.get(\"instance_id\", \"\").startswith(\"local\")}\n",
    "    \n",
    "    notebook_dict = defaultdict(int)\n",
    "    # fieldnames = [\"nb_name\", \"work_dir\", \"nb_header\", \"intent_number\", \"intent\", \"code\", \"inputs\", \"outputs\", \"d_types\", \"db_name\"]\n",
    "    dataset = []\n",
    "    for id, task in enumerate(local_task_list):\n",
    "        spider_task_id = task[\"instance_id\"]\n",
    "        gt_data = groundtruth_data.get(spider_task_id, {})\n",
    "        db_name = task_db_map[spider_task_id]\n",
    "        work_dir = f\"dataset_{db_name}/notebook_{notebook_dict[db_name]}\"\n",
    "        nb_name = f\"{work_dir}/annotated.ipynb\"\n",
    "        notebook_dict[db_name] += 1\n",
    "        task_data = {\n",
    "            \"spider_task_id\": spider_task_id,\n",
    "            \"nb_name\": nb_name,\n",
    "            \"work_dir\": work_dir,\n",
    "            \"nb_header\": \"\",\n",
    "            \"intent_number\": id,\n",
    "            \"intent\": task['question'],\n",
    "            \"code\": \"-1\",\n",
    "            \"inputs\": load_csv_database(task['db'], 10, as_dict=True),\n",
    "            \"outputs\": get_task_expected_output(task['instance_id'], rows_limit=10, as_dict=True),\n",
    "            \"d_types\": get_database_dtype_map(db_name),\n",
    "            \"db_name\": task['db'],\n",
    "            \"ground_truth_sql\": read_ground_truth_sql(task),\n",
    "            \"condition_cols\": gt_data.get(\"condition_cols\", None),\n",
    "            \"ignore_order\": gt_data.get(\"ignore_order\", True),\n",
    "            \"toks\": gt_data.get(\"toks\", None),\n",
    "            \"external_knowledge\": task[\"external_knowledge\"],\n",
    "        }\n",
    "        dataset.append(task_data)\n",
    "    return pd.DataFrame(dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    db_set = get_relevant_databases()\n",
    "    local_task_list = get_local_tasks()\n",
    "    dump_tables_to_csv(folders_path=DB_METADATAS_PATH,\n",
    "                   dbs_path=DATABASES_PATH,\n",
    "                   output_folder=DB_CSVS_BASE_PATH,\n",
    "                   force=False,\n",
    "                   db_set=db_set)\n",
    "    llm_etl_df = make_llm_etl_dataset_df(local_task_list)\n",
    "    this_df = llm_etl_df[llm_etl_df[\"d_types\"] != {}]\n",
    "    # llm_etl_df.to_pickle(\"datasets/spider2.single_intent.pickle\")\n",
    "    # llm_etl_df.to_json(\"datasets/spider2.single_intent.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_etl_df.to_csv(\"datasets/spider2.single_intent.csv\", index=False)\n",
    "llm_etl_df.to_json(\"datasets/spider2.single_intent.jsonl\", orient='records', lines=True)\n",
    "llm_etl_df.to_pickle(\"datasets/spider2.single_intent.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
