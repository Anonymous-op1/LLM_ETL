{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7294ae7d-0ccc-4ced-9471-8d9e9466c50d",
    "_uuid": "d2b8be80-b5d0-49cc-bd29-db029770264b",
    "execution": {
     "iopub.execute_input": "2025-04-10T06:36:08.613913Z",
     "iopub.status.busy": "2025-04-10T06:36:08.613192Z",
     "iopub.status.idle": "2025-04-10T06:36:09.078573Z",
     "shell.execute_reply": "2025-04-10T06:36:09.077340Z",
     "shell.execute_reply.started": "2025-04-10T06:36:08.613858Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/arcade-new'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:12:41.459476Z",
     "iopub.status.busy": "2025-04-10T06:12:41.459020Z",
     "iopub.status.idle": "2025-04-10T06:12:41.475532Z",
     "shell.execute_reply": "2025-04-10T06:12:41.474370Z",
     "shell.execute_reply.started": "2025-04-10T06:12:41.459434Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = ''\n",
    "os.environ['KAGGLE_KEY'] = ''\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Authenticate with Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# # write out the kaggle.json file anyhow\n",
    "# kaggle_token = {\"username\": os.environ['KAGGLE_USERNAME'],\n",
    "#              \"key\": os.environ['KAGGLE_KEY']}\n",
    "\n",
    "# os.makedirs(\"~/.kaggle\", exist_ok=True)\n",
    "# with open(\"~/.kaggle/kaggle.json\", \"w\") as f:\n",
    "#     json.dump(kaggle_token, f)\n",
    "\n",
    "# # Set permissions for the file\n",
    "# os.chmod(\"~/.kaggle/kaggle.json\", 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install ARCADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:29.032352Z",
     "iopub.status.busy": "2025-04-10T04:45:29.031919Z",
     "iopub.status.idle": "2025-04-10T04:45:30.294354Z",
     "shell.execute_reply": "2025-04-10T04:45:30.293018Z",
     "shell.execute_reply.started": "2025-04-10T04:45:29.032306Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/google-research/arcade-nl2code.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:30.296526Z",
     "iopub.status.busy": "2025-04-10T04:45:30.296118Z",
     "iopub.status.idle": "2025-04-10T04:45:30.315621Z",
     "shell.execute_reply": "2025-04-10T04:45:30.314522Z",
     "shell.execute_reply.started": "2025-04-10T04:45:30.296484Z"
    }
   },
   "outputs": [],
   "source": [
    "# createa a package out of arcade and install it\n",
    "setup_content = \"\"\"\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='arcade_nl2code',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    install_requires=[\n",
    "        'tensorflow',  # Add other dependencies here\n",
    "    ],\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "with open('/kaggle/working/arcade-nl2code/setup.py', 'w') as file:\n",
    "    file.write(setup_content)\n",
    "\n",
    "def create_init_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for dir_name in dirs:\n",
    "            init_file_path = os.path.join(root, dir_name, '__init__.py')\n",
    "            if not os.path.exists(init_file_path):\n",
    "                with open(init_file_path, 'w') as f:\n",
    "                    f.write(\"# This file makes the directory a Python package\\n\")\n",
    "                print(f\"Created: {init_file_path}\")\n",
    "\n",
    "directory = '//kaggle/working/arcade-nl2code'\n",
    "create_init_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:30.316844Z",
     "iopub.status.busy": "2025-04-10T04:45:30.316578Z",
     "iopub.status.idle": "2025-04-10T04:45:38.493154Z",
     "shell.execute_reply": "2025-04-10T04:45:38.491789Z",
     "shell.execute_reply.started": "2025-04-10T04:45:30.316821Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/working/arcade-nl2code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:38.495127Z",
     "iopub.status.busy": "2025-04-10T04:45:38.494685Z",
     "iopub.status.idle": "2025-04-10T04:45:38.501537Z",
     "shell.execute_reply": "2025-04-10T04:45:38.499904Z",
     "shell.execute_reply.started": "2025-04-10T04:45:38.495080Z"
    }
   },
   "outputs": [],
   "source": [
    "# add the package to python path\n",
    "sys.path.append('/kaggle/working/arcade-nl2code')\n",
    "sys.path.append('/kaggle/working/arcade-nl2code/arcade_nl2code')\n",
    "sys.path.append('/kaggle/working/arcade-nl2code/arcade_nl2code/annotated_dataset')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:38.503111Z",
     "iopub.status.busy": "2025-04-10T04:45:38.502741Z",
     "iopub.status.idle": "2025-04-10T04:45:38.522988Z",
     "shell.execute_reply": "2025-04-10T04:45:38.521860Z",
     "shell.execute_reply.started": "2025-04-10T04:45:38.503073Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a requirements file for possible versions \n",
    "# reqs_2022 = \"\"\"\n",
    "# tensorflow-cpu==2.10.0\n",
    "# absl-py==1.3.0\n",
    "# pandas==1.5.2\n",
    "# dacite==1.7.0\n",
    "# nbformat==5.7.0\n",
    "# dill==0.3.6\n",
    "# sacrebleu==2.3.1\n",
    "# astor==0.8.1\n",
    "# folium==0.12.1\n",
    "# seaborn==0.12.2\n",
    "# vega==3.5.0\n",
    "# bokeh==2.4.3\n",
    "# plotly==5.10.0\n",
    "# matplotlib==3.6.2\n",
    "# chart_studio==1.1.0\n",
    "# \"\"\"\n",
    "\n",
    "# with open('/kaggle/working/arcade-nl2code/requirements_2022.txt', 'w') as file:\n",
    "#     file.write(reqs_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:38.527119Z",
     "iopub.status.busy": "2025-04-10T04:45:38.526791Z",
     "iopub.status.idle": "2025-04-10T04:45:38.539466Z",
     "shell.execute_reply": "2025-04-10T04:45:38.538223Z",
     "shell.execute_reply.started": "2025-04-10T04:45:38.527091Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -r /kaggle/working/arcade-nl2code/arcade_nl2code/evaluation/requirements.txt\n",
    "# !pip install -r /kaggle/working/arcade-nl2code/requirements_2022.txt\n",
    "# !pip install seqio\n",
    "# !pip install diff_match_patch  # was missing in requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:38.541745Z",
     "iopub.status.busy": "2025-04-10T04:45:38.541310Z",
     "iopub.status.idle": "2025-04-10T04:45:45.829199Z",
     "shell.execute_reply": "2025-04-10T04:45:45.828046Z",
     "shell.execute_reply.started": "2025-04-10T04:45:38.541693Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip show tensorflow\n",
    "pip show tensorflow-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download ARCADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:45.830806Z",
     "iopub.status.busy": "2025-04-10T04:45:45.830486Z",
     "iopub.status.idle": "2025-04-10T04:45:48.674906Z",
     "shell.execute_reply": "2025-04-10T04:45:48.673499Z",
     "shell.execute_reply.started": "2025-04-10T04:45:45.830777Z"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d googleai/arcade-nl2code-dataset -p arcade_nl2code/annotated_dataset/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:48.676731Z",
     "iopub.status.busy": "2025-04-10T04:45:48.676299Z",
     "iopub.status.idle": "2025-04-10T04:45:49.375433Z",
     "shell.execute_reply": "2025-04-10T04:45:49.373938Z",
     "shell.execute_reply.started": "2025-04-10T04:45:48.676685Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/arcade_nl2code/annotated_dataset/dataset\n",
    "!unzip -o arcade-nl2code-dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:49.377721Z",
     "iopub.status.busy": "2025-04-10T04:45:49.377267Z",
     "iopub.status.idle": "2025-04-10T04:45:49.383282Z",
     "shell.execute_reply": "2025-04-10T04:45:49.382074Z",
     "shell.execute_reply.started": "2025-04-10T04:45:49.377661Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install --force-reinstall pandas==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:45:49.385177Z",
     "iopub.status.busy": "2025-04-10T04:45:49.384668Z",
     "iopub.status.idle": "2025-04-10T04:46:23.539107Z",
     "shell.execute_reply": "2025-04-10T04:46:23.537865Z",
     "shell.execute_reply.started": "2025-04-10T04:45:49.385143Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade --force-reinstall pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:46:23.540635Z",
     "iopub.status.busy": "2025-04-10T04:46:23.540320Z",
     "iopub.status.idle": "2025-04-10T04:46:23.636516Z",
     "shell.execute_reply": "2025-04-10T04:46:23.635452Z",
     "shell.execute_reply.started": "2025-04-10T04:46:23.540605Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "import pandas as pd\n",
    "\n",
    "# remove known missing datasets to avoid load errors\n",
    "new_dataset = pd.read_json(\"/kaggle/working/arcade_nl2code/annotated_dataset/dataset/new_tasks/dataset.json\")\n",
    "new_dataset = new_dataset[~(\n",
    "    (new_dataset.dataset == 'student-performance-gce-al-exam-2020-sri-lanka') |\n",
    "    (new_dataset.dataset == 'finding-donors-for-charityml') |\n",
    "    (new_dataset.dataset == 'alternative-fuel-vehicles-in-the-us') |\n",
    "    (new_dataset.dataset == 'full-filled-brain-stroke-dataset') |\n",
    "    (new_dataset.dataset == 'gameloft-android-games-collection-2022') |\n",
    "    (new_dataset.dataset == 'russia-to-world-trade14m-data-points') |\n",
    "        (new_dataset.dataset == 'rafael-nadal') |\n",
    "     (new_dataset.dataset == 'top-5000-albums-of-all-time-spotify-features') \n",
    "    \n",
    ")]\n",
    "new_dataset.to_json(\"/kaggle/working/arcade_nl2code/annotated_dataset/dataset/new_tasks/dataset.json\", orient='records')\n",
    "\n",
    "kaggle_dataset_prov = pd.read_csv(\"/kaggle/working/arcade_nl2code/annotated_dataset/dataset/new_tasks/kaggle_dataset_provenance.csv\")\n",
    "kaggle_dataset_prov = kaggle_dataset_prov[~(\n",
    "    (kaggle_dataset_prov.ref=='sasikaamarasinghe/student-performance-gce-al-exam-2020-sri-lanka') |\n",
    "    (kaggle_dataset_prov.ref=='nancyalaswad90/finding-donors-for-charityml') |\n",
    "    (kaggle_dataset_prov.ref=='saketpradhan/alternative-fuel-vehicles-in-the-us') |\n",
    "    (kaggle_dataset_prov.ref=='zzettrkalpakbal/full-filled-brain-stroke-dataset') |\n",
    "    (kaggle_dataset_prov.ref=='azminetoushikwasi/gameloft-android-games-collection-2022') |\n",
    "    (kaggle_dataset_prov.ref=='pranav941/russia-to-world-trade14m-data-points') |\n",
    "    (kaggle_dataset_prov.ref=='ankanhore545/rafael-nadal') |\n",
    "     (kaggle_dataset_prov.ref=='lucascantu/top-5000-albums-of-all-time-spotify-features')\n",
    ")]\n",
    "kaggle_dataset_prov['Version'] = kaggle_dataset_prov['Version'].fillna(0.0)\n",
    "kaggle_dataset_prov.to_csv(\"/kaggle/working/arcade_nl2code/annotated_dataset/dataset/new_tasks/kaggle_dataset_provenance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:46:23.637663Z",
     "iopub.status.busy": "2025-04-10T04:46:23.637393Z",
     "iopub.status.idle": "2025-04-10T04:46:23.653298Z",
     "shell.execute_reply": "2025-04-10T04:46:23.652323Z",
     "shell.execute_reply.started": "2025-04-10T04:46:23.637640Z"
    }
   },
   "outputs": [],
   "source": [
    "new_dataset[(\n",
    "    (new_dataset.dataset == 'student-performance-gce-al-exam-2020-sri-lanka') |\n",
    "    (new_dataset.dataset == 'finding-donors-for-charityml') |\n",
    "    (new_dataset.dataset == 'alternative-fuel-vehicles-in-the-us')\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:46:23.655053Z",
     "iopub.status.busy": "2025-04-10T04:46:23.654663Z",
     "iopub.status.idle": "2025-04-10T04:46:23.667928Z",
     "shell.execute_reply": "2025-04-10T04:46:23.666438Z",
     "shell.execute_reply.started": "2025-04-10T04:46:23.655010Z"
    }
   },
   "outputs": [],
   "source": [
    "kaggle_dataset_prov[(\n",
    "    (kaggle_dataset_prov.ref=='sasikaamarasinghe/student-performance-gce-al-exam-2020-sri-lanka') |\n",
    "    (kaggle_dataset_prov.ref=='nancyalaswad90/finding-donors-for-charityml') |\n",
    "        (kaggle_dataset_prov.ref=='saketpradhan/alternative-fuel-vehicles-in-the-us')\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:46:23.669322Z",
     "iopub.status.busy": "2025-04-10T04:46:23.668975Z",
     "iopub.status.idle": "2025-04-10T04:46:23.684064Z",
     "shell.execute_reply": "2025-04-10T04:46:23.683005Z",
     "shell.execute_reply.started": "2025-04-10T04:46:23.669292Z"
    }
   },
   "outputs": [],
   "source": [
    "# break up into parts to get around kaggle request limits\n",
    "\n",
    "def divide_dataset(dataset, rows_per_part):\n",
    "    \"\"\"\n",
    "    Divides the dataset into parts with a specified number of rows per part.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (pd.DataFrame): The dataset to divide.\n",
    "    - rows_per_part (int): The number of rows per part.\n",
    "\n",
    "    Returns:\n",
    "    - list of pd.DataFrame: A list containing the divided parts of the dataset.\n",
    "    \"\"\"\n",
    "    return [dataset[i:i + rows_per_part] for i in range(0, len(dataset), rows_per_part)]\n",
    "\n",
    "kaggle_dataset_prov_parts = divide_dataset(kaggle_dataset_prov, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:46:23.685821Z",
     "iopub.status.busy": "2025-04-10T04:46:23.685320Z",
     "iopub.status.idle": "2025-04-10T04:46:23.701977Z",
     "shell.execute_reply": "2025-04-10T04:46:23.700547Z",
     "shell.execute_reply.started": "2025-04-10T04:46:23.685769Z"
    }
   },
   "outputs": [],
   "source": [
    "len(kaggle_dataset_prov_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T04:46:23.703959Z",
     "iopub.status.busy": "2025-04-10T04:46:23.703575Z",
     "iopub.status.idle": "2025-04-10T04:50:39.365303Z",
     "shell.execute_reply": "2025-04-10T04:50:39.363727Z",
     "shell.execute_reply.started": "2025-04-10T04:46:23.703928Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_bash_command_for_each_part(dataset_parts):\n",
    "    \"\"\"\n",
    "    Runs the given bash command for each dataset part.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_parts (list): A list of dataset parts (e.g., DataFrames or file paths).\n",
    "    \"\"\"\n",
    "    for i, part in enumerate(dataset_parts):\n",
    "        print(f\"Processing part {i + 1}...\")\n",
    "\n",
    "        print(part)\n",
    "        # update the list of datasets to process\n",
    "        part.to_csv(\"/kaggle/working/arcade_nl2code/annotated_dataset/dataset/new_tasks/kaggle_dataset_provenance.csv\", index=False)\n",
    "        \n",
    "        # Define the bash command\n",
    "        bash_command = \"\"\"\n",
    "        cd /kaggle/working/arcade_nl2code/annotated_dataset\n",
    "        PYTHONPATH=../../  \n",
    "        python /kaggle/working/arcade-nl2code/arcade_nl2code/annotated_dataset/build_new_tasks_split.py\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the bash command and capture output\n",
    "        process = subprocess.run(\n",
    "            bash_command,\n",
    "            shell=True,\n",
    "            executable='/bin/bash',\n",
    "            capture_output=True,\n",
    "            text=True  # Ensures output is returned as a string\n",
    "        )\n",
    "        \n",
    "        # Check for errors\n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error occurred while processing part {i + 1}\")\n",
    "            print(f\"Standard Output:\\n{process.stdout}\")\n",
    "            print(f\"Standard Error:\\n{process.stderr}\")\n",
    "        else:\n",
    "            print(f\"Successfully processed part {i + 1}\")\n",
    "            print(f\"Standard Output:\\n{process.stdout}\")\n",
    "\n",
    "run_bash_command_for_each_part(kaggle_dataset_prov_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload source files to new version of ARCADE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:04:59.117008Z",
     "iopub.status.busy": "2025-04-10T06:04:59.116590Z",
     "iopub.status.idle": "2025-04-10T06:05:01.168021Z",
     "shell.execute_reply": "2025-04-10T06:05:01.166844Z",
     "shell.execute_reply.started": "2025-04-10T06:04:59.116978Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "def copy_to_new_dataset_version(source_dir, dataset_slug, version_message):\n",
    "    \"\"\"\n",
    "    Copies all files and folders from the source directory to a new version of an existing Kaggle dataset\n",
    "    using the command-line Kaggle API tools.\n",
    "\n",
    "    Parameters:\n",
    "    - source_dir (str): Path to the source directory containing files and folders to copy.\n",
    "    - dataset_slug (str): The Kaggle dataset slug in the format 'username/dataset-name'.\n",
    "    - version_message (str): A message describing the changes in the new dataset version.\n",
    "    \"\"\"\n",
    "    # Create a temporary directory to stage files for upload\n",
    "    temp_dir = \"/kaggle/temp_dataset\"\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)  # Clear the temp directory if it exists\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Copy all files and folders from the source directory to the temp directory\n",
    "    for item in os.listdir(source_dir):\n",
    "        source_path = os.path.join(source_dir, item)\n",
    "        dest_path = os.path.join(temp_dir, item)\n",
    "        if os.path.isdir(source_path):\n",
    "            shutil.copytree(source_path, dest_path)\n",
    "        else:\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "\n",
    "    # Create the dataset-metadata.json file in the temp directory\n",
    "    metadata_file = os.path.join(temp_dir, \"dataset-metadata.json\")\n",
    "    metadata_content = {\n",
    "        \"title\": dataset_slug.split(\"/\")[-1],  # Use the dataset name as the title\n",
    "        \"id\": dataset_slug,  # The dataset slug (e.g., 'username/dataset-name')\n",
    "        \"licenses\": [{\"name\": \"CC0-1.0\"}]  # Default license\n",
    "    }\n",
    "    with open(metadata_file, \"w\") as f:\n",
    "        json.dump(metadata_content, f, indent=4)\n",
    "\n",
    "    # Use the Kaggle CLI to create a new version of the dataset\n",
    "    print(f\"Creating a new version of the dataset: {dataset_slug}\")\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"kaggle\", \"datasets\", \"version\",\n",
    "                \"-p\", temp_dir,\n",
    "                \"-m\", version_message,\n",
    "                \"--dir-mode\", \"zip\"\n",
    "            ],\n",
    "            check=True\n",
    "        )\n",
    "        print(\"New dataset version created successfully!\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error creating a new dataset version: {e}\")\n",
    "\n",
    "    # Clean up the temporary directory\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "# Example usage\n",
    "source_directory = \"/kaggle/working/arcade_nl2code/annotated_dataset/dataset/new_tasks/artifacts\"  # Path to the Kaggle working directory\n",
    "existing_dataset_slug = \"existing_dataset_slug\"  # Replace with your dataset slug\n",
    "version_description = \"Updated dataset with new files and folders from the working directory.\"\n",
    "\n",
    "copy_to_new_dataset_version(source_directory, existing_dataset_slug, version_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:18:28.557953Z",
     "iopub.status.busy": "2025-04-10T06:18:28.557529Z",
     "iopub.status.idle": "2025-04-10T06:18:59.516738Z",
     "shell.execute_reply": "2025-04-10T06:18:59.515654Z",
     "shell.execute_reply.started": "2025-04-10T06:18:28.557911Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def create_zip_of_folder(folder_path, output_zip_path):\n",
    "    \"\"\"\n",
    "    Create a zip file of the specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder to be zipped.\n",
    "    - output_zip_path (str): Path where the zip file will be created (without .zip extension).\n",
    "    \"\"\"\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Error: Folder {folder_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Create the zip file\n",
    "    shutil.make_archive(output_zip_path, 'zip', folder_path)\n",
    "    print(f\"Zip file created at: {output_zip_path}.zip\")\n",
    "\n",
    "# Example usage\n",
    "folder_to_zip = \"/kaggle/working/arcade_nl2code/annotated_dataset/dataset/new_tasks/artifacts\"\n",
    "output_zip = \"/kaggle/working/arcade_new_datasets\"\n",
    "\n",
    "create_zip_of_folder(folder_to_zip, output_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform to Create New Notebook Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:37:07.100863Z",
     "iopub.status.busy": "2025-04-10T06:37:07.100406Z",
     "iopub.status.idle": "2025-04-10T06:37:07.151766Z",
     "shell.execute_reply": "2025-04-10T06:37:07.150417Z",
     "shell.execute_reply.started": "2025-04-10T06:37:07.100824Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "import chardet\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pprint\n",
    "import re\n",
    "import gc\n",
    "\n",
    "from model_eval import execute_intent_code\n",
    "\n",
    "def replace_csv_reads_with_dataframe(code, dataset_file_name, dataframe_name=\"first_n_rows\"):\n",
    "    \"\"\"\n",
    "    Detects and replaces instances of `pd.read_csv` in the provided code with a predefined DataFrame.\n",
    "\n",
    "    Args:\n",
    "        code (str): The code string to process.\n",
    "        dataset_file_path (str): The path to the dataset file being replaced.\n",
    "        dataframe_name (str): The name of the DataFrame to replace `pd.read_csv` calls with.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified code with `pd.read_csv` calls replaced.\n",
    "    \"\"\"\n",
    "    # Step 1: Replace direct `pd.read_csv` calls with the file name\n",
    "    # Match patterns like pd.read_csv('athlete_events.csv') or pd.read_csv(\"athlete_events.csv\")\n",
    "    direct_read_csv_pattern = rf\"pd\\.read_csv\\(['\\\"]{re.escape(dataset_file_name)}['\\\"]\\)\"\n",
    "    code = re.sub(direct_read_csv_pattern, dataframe_name, code)\n",
    "\n",
    "    # Step 2: Replace `pd.read_csv` calls that use a variable\n",
    "    # Match patterns like pd.read_csv(PATH)\n",
    "    read_csv_variable_pattern = r\"pd\\.read_csv\\(\\s*\\w+\\s*\\)\"\n",
    "    code = re.sub(read_csv_variable_pattern, dataframe_name, code)\n",
    "    \n",
    "    return code\n",
    "\n",
    "\n",
    "def cleanup_exec(outputs):\n",
    "    '''Cleans up memory after a notebook execution'''\n",
    "\n",
    "    # Deallocate the variables\n",
    "    for var_name in outputs.keys():\n",
    "        if var_name in globals():\n",
    "            del globals()[var_name]\n",
    "        elif var_name in locals():\n",
    "            del locals()[var_name]\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "def calc_total_input_sizes(csv_files):\n",
    "    \"\"\"\n",
    "    Calculate the total size, total rows, and total columns for a list of CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_files (list): List of paths to CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (total_size_bytes, total_rows, total_columns)\n",
    "    \"\"\"\n",
    "    # Initialize variables to track total size, row count, and column count\n",
    "    total_size_bytes = 0\n",
    "    total_rows = 0\n",
    "    total_columns = 0\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Get the file size in bytes\n",
    "            file_size = os.path.getsize(csv_file)\n",
    "            total_size_bytes += file_size\n",
    "\n",
    "            # Count the number of rows in the CSV file\n",
    "            with open(csv_file, 'r') as f:\n",
    "                row_count = sum(1 for _ in f) - 1  # Subtract 1 for the header row\n",
    "            total_rows += row_count\n",
    "\n",
    "            # Count the number of columns in the CSV file\n",
    "            with open(csv_file, 'r') as f:\n",
    "                first_line = f.readline()\n",
    "                column_count = len(first_line.split(','))\n",
    "            total_columns += column_count\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {csv_file}: {e}\")\n",
    "\n",
    "    return total_size_bytes, total_rows, total_columns\n",
    "\n",
    "def transform_new_dataset(datasets_json, \n",
    "                          artifact_path='/kaggle/input/arcade-new', \n",
    "                          n_rows=10, \n",
    "                          top_n_entries=None, \n",
    "                          specific_nb=None):\n",
    "    \"\"\"\n",
    "    Transforms the ARCADE dataset to the desired format by reading the initial input\n",
    "    and executing each intent one by one, processing only the top `top_n_entries` entries.\n",
    "    \"\"\"\n",
    "    # Load the JSON file\n",
    "    with open(datasets_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Limit to the top `top_n_entries` if specified\n",
    "    if top_n_entries is not None:\n",
    "        data = data[:top_n_entries]\n",
    "    \n",
    "    # Number of rows to extract\n",
    "    N = n_rows\n",
    "    \n",
    "    # Extract intent, code pairs, and execute each intent\n",
    "    rows = []\n",
    "    for entry in tqdm(data):\n",
    "        nb_name = entry.get(\"notebook_name\")\n",
    "        work_dir = entry.get(\"work_dir\")\n",
    "        print(\"Running: \", nb_name)\n",
    "\n",
    "        # Construct the dataset folder path\n",
    "        dataset_folder_path = os.path.join(artifact_path, work_dir)\n",
    "\n",
    "        # Find all CSV files in the folder\n",
    "        csv_files = glob(os.path.join(dataset_folder_path, 'dataset', \"*.csv\"))\n",
    "        count_csvs = len(csv_files)\n",
    "        total_size_bytes, total_rows, total_cols = calc_total_input_sizes(csv_files)\n",
    "       \n",
    "        # Print the total size and row count\n",
    "        print(f\"Number input files: {count_csvs}\")\n",
    "        print(f\"Total size: {total_size_bytes} bytes\")\n",
    "        print(f\"Total rows: {total_rows}\")\n",
    "        print(f\"Total cols: {total_cols}\")\n",
    "        print(\"Input files:\", csv_files)\n",
    "        \n",
    "        # First turn input are the imports and dataset load, so execute it first\n",
    "        nb_header = entry.get(\"turns\", [])[0][\"input\"]\n",
    "\n",
    "        # Prepend code to change the working directory\n",
    "        change_dir_code = f\"import os\\nos.chdir('{dataset_folder_path}')\\n\"\n",
    "        nb_header = change_dir_code + nb_header\n",
    "\n",
    "        print(\"Executing notebook header\")\n",
    "        exec_state = {\"pd\": pd}  # Initialize execution state with Pandas\n",
    "        outputs, exec_state = execute_intent_code(exec_state, nb_header)\n",
    "        \n",
    "        # Initialize the execution state with the output from the header execution\n",
    "        inputs = outputs \n",
    "\n",
    "        # Serialize the exec_state using pickle\n",
    "        # serialized_exec_state = pickle.dumps(exec_state)\n",
    "\n",
    "        # Mark intents with errors if any of the previous intents had errors\n",
    "        execute_error = False\n",
    "\n",
    "        # Check if header had errors\n",
    "        if \"error\" in outputs:\n",
    "            execute_error = True\n",
    "        \n",
    "        for i, turn in enumerate(entry.get(\"turns\", [])):\n",
    "            print(\"Executing intent:\", i)\n",
    "            intent = turn[\"turn\"][\"intent\"][\"value\"]\n",
    "            code = turn[\"turn\"][\"code\"][\"value\"]\n",
    "            \n",
    "            # Execute the code intent\n",
    "            outputs, exec_state = execute_intent_code(exec_state, code)\n",
    "\n",
    "            # Check if this intent had an error\n",
    "            if \"error\" in outputs:\n",
    "                execute_error = True\n",
    "            \n",
    "            # Append the results\n",
    "            rows.append({\n",
    "                \"nb_name\": nb_name,\n",
    "                \"work_dir\": work_dir,\n",
    "                'nb_setup_code': nb_header,\n",
    "                \"intent_number\": i,\n",
    "                \"intent\": intent,\n",
    "                \"code\": code,\n",
    "                #\"exec_state\": str(serialized_exec_state),\n",
    "                \"inputs\": str(inputs),  # Inputs for this intent\n",
    "                \"outputs\": str(outputs),  # Outputs from this intent\n",
    "                \"execute_error\": execute_error,\n",
    "                'error_msg': outputs.get('error', ''),\n",
    "                'num_intput_files': count_csvs,\n",
    "                'total_input_size': total_size_bytes,\n",
    "                'total_input_rows': total_rows,\n",
    "                'total_input_cols': total_cols\n",
    "            })\n",
    "\n",
    "            # Update inputs for the next intent\n",
    "            inputs = outputs\n",
    "\n",
    "        # Clean up memory for all notebook outputs\n",
    "        cleanup_exec(outputs)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df\n",
    "    \n",
    "# def transform_new_dataset(datasets_json, \n",
    "#                       artifact_path= '/kaggle/working/arcade_nl2code/annotated_dataset/dataset/existing_tasks/artifacts', \n",
    "#                       n_rows=10, \n",
    "#                       top_n_entries=None, \n",
    "#                       specific_nb=None):\n",
    "#     \"\"\"\n",
    "#     Transforms the ARCADE dataset to the desired format by reading the initial input\n",
    "#     and executing each intent one by one, processing only the top `top_n_entries` entries.\n",
    "#     \"\"\"\n",
    "#     # Load the JSON file\n",
    "#     with open(datasets_json, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     # Limit to the top `top_n_entries` if specified\n",
    "#     if top_n_entries is not None:\n",
    "#         data = data[:top_n_entries]\n",
    "    \n",
    "#     # Number of rows to extract\n",
    "#     N = n_rows\n",
    "    \n",
    "#     # Extract intent, code pairs, and first N rows of the dataset, and execute each intent\n",
    "#     rows = []\n",
    "#     for entry in tqdm(data):\n",
    "#         nb_name = entry.get(\"notebook_name\")\n",
    "#         work_dir = entry.get(\"work_dir\")\n",
    "#         print(\"Running: \", nb_name)\n",
    "\n",
    "#         # Construct the dataset folder path\n",
    "#         dataset_folder_path = os.path.join(artifact_path, work_dir, 'dataset')\n",
    "\n",
    "#         # change to the folder containing the datasets for this notebook\n",
    "#         #print(\"Changing path to:\", dataset_folder_path)\n",
    "#         #os.chdir(dataset_folder_path)\n",
    "        \n",
    "#         # Find all CSV files in the folder\n",
    "#         csv_files = glob(os.path.join(dataset_folder_path, \"*.csv\"))\n",
    "#         print(csv_files)\n",
    "        \n",
    "#         # Load the first CSV file if any exist\n",
    "#         if csv_files:\n",
    "#             dataset_file_path = csv_files[0]  # Use the first CSV file\n",
    "    \n",
    "#             # Detect the file encoding\n",
    "#             with open(dataset_file_path, \"rb\") as f:\n",
    "#                 result = chardet.detect(f.read())\n",
    "#                 encoding = result[\"encoding\"]\n",
    "    \n",
    "#             # Use the detected encoding\n",
    "#             try:\n",
    "#                 dataset_df = pd.read_csv(dataset_file_path, encoding=encoding, on_bad_lines='skip')\n",
    "#                 first_n_rows = pd.DataFrame(dataset_df.head(N))  # Convert to DataFrame\n",
    "#             except Exception as e:\n",
    "#                 print(\"Error reading first n rows\", e)\n",
    "#                 first_n_rows = pd.DataFrame()\n",
    "                \n",
    "#         else:\n",
    "#             first_n_rows = None  # Handle missing dataset files\n",
    "        \n",
    "#         # First turn input are the imports and dataset load, so execute it first\n",
    "#         nb_header = entry.get(\"turns\", [])[0][\"input\"]\n",
    "\n",
    "#         # Replace CSV reads with the first_n_rows DataFrame\n",
    "#         if first_n_rows is not None:\n",
    "#             nb_header = replace_csv_reads_with_dataframe(nb_header, \n",
    "#                                                          os.path.join('dataset', os.path.basename(dataset_file_path)), \n",
    "#                                                          dataframe_name=\"first_n_rows\")\n",
    "\n",
    "#         print(\"Executing nb header\")\n",
    "#         exec_state = {\"pd\": pd, \"first_n_rows\": first_n_rows}  # Add first_n_rows to exec_state\n",
    "#         outputs, exec_state = execute_intent_code(exec_state, nb_header)\n",
    "        \n",
    "#         # Initialize the execution state with the output from the header execution\n",
    "#         inputs = outputs \n",
    "\n",
    "#         # Serialize the exec_state using pickle\n",
    "#         #serialized_exec_state = pickle.dumps(exec_state)\n",
    "\n",
    "#         # we mark an intents with erorr if any of the previous intents had errors\n",
    "#         execute_error = False\n",
    "\n",
    "#         # check if header had errors\n",
    "#         if \"error\" in outputs:\n",
    "#             execute_error = True\n",
    "        \n",
    "#         for i, turn in enumerate(entry.get(\"turns\", [])):\n",
    "#             print(\"Executing intent:\", i)\n",
    "#             intent = turn[\"turn\"][\"intent\"][\"value\"]\n",
    "#             code = turn[\"turn\"][\"code\"][\"value\"]\n",
    "            \n",
    "#             # Execute the code intent\n",
    "#             outputs, exec_state = execute_intent_code(exec_state, code)\n",
    "\n",
    "#             # check if this intent had an error\n",
    "#             if \"error\" in outputs:\n",
    "#                 execute_error=True\n",
    "            \n",
    "#             # Append the results\n",
    "#             rows.append({\n",
    "#                 \"nb_name\": nb_name,\n",
    "#                 \"work_dir\": work_dir,\n",
    "#                 'nb_setup_code': nb_header,\n",
    "#                 \"intent_number\": i,\n",
    "#                 \"intent\": intent,\n",
    "#                 \"code\": code,\n",
    "#                 #\"exec_state\": str(serialized_exec_state),\n",
    "#                 \"inputs\": str(inputs),  # Inputs for this intent\n",
    "#                 \"outputs\": str(outputs),  # Outputs from this intent\n",
    "#                 \"execute_error\": execute_error,\n",
    "#                 'error_msg': outputs.get('error','')\n",
    "#             })\n",
    "\n",
    "#             # Update inputs for the next intent\n",
    "#             inputs = outputs\n",
    "\n",
    "#         #clean up memory for all notebook ouputs\n",
    "#         cleanup_exec(outputs)\n",
    "\n",
    "#     # Create a DataFrame\n",
    "#     df = pd.DataFrame(rows)\n",
    "\n",
    "#     return df\n",
    "\n",
    "def save_to_pickle(df, file_path='arcade_existing_transformed.pkl'):\n",
    "    # Extract just what you need, with code as raw strings\n",
    "    extracted_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        entry = {\n",
    "            'nb_name': row['nb_name'],\n",
    "            'work_dir': row['work_dir'],\n",
    "            'intent_number': row['intent_number'],\n",
    "            'intent': row['intent'],\n",
    "            'code': row['code'],  # This preserves exact formatting\n",
    "            'nb_setup_code': row['nb_setup_code'],\n",
    "            'inputs': row['inputs'],\n",
    "            'outputs': row['outputs'],\n",
    "            'execute_error': row['execute_error']\n",
    "        }\n",
    "        extracted_data.append(entry)\n",
    "    \n",
    "    # Save using pickle to preserve exact string representation\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(extracted_data, f)\n",
    "    \n",
    "    print(\"Saved data with preserved formatting to 'arcade_existing_transformed.pkl'\")\n",
    "\n",
    "def load_from_pkl(file_path):\n",
    "    # Load the pickled data (list of dictionaries)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        extracted_data = pickle.load(f)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df_loaded = pd.DataFrame(extracted_data)\n",
    "    \n",
    "    # Verify the data loaded correctly\n",
    "    print(f\"Loaded DataFrame with shape: {df_loaded.shape}\")\n",
    "    print(f\"Columns: {df_loaded.columns.tolist()}\")\n",
    "    \n",
    "    # Check a sample of the code to ensure formatting is preserved\n",
    "    if len(df_loaded) > 0:\n",
    "        print(\"\\nSample code from first row:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(df_loaded.iloc[0]['code'])\n",
    "        print(\"-\" * 50)\n",
    "    return df_loaded\n",
    "\n",
    "\n",
    "def divide_and_process_dataset(json_path, artifact_path, n_parts, n_rows_output, start=0, end=None):\n",
    "    \"\"\"\n",
    "    Divide the dataset.json into n parts and process each part using transform_new_dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - json_path (str): Path to the original dataset.json file.\n",
    "    - artifact_path (str): Path to the artifact directory.\n",
    "    - n_parts (int): Number of parts to divide the dataset into.\n",
    "    - n_rows_output (int): Number of output rows to include.\n",
    "    - start (int): Starting index of the parts to process (inclusive).\n",
    "    - end (int): Ending index of the parts to process (exclusive). If None, process until the last part.\n",
    "    \"\"\"\n",
    "    # Load the dataset.json into a DataFrame\n",
    "    with open(json_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame(dataset)\n",
    "\n",
    "    # Divide the DataFrame into n parts\n",
    "    chunk_size = len(df) // n_parts\n",
    "    dataset_parts = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(artifact_path, exist_ok=True)\n",
    "\n",
    "    # Adjust the end parameter if not provided\n",
    "    if end is None:\n",
    "        end = len(dataset_parts)\n",
    "\n",
    "    # Process only the specified range of parts\n",
    "    for i, part in enumerate(dataset_parts[start:end], start=start):\n",
    "        print(f\"Processing part {i + 1}/{len(dataset_parts)}...\")\n",
    "\n",
    "        # Save the current part to a temporary JSON file\n",
    "        part_path = os.path.join(artifact_path, f\"dataset_part_{i + 1}.json\")\n",
    "        part.to_json(part_path, orient='records', lines=False)\n",
    "\n",
    "        # Call the transform_new_dataset function for this part\n",
    "        df_part = transform_new_dataset(\n",
    "            part_path,\n",
    "            artifact_path=artifact_path,\n",
    "            n_rows=n_rows_output,\n",
    "            top_n_entries=None  # Adjust as needed\n",
    "        )\n",
    "\n",
    "        # Display the processed DataFrame for this part\n",
    "        print(f\"Processed DataFrame for part {i + 1}:\")\n",
    "        print(df_part)\n",
    "\n",
    "        # Count errors in the processed DataFrame\n",
    "        error_count = df_part['execute_error'].sum()\n",
    "        print(f\"Number of notebooks with errors: {error_count}\")\n",
    "\n",
    "        # Save the processed DataFrame to a pickle file\n",
    "        save_to_pickle(df_part, f'/kaggle/working/arcade_new_transformed_{i}.pkl')\n",
    "\n",
    "        # Test whether we can read back the DataFrame from the pickle file\n",
    "        df_loaded = load_from_pkl(f'/kaggle/working/arcade_new_transformed_{i}.pkl')\n",
    "        \n",
    "        # Verify the data loaded correctly\n",
    "        print(f\"Loaded DataFrame with shape: {df_loaded.shape}\")\n",
    "        print(f\"Columns: {df_loaded.columns.tolist()}\")\n",
    "        \n",
    "        # Check a sample of the code to ensure formatting is preserved\n",
    "        if len(df_loaded) > 0:\n",
    "            print(\"\\nSample code from first row:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(df_loaded.iloc[0]['code'])\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        # Clear memory explicitly\n",
    "        del part\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:37:33.186439Z",
     "iopub.status.busy": "2025-04-10T06:37:33.186062Z",
     "iopub.status.idle": "2025-04-10T06:39:51.485194Z",
     "shell.execute_reply": "2025-04-10T06:39:51.484035Z",
     "shell.execute_reply.started": "2025-04-10T06:37:33.186410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform arcade new to our dataset format\n",
    "df = transform_new_dataset(\n",
    "    datasets_json='/kaggle/working/arcade_nl2code/annotated_dataset/dataset/new_tasks/dataset.json',\n",
    "    artifact_path='/kaggle/input/arcade-new',\n",
    "    n_rows=10,\n",
    "    top_n_entries=None  # Adjust as needed\n",
    ")\n",
    "\n",
    "# Save the processed DataFrame to a pickle file\n",
    "save_to_pickle(df, f'/kaggle/working/arcade_new_transformed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:39:51.487435Z",
     "iopub.status.busy": "2025-04-10T06:39:51.486992Z",
     "iopub.status.idle": "2025-04-10T06:39:51.510754Z",
     "shell.execute_reply": "2025-04-10T06:39:51.509626Z",
     "shell.execute_reply.started": "2025-04-10T06:39:51.487393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test whether we can read back the DataFrame from the pickle file\n",
    "df = load_from_pkl(f'/kaggle/working/arcade_new_transformed.pkl')\n",
    "\n",
    "#counf of notebooks\n",
    "print(f\"Number of notebooks: {df['nb_name'].nunique()}\")\n",
    "\n",
    "\n",
    "# Count errors in the processed DataFrame\n",
    "error_count = df['execute_error'].sum()\n",
    "print(f\"Number of notebooks with errors: {error_count}\")\n",
    "\n",
    "# Verify the data loaded correctly\n",
    "print(f\"Loaded DataFrame with shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Check a sample of the code to ensure formatting is preserved\n",
    "if len(df) > 0:\n",
    "    print(\"\\nSample code from first row:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(df.iloc[0]['code'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:44:13.065660Z",
     "iopub.status.busy": "2025-04-10T06:44:13.065199Z",
     "iopub.status.idle": "2025-04-10T06:44:13.071935Z",
     "shell.execute_reply": "2025-04-10T06:44:13.070348Z",
     "shell.execute_reply.started": "2025-04-10T06:44:13.065626Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.nb_setup_code.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:44:47.183155Z",
     "iopub.status.busy": "2025-04-10T06:44:47.182719Z",
     "iopub.status.idle": "2025-04-10T06:44:47.194231Z",
     "shell.execute_reply": "2025-04-10T06:44:47.192878Z",
     "shell.execute_reply.started": "2025-04-10T06:44:47.183123Z"
    }
   },
   "outputs": [],
   "source": [
    "# at which intent do the errors typically start\n",
    "df[df.execute_error == True].intent_number.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:44:54.596099Z",
     "iopub.status.busy": "2025-04-10T06:44:54.595697Z",
     "iopub.status.idle": "2025-04-10T06:44:54.605588Z",
     "shell.execute_reply": "2025-04-10T06:44:54.604393Z",
     "shell.execute_reply.started": "2025-04-10T06:44:54.596068Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df.execute_error == True].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T05:13:37.853342Z",
     "iopub.status.busy": "2025-04-10T05:13:37.852910Z",
     "iopub.status.idle": "2025-04-10T05:13:37.857677Z",
     "shell.execute_reply": "2025-04-10T05:13:37.856318Z",
     "shell.execute_reply.started": "2025-04-10T05:13:37.853305Z"
    }
   },
   "outputs": [],
   "source": [
    "# # part 1\n",
    "# divide_and_process_dataset(\n",
    "    \n",
    "#     n_parts=5,  # Divide into 5 parts\n",
    "#     n_rows_output=10,\n",
    "#     start=0,  \n",
    "#     end=1 \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-08T14:13:06.384637Z",
     "iopub.status.idle": "2025-04-08T14:13:06.384973Z",
     "shell.execute_reply": "2025-04-08T14:13:06.384847Z"
    }
   },
   "outputs": [],
   "source": [
    "# def concatenate_pickles(file_paths, output_path):\n",
    "#     \"\"\"\n",
    "#     Concatenate multiple pickle files into a single DataFrame and save the result.\n",
    "\n",
    "#     Parameters:\n",
    "#     - file_paths (list): List of file paths to the pickle files.\n",
    "#     - output_path (str): Path to save the concatenated DataFrame as a pickle file.\n",
    "#     \"\"\"\n",
    "#     dataframes = []\n",
    "\n",
    "#     # Load each pickle file and append to the list\n",
    "#     for file_path in file_paths:\n",
    "#         print(f\"Loading {file_path}...\")\n",
    "#         df = pd.read_pickle(file_path)\n",
    "#         dataframes.append(df)\n",
    "\n",
    "#     # Concatenate all DataFrames\n",
    "#     concatenated_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "#     # Save the concatenated DataFrame to a new pickle file\n",
    "#     concatenated_df.to_pickle(output_path)\n",
    "#     print(f\"Concatenated DataFrame saved to {output_path}\")\n",
    "\n",
    "#     return concatenated_df\n",
    "\n",
    "# List of pickle files to concatenate\n",
    "# file_paths = [\n",
    "#     '/kaggle/working/arcade_new_transformed_0.pkl',\n",
    "#     '/kaggle/working/arcade_new_transformed_1.pkl',\n",
    "#     '/kaggle/working/arcade_new_transformed_2.pkl',\n",
    "#     '/kaggle/working/arcade_new_transformed_3.pkl',\n",
    "#     '/kaggle/working/arcade_new_transformed_4.pkl'\n",
    "# ]\n",
    "\n",
    "# # Output path for the concatenated DataFrame\n",
    "# output_path = '/kaggle/working/arcade_new_transformed.pkl'\n",
    "\n",
    "# # Concatenate the files\n",
    "# concatenated_df = concatenate_pickles(file_paths, output_path)\n",
    "\n",
    "# # Display the concatenated DataFrame\n",
    "# print(\"Concatenated DataFrame:\")\n",
    "# print(concatenated_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 232527231,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
