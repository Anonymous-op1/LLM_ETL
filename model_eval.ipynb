{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69cb6e",
   "metadata": {
    "_cell_guid": "22b9cc55-7c60-4ca9-b26c-96366791d21e",
    "_uuid": "0beeb073-ffb8-47e0-8eb6-bb3a3d4b89f4",
    "execution": {
     "iopub.execute_input": "2025-05-21T19:43:53.955404Z",
     "iopub.status.busy": "2025-05-21T19:43:53.954932Z",
     "iopub.status.idle": "2025-05-21T19:44:27.938803Z",
     "shell.execute_reply": "2025-05-21T19:44:27.937647Z",
     "shell.execute_reply.started": "2025-05-21T19:43:53.955364Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 23.552,
     "end_time": "2025-04-15T02:52:21.169391",
     "exception": false,
     "start_time": "2025-04-15T02:51:57.617391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade openai anthropic google-generativeai ipywidgets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import pickle\n",
    "import tracemalloc\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "import difflib\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from spider_utils_py import load_csv_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08439c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T02:52:21.178207Z",
     "iopub.status.busy": "2025-04-15T02:52:21.177824Z",
     "iopub.status.idle": "2025-04-15T02:52:21.181027Z",
     "shell.execute_reply": "2025-04-15T02:52:21.180412Z"
    },
    "papermill": {
     "duration": 0.00872,
     "end_time": "2025-04-15T02:52:21.182335",
     "exception": false,
     "start_time": "2025-04-15T02:52:21.173615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model endpoints\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a452131",
   "metadata": {
    "_cell_guid": "5aa1da49-54d1-46e2-ad9f-2d0ce27751c9",
    "_uuid": "a7097a10-aa68-405d-aed4-4497007fc779",
    "execution": {
     "iopub.execute_input": "2025-04-15T02:52:21.190749Z",
     "iopub.status.busy": "2025-04-15T02:52:21.190547Z",
     "iopub.status.idle": "2025-04-15T02:52:21.199852Z",
     "shell.execute_reply": "2025-04-15T02:52:21.199170Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014372,
     "end_time": "2025-04-15T02:52:21.200825",
     "exception": false,
     "start_time": "2025-04-15T02:52:21.186453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_first_n_rows(input_data):\n",
    "    \"\"\"\n",
    "    Processes the input dictionary and creates a DataFrame from the 'first_n_rows' JSON string.\n",
    "\n",
    "    Args:\n",
    "        input_data (dict): A dictionary containing the 'first_n_rows' key with a JSON string.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame created from the 'first_n_rows' JSON string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract the 'first_n_rows' JSON string\n",
    "        first_n_rows_json = input_data.get('first_n_rows', '')\n",
    "\n",
    "        # Fix improperly escaped backslashes\n",
    "        first_n_rows_json = first_n_rows_json.replace(\"\\\\/\", \"/\")  # Fix escaped forward slashes\n",
    "        first_n_rows_json = first_n_rows_json.replace(\"\\\\\", \"\\\\\\\\\")  # Properly escape backslashes\n",
    "        \n",
    "        # Ensure the string is properly escaped for JSON parsing\n",
    "        first_n_rows_json = first_n_rows_json.replace('None', 'null')  # Replace Python None with JSON null\n",
    "\n",
    "        # Parse the JSON string into a Python object\n",
    "        first_n_rows_data = json.loads(first_n_rows_json)\n",
    "\n",
    "        # Convert the parsed data into a pandas DataFrame\n",
    "        first_n_rows_df = pd.DataFrame(first_n_rows_data)\n",
    "\n",
    "        return first_n_rows_df\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Error processing 'first_n_rows': {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of an error\n",
    "def clean_code_markers(code_string):\n",
    "    \"\"\"\n",
    "    Removes ```python and ``` markers from a code string.\n",
    "    \n",
    "    Args:\n",
    "        code_string (str): The input string containing code with markers\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned code without the markers\n",
    "    \"\"\"\n",
    "    # Remove ```python at the start (with optional whitespace)\n",
    "    cleaned = code_string.replace('```python', '').strip()\n",
    "    \n",
    "    # Remove ``` at the end (with optional whitespace)\n",
    "    cleaned = cleaned.replace('```', '').strip()\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def execute_intent_code(exec_state, code, num_output_rows=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Executes the given code in the provided execution state.\n",
    "    Returns the updated execution state and any outputs, capturing only primitive types, tuples, \n",
    "    and DataFrames (DataFrames are stored in JSON format).\n",
    "\n",
    "    exec_state: python exec namespace\n",
    "\n",
    "    examples:\n",
    "\n",
    "        for executing notebook header:\n",
    "        \n",
    "            first_n_rows = pd.DataFrame(eval(eval(intents.iloc[0][INPUT_DATA_COL].replace('null', 'None'))['first_n_rows']))\n",
    "            exec_state = {\"pd\": pd, \"first_n_rows\": first_n_rows}  # Initialize execution state\n",
    "            try:\n",
    "                outputs, exec_state = execute_intent_code(exec_state, nb_header, verbose=False)\n",
    "                inputs = outputs  # Initialize inputs with the header execution outputs\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing notebook header for {nb_name}: {e}\")\n",
    "                continue  # Skip this notebook if the header fails\n",
    "\n",
    "        for executing intent code (note exec_state would have been previously modified from previous intent code execution)\n",
    "            # Execute original code\n",
    "            try:\n",
    "                print(\"Executing original code...\")\n",
    "                original_outputs, exec_state = execute_intent_code(exec_state, actual_code, verbose=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing original code: {e}\")\n",
    "                original_outputs = {}\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use a non-interactive backend for matplotlib to suppress plots\n",
    "        plt.switch_backend('Agg')\n",
    "\n",
    "        if verbose:\n",
    "            print(\"IN STATE\")\n",
    "            print(exec_state)\n",
    "            print(\"CODE\")\n",
    "            print(code)\n",
    "\n",
    "        # Execute the code in the provided execution state\n",
    "        exec(code, exec_state)\n",
    "\n",
    "        # Clear any matplotlib figures created during execution\n",
    "        plt.close('all')\n",
    "        \n",
    "        # Capture the outputs (all variables in the execution state)\n",
    "        outputs = {}\n",
    "        for key, value in exec_state.items():\n",
    "            if not key.startswith(\"__\"):\n",
    "                if isinstance(value, (int, float, str, bool, tuple)):\n",
    "                    outputs[key] = value\n",
    "                elif isinstance(value, pd.DataFrame):\n",
    "                    # Convert DataFrame to JSON format\n",
    "                    outputs[key] = str(value.head(num_output_rows).to_json(orient=\"records\"))\n",
    "                elif isinstance(value, pd.Series):\n",
    "                    # Convert Series to JSON format\n",
    "                    outputs[key] = str(value.head(num_output_rows).to_json())\n",
    "\n",
    "        if verbose:\n",
    "            print(\"OUT STATE\")\n",
    "            print(exec_state)\n",
    "            print(\"OUTPUTS\")\n",
    "            print(outputs)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in executing code: \", e)\n",
    "        outputs = {\"error\": str(e)}\n",
    "    \n",
    "    return outputs, exec_state\n",
    "\n",
    "def load_from_pkl(file_path):\n",
    "    # Load the pickled data (list of dictionaries)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        extracted_data = pickle.load(f)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df_loaded = pd.DataFrame(extracted_data)\n",
    "    \n",
    "    # Verify the data loaded correctly\n",
    "    print(f\"Loaded DataFrame with shape: {df_loaded.shape}\")\n",
    "    print(f\"Columns: {df_loaded.columns.tolist()}\")\n",
    "    \n",
    "    # Check a sample of the code to ensure formatting is preserved\n",
    "    if len(df_loaded) > 0:\n",
    "        print(\"\\nSample code from first row:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(df_loaded.iloc[0]['code'])\n",
    "        print(\"-\" * 50)\n",
    "    return df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69009fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T02:52:21.210013Z",
     "iopub.status.busy": "2025-04-15T02:52:21.209741Z",
     "iopub.status.idle": "2025-04-15T02:52:21.397273Z",
     "shell.execute_reply": "2025-04-15T02:52:21.396525Z"
    },
    "papermill": {
     "duration": 0.193961,
     "end_time": "2025-04-15T02:52:21.398728",
     "exception": false,
     "start_time": "2025-04-15T02:52:21.204767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model endpoints\n",
    "# OpenAI\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Anthropic\n",
    "anthropic_client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Gemini\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797260dd",
   "metadata": {
    "_cell_guid": "5aa1da49-54d1-46e2-ad9f-2d0ce27751c9",
    "_uuid": "a7097a10-aa68-405d-aed4-4497007fc779",
    "execution": {
     "iopub.execute_input": "2025-04-15T02:52:21.406910Z",
     "iopub.status.busy": "2025-04-15T02:52:21.406681Z",
     "iopub.status.idle": "2025-04-15T02:52:21.414883Z",
     "shell.execute_reply": "2025-04-15T02:52:21.414244Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013386,
     "end_time": "2025-04-15T02:52:21.416030",
     "exception": false,
     "start_time": "2025-04-15T02:52:21.402644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_llm(provider, prompt, model=None, temperature=0.0, max_tokens=512):\n",
    "    try:\n",
    "        if provider.lower() == \"openai\":\n",
    "            client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "            response = client.chat.completions.create(\n",
    "                    model=model or \"gpt-4o-mini\",\n",
    "                    store=False,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens\n",
    "                )\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        elif provider.lower() == \"anthropic\":\n",
    "            response = anthropic_client.messages.create(\n",
    "                model=model or \"claude-3-5-haiku-20241022\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            return response.content[0].text\n",
    "\n",
    "        elif provider.lower() == \"google\":\n",
    "            gemini_model = genai.GenerativeModel(\n",
    "                model_name=model or \"gemini-2.0-flash\",\n",
    "                generation_config={\n",
    "                    \"temperature\": temperature,\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"top_k\": 40,\n",
    "                    \"max_output_tokens\": max_tokens,\n",
    "                    \"response_mime_type\": \"text/plain\",\n",
    "                }\n",
    "            )\n",
    "            response = gemini_model.generate_content(prompt)\n",
    "            time.sleep(2)\n",
    "            return response.text\n",
    "\n",
    "        else:\n",
    "            return \"Unknown provider.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "def chat_gpt_completion(model = \"gpt-4o-mini\", \n",
    "             role = \"developer\",\n",
    "            prompt = \"Transform Input to get Output. Input: (A, 1) (A, 2) Output: (A, 1, 2)\",\n",
    "            temp=None):    \n",
    "    # Set your API key\n",
    "    API_KEY = \"API_KEY\"\n",
    "    print(\"Prompt:\")\n",
    "    print(prompt)\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "    if temp is None:\n",
    "        completion = client.chat.completions.create(\n",
    "          model=model,\n",
    "          store=False,\n",
    "          messages=[{\"role\": role, \"content\": prompt}])\n",
    "    else:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            store=False,\n",
    "            messages=[{\"role\": role, \"content\": prompt}],\n",
    "            temperature=temp)\n",
    "        \n",
    "    response = completion.choices[0].message.content\n",
    "    print(\"Response:\")\n",
    "    print(response)\n",
    "    print()\n",
    "    return response\n",
    "\n",
    "def deepseek_completion(\n",
    "    model = \"deepseek-chat\", \n",
    "    role = \"system\",\n",
    "    prompt = \"Transform Input to get Output. Input: (A, 1) (A, 2) Output: (A, 1, 2)\",\n",
    "    temp=None):\n",
    "    API_KEY = 'api_key'\n",
    "    deepseek_client = OpenAI(\n",
    "        api_key=API_KEY,\n",
    "        base_url=\"https://api.deepseek.com/v1\"\n",
    "    )\n",
    "    if temp is None:\n",
    "        completion = deepseek_client.chat.completions.create(\n",
    "            model=model,\n",
    "            store=False,\n",
    "             messages=[{\"role\": role, \"content\": prompt}])\n",
    "    else:\n",
    "        completion = deepseek_client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            store=False,\n",
    "             messages=[{\"role\": role, \"content\": prompt}],\n",
    "            temperature=temp)\n",
    "    response = completion.choices[0].message.content\n",
    "    print(\"Response:\")\n",
    "    print(response)\n",
    "    print()\n",
    "    return response\n",
    "\n",
    "def code_llama_completion():\n",
    "    pass\n",
    "\n",
    "def github_copilot_completion():\n",
    "    pass\n",
    "\n",
    "def code_llama_tuned_completion():\n",
    "    pass\n",
    "\n",
    "\n",
    "def code_llama_tuned_with_cot_iterative_completion():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a0a81",
   "metadata": {
    "_cell_guid": "f0512d36-0e11-4314-96fe-045e882f7a84",
    "_uuid": "fecc5d1a-5ab5-4adc-b440-8a711780d7ae",
    "execution": {
     "iopub.execute_input": "2025-04-15T02:52:21.425110Z",
     "iopub.status.busy": "2025-04-15T02:52:21.424864Z",
     "iopub.status.idle": "2025-04-15T02:52:21.428580Z",
     "shell.execute_reply": "2025-04-15T02:52:21.427809Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009219,
     "end_time": "2025-04-15T02:52:21.429578",
     "exception": false,
     "start_time": "2025-04-15T02:52:21.420359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASELINE_MODELS = {'chat_gpt': chat_gpt_completion, \n",
    "                   'deepseek_chat': deepseek_completion}\n",
    "\n",
    "TRANSFORMATION_PATTERNS = [\n",
    "        r'\\.merge\\(', r'\\.join\\(', r'\\.filter\\(', r'\\.groupby\\(',\n",
    "        r'\\.concat\\(', r'\\.drop\\(', r'\\.loc\\[',\n",
    "        r'\\.iloc\\[', r'\\.assign\\(', r'\\.rename\\(',\n",
    "        r'\\.pivot\\(', r'\\.melt\\(', r'\\.sort_values\\(',\n",
    "        r'\\.reset_index\\(', r'\\.set_index\\(', r'\\.query\\(',\n",
    "        r'\\.drop_duplicates\\(', r'\\.fillna\\(', r'\\.replace\\('\n",
    "    ]\n",
    "\n",
    "\n",
    "MODEL_COL = 'model'\n",
    "ORIGINAL_NOTEBOOK_COL = 'nb_source'\n",
    "ACTUAL_CODE_COL = 'code'\n",
    "INPUT_DATA_COL = 'inputs'\n",
    "OUTPUT_DATA_COL = 'outputs'\n",
    "TRANSFORMATION_DESCRIPTION_COL = 'intent'\n",
    "GENERATED_CODE_COL = 'gen_code'\n",
    "GENERATED_INPUT_DATA_COL = 'gen_inputs'\n",
    "GENERATED_OUTPUT_DATA_COL = 'gen_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c071ec",
   "metadata": {
    "_cell_guid": "05908533-e26d-4ed5-a849-ca823851a5d3",
    "_uuid": "926cdd35-e40f-4e2f-bcdd-0ed8b929ebfc",
    "execution": {
     "iopub.execute_input": "2025-04-15T02:52:21.437921Z",
     "iopub.status.busy": "2025-04-15T02:52:21.437710Z",
     "iopub.status.idle": "2025-04-15T02:52:21.456054Z",
     "shell.execute_reply": "2025-04-15T02:52:21.455295Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023657,
     "end_time": "2025-04-15T02:52:21.457193",
     "exception": false,
     "start_time": "2025-04-15T02:52:21.433536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stub function to invoke an LLM (to be replaced with actual model call)\n",
    "def invoke_llm(model_completion_fn, description):\n",
    "    \"\"\"\n",
    "    Calls a given LLM to generate code\n",
    "    \"\"\"\n",
    "    return model_completion_fn(prompt = f\"{description} \\nOutput only Python code. Write the first 10 rows of the final resulting dataframe to a variable named 'output' using to_dict(orient='records').\")\n",
    "\n",
    "\n",
    "def filter_comments(code):\n",
    "    \"\"\"Remove comments from Python code.\"\"\"\n",
    "    return '\\n'.join([line for line in code.split('\\n') if not line.strip().startswith('#')])\n",
    "\n",
    "def compare_code(generated_code, actual_code):\n",
    "    '''Compares code using the diff library similar to version control'''\n",
    "    generated_code = filter_comments(generated_code)\n",
    "    actual_code = filter_comments(actual_code)\n",
    "    \n",
    "    generated_lines = generated_code.split('\\n')\n",
    "    actual_lines = actual_code.split('\\n')\n",
    "\n",
    "    diff = difflib.unified_diff(\n",
    "        generated_lines, actual_lines,\n",
    "        fromfile='Generated Code', tofile='Actual Code',\n",
    "        lineterm=''\n",
    "    )\n",
    "    \n",
    "    for line in diff:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            print(f'\\033[1;34m{line}\\033[0m')  # Blue header\n",
    "        elif line.startswith('-'):\n",
    "            print(f'\\033[1;31m{line}\\033[0m')  # Red for deletions\n",
    "        elif line.startswith('+'):\n",
    "            print(f'\\033[1;32m{line}\\033[0m')  # Green for additions\n",
    "        else:\n",
    "            print(line)  # Normal text\n",
    "\n",
    "def clean_up_llm_gen_code(generated_code):\n",
    "    '''Applies clean up to code generate by LLMs eg Markdown code ``` delimiters'''\n",
    "    generated_code = generated_code.str.removeprefix(\"```python\").str.lstrip()\n",
    "    generated_code = generated_code.str.removesuffix(\"```\").str.rstrip()\n",
    "    return generated_code\n",
    "\n",
    "# Function to calculate BLEU-3 score using simple n-gram matching\n",
    "def calculate_bleu_3(reference, candidate):\n",
    "    \"\"\"\n",
    "    Computes a simple BLEU-3-like score based on n-gram overlap.\n",
    "    \"\"\"\n",
    "    def get_ngrams(text, n=3):\n",
    "        tokens = text.split()\n",
    "        return set([\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
    "\n",
    "    ref_ngrams = get_ngrams(reference, 3)\n",
    "    cand_ngrams = get_ngrams(candidate, 3)\n",
    "\n",
    "    if not ref_ngrams or not cand_ngrams:\n",
    "        return 0.0\n",
    "\n",
    "    overlap = len(ref_ngrams.intersection(cand_ngrams))\n",
    "    return overlap / len(ref_ngrams)\n",
    "\n",
    "def calculate_bleu_3_trans(trans1, trans2):\n",
    "    \"\"\"\n",
    "    Computes the BLEU-3 score for the transformation sequences.\n",
    "    Uses up to trigrams (n=3) for measuring sequence similarity.\n",
    "    \"\"\"\n",
    "    if not trans1 or not trans2:  # Avoid BLEU error when input is empty\n",
    "        return 0.0\n",
    "\n",
    "    smooth_fn = SmoothingFunction().method1  # Smoothing to avoid zero scores\n",
    "\n",
    "    return sentence_bleu(\n",
    "        [trans1],  # Reference sequence (list of lists)\n",
    "        trans2,  # Candidate sequence\n",
    "        weights=(0.33, 0.33, 0.33),  # BLEU-3: Trigram weighting\n",
    "        smoothing_function=smooth_fn)\n",
    "\n",
    "\n",
    "# function to calculate percentage structure and data correctness\n",
    "def compare_dataframes(df_actual, df_gen):\n",
    "\n",
    "    if (df_gen is None) or (df_actual is None):\n",
    "        return (0.0, 0.0)\n",
    "    \n",
    "    # Structural similarity: Check matching columns\n",
    "    common_columns = set(df_gen.columns).intersection(set(df_actual.columns))\n",
    "    structure_score = len(common_columns) / max(len(df_gen.columns), len(df_actual.columns))\n",
    "    \n",
    "    # Align data by reindexing to the same number of rows\n",
    "    max_rows = max(len(df_gen), len(df_actual))\n",
    "    df_gen = df_gen.reindex(range(max_rows))\n",
    "    df_actual = df_actual.reindex(range(max_rows))\n",
    "    \n",
    "    # Content correctness: Count exact matches including missing values\n",
    "    exact_matches = 0\n",
    "    total_values = 0\n",
    "    \n",
    "    for col in common_columns:\n",
    "        matches = (df_gen[col] == df_actual[col]) & df_gen[col].notna() & df_actual[col].notna()\n",
    "        exact_matches += matches.sum()\n",
    "        total_values += max(len(df_gen[col].dropna()), len(df_actual[col].dropna()))\n",
    "    \n",
    "    content_score = exact_matches / total_values if total_values > 0 else 0\n",
    "    \n",
    "    return (structure_score, content_score)\n",
    "\n",
    "# Function to evaluate performance\n",
    "def measure_performance(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Measures execution time and memory usage of a function.\n",
    "    \"\"\"\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_memory = process.memory_info().rss\n",
    "\n",
    "    result = func(*args, **kwargs)\n",
    "\n",
    "    end_time = time.time()\n",
    "    peak_memory = tracemalloc.get_traced_memory()[1]  # Peak memory usage\n",
    "    end_memory = process.memory_info().rss\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    avg_memory = (start_memory + end_memory) / 2\n",
    "\n",
    "    return result, elapsed_time, peak_memory, avg_memory\n",
    "\n",
    "def extract_transformations_from_code(code):\n",
    "    \"\"\"\n",
    "    Extracts pandas transformation operations from a Python script string.\n",
    "    \"\"\"\n",
    "    transformation_patterns = TRANSFORMATION_PATTERNS\n",
    "\n",
    "    transformations = []\n",
    "    for pattern in transformation_patterns:\n",
    "        matches = re.findall(pattern, code)\n",
    "        transformations.extend(matches)\n",
    "\n",
    "    return transformations\n",
    "\n",
    "def count_unique_matches(trans1, trans2):\n",
    "    \"\"\"\n",
    "    Counts unique matches for common transformations between two lists.\n",
    "    Ensures that a transformation in one list is matched only once to another.\n",
    "    \"\"\"\n",
    "    trans1_counts = Counter(trans1)\n",
    "    trans2_counts = Counter(trans2)\n",
    "\n",
    "    unique_match_count = 0\n",
    "\n",
    "    for transformation in trans1_counts:\n",
    "        if transformation in trans2_counts:\n",
    "            # Match only the minimum occurrences in both lists (ensuring unique matches)\n",
    "            unique_match_count += min(trans1_counts[transformation], trans2_counts[transformation])\n",
    "\n",
    "    return unique_match_count\n",
    "\n",
    "def compare_python_scripts(code1, code2):\n",
    "    \"\"\"\n",
    "    Compares two Python scripts based on common data transformations\n",
    "    and their sequence similarity.\n",
    "    \"\"\"\n",
    "    trans1 = extract_transformations_from_code(code1)\n",
    "    trans2 = extract_transformations_from_code(code2)\n",
    "\n",
    "    # Count unique transformation matches\n",
    "    unique_match_count = count_unique_matches(trans1, trans2)\n",
    "\n",
    "    # Check sequence similarity\n",
    "    seq_matcher = SequenceMatcher(None, trans1, trans2)\n",
    "    sequence_similarity = seq_matcher.ratio()  # Between 0 and 1\n",
    "\n",
    "    return {\n",
    "        \"unique_common_transformation_count\": unique_match_count,\n",
    "        \"total_trans_code1\": len(trans1),\n",
    "        \"total_trans_code2\": len(trans2),\n",
    "        \"sequence_similarity\": sequence_similarity,\n",
    "        \"trans_code1\": trans1,\n",
    "        \"trans_code2\": trans2,\n",
    "        \"trans_bleu3\" : calculate_bleu_3_trans(trans1, trans2)       \n",
    "    }\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_completions(test_df):\n",
    "    \"\"\"\n",
    "    Evaluates the LLM-generated code against expected outputs.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        model = row[MODEL_COL]\n",
    "        input_data = row[INPUT_DATA_COL]\n",
    "        description = row[TRANSFORMATION_DESCRIPTION_COL]\n",
    "        actual_code = filter_comments(row[ACTUAL_CODE_COL])\n",
    "        output_data = row[OUTPUT_DATA_COL]\n",
    "        generated_code = filter_comments(row[GENERATED_CODE_COL])\n",
    "        \n",
    "        # Compute BLEU-3 score for overall code correctness\n",
    "        bleu_score = calculate_bleu_3(actual_code, generated_code)\n",
    "\n",
    "        # Execute generated code and capture output\n",
    "        exec_globals = {}\n",
    "        try:\n",
    "            print(\"Executing: \", generated_code)\n",
    "            result, elapsed_time, peak_memory, avg_memory = measure_performance(\n",
    "                exec, generated_code, exec_globals)\n",
    "            generated_output = exec_globals.get(\"output\", None) # Assuming \"output\" variable holds the result\n",
    "            print(\"Generated output: \", generated_output)\n",
    "            #generated_output_list = ast.literal_eval(generated_output)\n",
    "            generated_output_df = pd.DataFrame(generated_output)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            generated_output_df = None\n",
    "            elapsed_time, peak_memory, avg_memory = None, None, None\n",
    "\n",
    "        # TODO: bind the original code to input and execute\n",
    "        \n",
    "        # Check output correctness\n",
    "        print(\"Actual output:\")\n",
    "        print(output_data)\n",
    "        # Convert string to list of dictionaries safely\n",
    "        output_data_list = ast.literal_eval(output_data)\n",
    "        output_df = pd.DataFrame(output_data_list)\n",
    "        #output_correctness = check_output_correctness(output_df, generated_output_df)\n",
    "        output_structure_score, output_data_score = compare_dataframes(output_df, generated_output_df)\n",
    "\n",
    "        # Compare transformations\n",
    "        trans_metrics = compare_python_scripts(row[GENERATED_CODE_COL], \n",
    "                               row[ACTUAL_CODE_COL])\n",
    "        unique_match_count = trans_metrics[\"unique_common_transformation_count\"]\n",
    "        trans_count_generated = trans_metrics[\"total_trans_code1\"]\n",
    "        trans_count_actual = trans_metrics[\"total_trans_code2\"]\n",
    "        sequence_similarity = trans_metrics[\"sequence_similarity\"]\n",
    "        trans_generated = trans_metrics[\"trans_code1\"]\n",
    "        trans_actual = trans_metrics[\"trans_code2\"]        \n",
    "        trans_bleu_3 = trans_metrics[\"trans_bleu3\"]\n",
    "        \n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"actual_code\": actual_code,\n",
    "            \"generated_code\": generated_code,\n",
    "            \"output_data\": output_data,\n",
    "            \"generated_output\": generated_output,\n",
    "            \"bleu_3_exact_code\": bleu_score,\n",
    "            \"count_common_trans\": unique_match_count,\n",
    "            \"count_trans_generated\": trans_count_generated,\n",
    "            \"count_trans_actual\": trans_count_actual,\n",
    "            \"bleu_3_trans\": trans_bleu_3,\n",
    "            \"trans_sequence_similarity\": sequence_similarity,\n",
    "            \"trans_generated\": trans_generated,\n",
    "            \"trans_actual\": trans_actual,\n",
    "            \"output_structure_score\": output_structure_score,\n",
    "            'output_data_score': output_data_score,\n",
    "            \"elapsed_time\": elapsed_time,\n",
    "            \"peak_memory\": peak_memory,\n",
    "            \"avg_memory\": avg_memory\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def run_models(test_df, models=BASELINE_MODELS):\n",
    "    '''Generates the transformations for a test dataset for list of models'''\n",
    "    # Initialize an empty DataFrame to store all predictions\n",
    "    all_preds = pd.DataFrame()\n",
    "    \n",
    "    # Generate predictions using each model endpoint\n",
    "    for m, ep in tqdm(models.items()):\n",
    "        # Create a temporary DataFrame to store predictions for the current model\n",
    "        temp_df = test_df.copy()\n",
    "        temp_df[GENERATED_CODE_COL] = temp_df.progress_apply(\n",
    "            lambda x: invoke_llm(ep, x[TRANSFORMATION_DESCRIPTION_COL]), axis=1)\n",
    "\n",
    "        # clean up the gen code\n",
    "        temp_df.loc[:,GENERATED_CODE_COL] = clean_up_llm_gen_code(temp_df[GENERATED_CODE_COL]) \n",
    "\n",
    "        # record model used for these predictions\n",
    "        temp_df[MODEL_COL] = m\n",
    "        \n",
    "        # Concatenate the temporary DataFrame to all_preds\n",
    "        all_preds = pd.concat([all_preds, temp_df], axis=0)\n",
    "    \n",
    "    # Assign the concatenated predictions back to test_df\n",
    "    test_df = all_preds.reset_index(drop=True)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac150d9a",
   "metadata": {
    "_cell_guid": "38d25491-8959-4088-8dcc-83eaaea5d47f",
    "_uuid": "586dd995-3e39-402a-add8-35714fcadffb",
    "execution": {
     "iopub.execute_input": "2025-04-15T02:52:21.465364Z",
     "iopub.status.busy": "2025-04-15T02:52:21.465124Z",
     "iopub.status.idle": "2025-04-15T02:52:21.468258Z",
     "shell.execute_reply": "2025-04-15T02:52:21.467616Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.008124,
     "end_time": "2025-04-15T02:52:21.469201",
     "exception": false,
     "start_time": "2025-04-15T02:52:21.461077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Temporary utility functions\n",
    "# # can be removed after we fully integrate the pipeline\n",
    "# import json\n",
    "# import ast\n",
    "\n",
    "# def extract_markdown_cells(notebook_json):\n",
    "#     \"\"\"\n",
    "#     Extracts and combines all markdown cells from a Jupyter Notebook JSON into a single string.\n",
    "    \n",
    "#     Parameters:\n",
    "#         notebook_json (dict or str): The JSON content of the notebook as a dictionary or string.\n",
    "    \n",
    "#     Returns:\n",
    "#         str: A single string containing all markdown cells combined.\n",
    "#     \"\"\"\n",
    "#     # Load JSON if it's a string\n",
    "#     if isinstance(notebook_json, str):\n",
    "#         notebook_json = json.loads(notebook_json)\n",
    "    \n",
    "#     # Extract markdown cells\n",
    "#     markdown_cells = [\n",
    "#         \"\\n\".join(cell[\"source\"]) for cell in notebook_json.get(\"cells\", []) if cell.get(\"cell_type\") == \"markdown\"\n",
    "#     ]\n",
    "    \n",
    "#     # Combine all markdown content into a single string\n",
    "#     return \"\\n\\n\".join(markdown_cells)\n",
    "\n",
    "# def process_notebook_source(nb_source: str) -> str:\n",
    "#     \"\"\"Extract and merge code cells from nb_source\"\"\"\n",
    "#     try:\n",
    "#         parsed = json.loads(nb_source)\n",
    "#     except (json.JSONDecodeError, TypeError):\n",
    "#         try:\n",
    "#             parsed = ast.literal_eval(nb_source)\n",
    "#         except (ValueError, SyntaxError) as e:\n",
    "#             print(f\"Error parsing nb_source: {e}\")\n",
    "#             return []\n",
    "\n",
    "#     if isinstance(parsed, dict) and 'cells' in parsed:\n",
    "#         cells = parsed['cells']\n",
    "#     elif isinstance(parsed, list):\n",
    "#         cells = parsed\n",
    "#     else:\n",
    "#         print(f\"nb_source is neither a list nor a dict with 'cells': {parsed}\")\n",
    "#         return []\n",
    "\n",
    "#     if not isinstance(cells, list):\n",
    "#         print(f\"cells is not a list: {cells}\")\n",
    "#         return []\n",
    "\n",
    "#     # Merge all code cells into one string, preserving original formatting\n",
    "#     merged_code = []\n",
    "#     for cell in cells:\n",
    "#         if isinstance(cell, dict) and cell.get('cell_type') == 'code':\n",
    "#             code = cell.get('source', '')\n",
    "#             if isinstance(code, list):\n",
    "#                 code = '\\n'.join(code)  # Join lines from source, preserving indentation\n",
    "#             elif not isinstance(code, str):\n",
    "#                 code = str(code)\n",
    "#             merged_code.append(code)\n",
    "    \n",
    "#     full_code = '\\n'.join(merged_code)\n",
    "#     return full_code"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6769266,
     "sourceId": 11256735,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 229490371,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.85403,
   "end_time": "2025-04-15T02:52:22.492945",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-15T02:51:55.638915",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
