{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7294ae7d-0ccc-4ced-9471-8d9e9466c50d",
    "_uuid": "d2b8be80-b5d0-49cc-bd29-db029770264b",
    "execution": {
     "iopub.execute_input": "2025-04-11T17:31:55.597252Z",
     "iopub.status.busy": "2025-04-11T17:31:55.596858Z",
     "iopub.status.idle": "2025-04-11T17:31:56.455003Z",
     "shell.execute_reply": "2025-04-11T17:31:56.452407Z",
     "shell.execute_reply.started": "2025-04-11T17:31:55.597212Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "#from model_eval import *  #this is causing library conflicts with arcade\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:31:56.456802Z",
     "iopub.status.busy": "2025-04-11T17:31:56.456315Z",
     "iopub.status.idle": "2025-04-11T17:31:56.904375Z",
     "shell.execute_reply": "2025-04-11T17:31:56.903285Z",
     "shell.execute_reply.started": "2025-04-11T17:31:56.456772Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = ''\n",
    "os.environ['KAGGLE_KEY'] = ''\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Authenticate with Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install ARCADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:31:56.905893Z",
     "iopub.status.busy": "2025-04-11T17:31:56.905522Z",
     "iopub.status.idle": "2025-04-11T17:31:57.875352Z",
     "shell.execute_reply": "2025-04-11T17:31:57.874032Z",
     "shell.execute_reply.started": "2025-04-11T17:31:56.905857Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/google-research/arcade-nl2code.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:31:57.878771Z",
     "iopub.status.busy": "2025-04-11T17:31:57.878454Z",
     "iopub.status.idle": "2025-04-11T17:31:57.899409Z",
     "shell.execute_reply": "2025-04-11T17:31:57.898371Z",
     "shell.execute_reply.started": "2025-04-11T17:31:57.878743Z"
    }
   },
   "outputs": [],
   "source": [
    "# createa a package out of arcade and install it\n",
    "setup_content = \"\"\"\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='arcade_nl2code',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    install_requires=[\n",
    "        'tensorflow',  # Add other dependencies here\n",
    "    ],\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "with open('/kaggle/working/arcade-nl2code/setup.py', 'w') as file:\n",
    "    file.write(setup_content)\n",
    "\n",
    "def create_init_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for dir_name in dirs:\n",
    "            init_file_path = os.path.join(root, dir_name, '__init__.py')\n",
    "            if not os.path.exists(init_file_path):\n",
    "                with open(init_file_path, 'w') as f:\n",
    "                    f.write(\"# This file makes the directory a Python package\\n\")\n",
    "                print(f\"Created: {init_file_path}\")\n",
    "\n",
    "directory = '//kaggle/working/arcade-nl2code'\n",
    "create_init_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:31:57.901991Z",
     "iopub.status.busy": "2025-04-11T17:31:57.901555Z",
     "iopub.status.idle": "2025-04-11T17:32:05.851659Z",
     "shell.execute_reply": "2025-04-11T17:32:05.850397Z",
     "shell.execute_reply.started": "2025-04-11T17:31:57.901922Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/working/arcade-nl2code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:32:05.853378Z",
     "iopub.status.busy": "2025-04-11T17:32:05.852977Z",
     "iopub.status.idle": "2025-04-11T17:32:05.860786Z",
     "shell.execute_reply": "2025-04-11T17:32:05.859345Z",
     "shell.execute_reply.started": "2025-04-11T17:32:05.853338Z"
    }
   },
   "outputs": [],
   "source": [
    "# add the package to python path\n",
    "sys.path.append('/kaggle/working/arcade-nl2code')\n",
    "sys.path.append('/kaggle/working/arcade-nl2code/arcade_nl2code')\n",
    "sys.path.append('/kaggle/working/arcade-nl2code/arcade_nl2code/annotated_dataset')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:32:05.862311Z",
     "iopub.status.busy": "2025-04-11T17:32:05.861925Z",
     "iopub.status.idle": "2025-04-11T17:32:05.884094Z",
     "shell.execute_reply": "2025-04-11T17:32:05.882869Z",
     "shell.execute_reply.started": "2025-04-11T17:32:05.862272Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a requirements file for possible versions \n",
    "reqs_2022 = \"\"\"\n",
    "tensorflow-cpu==2.10.0\n",
    "absl-py==1.3.0\n",
    "pandas==1.5.2\n",
    "dacite==1.7.0\n",
    "nbformat==5.7.0\n",
    "dill==0.3.6\n",
    "sacrebleu==2.3.1\n",
    "astor==0.8.1\n",
    "folium==0.12.1\n",
    "seaborn==0.12.2\n",
    "vega==3.5.0\n",
    "bokeh==2.4.3\n",
    "plotly==5.10.0\n",
    "matplotlib==3.6.2\n",
    "chart_studio==1.1.0\n",
    "\"\"\"\n",
    "\n",
    "with open('/kaggle/working/arcade-nl2code/requirements_2022.txt', 'w') as file:\n",
    "    file.write(reqs_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:32:05.885673Z",
     "iopub.status.busy": "2025-04-11T17:32:05.885299Z",
     "iopub.status.idle": "2025-04-11T17:34:12.242127Z",
     "shell.execute_reply": "2025-04-11T17:34:12.241071Z",
     "shell.execute_reply.started": "2025-04-11T17:32:05.885634Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -r /kaggle/working/arcade-nl2code/arcade_nl2code/evaluation/requirements.txt\n",
    "!pip install -r /kaggle/working/arcade-nl2code/requirements_2022.txt\n",
    "!pip install seqio\n",
    "!pip install diff_match_patch  # was missing in requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:34:12.243608Z",
     "iopub.status.busy": "2025-04-11T17:34:12.243331Z",
     "iopub.status.idle": "2025-04-11T17:34:19.606341Z",
     "shell.execute_reply": "2025-04-11T17:34:19.605264Z",
     "shell.execute_reply.started": "2025-04-11T17:34:12.243583Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip show tensorflow\n",
    "pip show tensorflow-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download ARCADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:34:19.607873Z",
     "iopub.status.busy": "2025-04-11T17:34:19.607483Z",
     "iopub.status.idle": "2025-04-11T17:34:21.713177Z",
     "shell.execute_reply": "2025-04-11T17:34:21.711903Z",
     "shell.execute_reply.started": "2025-04-11T17:34:19.607821Z"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d googleai/arcade-nl2code-dataset -p arcade_nl2code/annotated_dataset/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:34:21.714804Z",
     "iopub.status.busy": "2025-04-11T17:34:21.714399Z",
     "iopub.status.idle": "2025-04-11T17:34:22.444382Z",
     "shell.execute_reply": "2025-04-11T17:34:22.443212Z",
     "shell.execute_reply.started": "2025-04-11T17:34:21.714761Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/arcade_nl2code/annotated_dataset/dataset\n",
    "!unzip -o arcade-nl2code-dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:34:22.446172Z",
     "iopub.status.busy": "2025-04-11T17:34:22.445832Z",
     "iopub.status.idle": "2025-04-11T17:36:01.116608Z",
     "shell.execute_reply": "2025-04-11T17:36:01.114861Z",
     "shell.execute_reply.started": "2025-04-11T17:34:22.446140Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /kaggle/working/arcade_nl2code/annotated_dataset\n",
    "PYTHONPATH=../../ \n",
    "python /kaggle/working/arcade-nl2code/arcade_nl2code/annotated_dataset/build_existing_tasks_split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a zip of source datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:10:31.994599Z",
     "iopub.status.busy": "2025-04-11T16:10:31.994016Z",
     "iopub.status.idle": "2025-04-11T16:11:18.650392Z",
     "shell.execute_reply": "2025-04-11T16:11:18.648842Z",
     "shell.execute_reply.started": "2025-04-11T16:10:31.994523Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def create_zip_of_folder(folder_path, output_zip_path):\n",
    "    \"\"\"\n",
    "    Create a zip file of the specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder to be zipped.\n",
    "    - output_zip_path (str): Path where the zip file will be created (without .zip extension).\n",
    "    \"\"\"\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Error: Folder {folder_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Create the zip file\n",
    "    shutil.make_archive(output_zip_path, 'zip', folder_path)\n",
    "    print(f\"Zip file created at: {output_zip_path}.zip\")\n",
    "\n",
    "# Example usage\n",
    "folder_to_zip = \"/kaggle/working/arcade_nl2code/annotated_dataset/dataset/existing_tasks/artifacts\"\n",
    "output_zip = \"/kaggle/working/arcade_existing_datasets\"\n",
    "\n",
    "create_zip_of_folder(folder_to_zip, output_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:36:01.122428Z",
     "iopub.status.busy": "2025-04-11T17:36:01.122027Z",
     "iopub.status.idle": "2025-04-11T17:36:01.128420Z",
     "shell.execute_reply": "2025-04-11T17:36:01.127035Z",
     "shell.execute_reply.started": "2025-04-11T17:36:01.122396Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install --force-reinstall pandas==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:37:22.547288Z",
     "iopub.status.busy": "2025-04-11T17:37:22.546819Z",
     "iopub.status.idle": "2025-04-11T17:37:42.399076Z",
     "shell.execute_reply": "2025-04-11T17:37:42.397976Z",
     "shell.execute_reply.started": "2025-04-11T17:37:22.547253Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade --force-reinstall pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:38:00.018238Z",
     "iopub.status.busy": "2025-04-11T17:38:00.017814Z",
     "iopub.status.idle": "2025-04-11T17:38:00.036422Z",
     "shell.execute_reply": "2025-04-11T17:38:00.035039Z",
     "shell.execute_reply.started": "2025-04-11T17:38:00.018202Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "import chardet\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pprint\n",
    "import re\n",
    "import gc\n",
    "\n",
    "from model_eval import execute_intent_code\n",
    "\n",
    "\n",
    "def cleanup_exec(outputs):\n",
    "    '''Cleans up memory after a notebook execution'''\n",
    "\n",
    "    # Deallocate the variables\n",
    "    for var_name in outputs.keys():\n",
    "        if var_name in globals():\n",
    "            del globals()[var_name]\n",
    "        elif var_name in locals():\n",
    "            del locals()[var_name]\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "def calc_total_input_sizes(csv_files):\n",
    "    \"\"\"\n",
    "    Calculate the total size, total rows, and total columns for a list of CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_files (list): List of paths to CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (total_size_bytes, total_rows, total_columns)\n",
    "    \"\"\"\n",
    "    # Initialize variables to track total size, row count, and column count\n",
    "    total_size_bytes = 0\n",
    "    total_rows = 0\n",
    "    total_columns = 0\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Get the file size in bytes\n",
    "            file_size = os.path.getsize(csv_file)\n",
    "            total_size_bytes += file_size\n",
    "\n",
    "            # Count the number of rows in the CSV file\n",
    "            with open(csv_file, 'r') as f:\n",
    "                row_count = sum(1 for _ in f) - 1  # Subtract 1 for the header row\n",
    "            total_rows += row_count\n",
    "\n",
    "            # Count the number of columns in the CSV file\n",
    "            with open(csv_file, 'r') as f:\n",
    "                first_line = f.readline()\n",
    "                column_count = len(first_line.split(','))\n",
    "            total_columns += column_count\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {csv_file}: {e}\")\n",
    "\n",
    "    return total_size_bytes, total_rows, total_columns\n",
    "\n",
    "def transform_existing_dataset(datasets_json, \n",
    "                          artifact_path='/kaggle/input/arcade-existing', \n",
    "                          n_rows=10, \n",
    "                          top_n_entries=None, \n",
    "                          specific_nb=None):\n",
    "    \"\"\"\n",
    "    Transforms the ARCADE dataset to the desired format by reading the initial input\n",
    "    and executing each intent one by one, processing only the top `top_n_entries` entries.\n",
    "    \"\"\"\n",
    "    # Load the JSON file\n",
    "    with open(datasets_json, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Limit to the top `top_n_entries` if specified\n",
    "    if top_n_entries is not None:\n",
    "        data = data[:top_n_entries]\n",
    "    \n",
    "    # Number of rows to extract\n",
    "    N = n_rows\n",
    "    \n",
    "    # Extract intent, code pairs, and execute each intent\n",
    "    rows = []\n",
    "    for entry in tqdm(data):\n",
    "        nb_name = entry.get(\"notebook_name\")\n",
    "        work_dir = entry.get(\"work_dir\")\n",
    "        print(\"Running: \", nb_name)\n",
    "\n",
    "        # Construct the dataset folder path\n",
    "        dataset_folder_path = os.path.join(artifact_path, work_dir)\n",
    "\n",
    "        # Find all CSV files in the folder\n",
    "        csv_files = glob(os.path.join(dataset_folder_path, \"*.csv\"))\n",
    "        count_csvs = len(csv_files)\n",
    "        total_size_bytes, total_rows, total_cols = calc_total_input_sizes(csv_files)\n",
    "       \n",
    "        # Print the total size and row count\n",
    "        print(f\"Number input files: {count_csvs}\")\n",
    "        print(f\"Total size: {total_size_bytes} bytes\")\n",
    "        print(f\"Total rows: {total_rows}\")\n",
    "        print(f\"Total cols: {total_cols}\")\n",
    "        print(\"Input files:\", csv_files)\n",
    "        \n",
    "        # First turn input are the imports and dataset load, so execute it first\n",
    "        nb_header = entry.get(\"turns\", [])[0][\"input\"]\n",
    "\n",
    "        # Prepend code to change the working directory\n",
    "        change_dir_code = f\"import os\\nos.chdir('{dataset_folder_path}')\\n\"\n",
    "        nb_header = change_dir_code + nb_header\n",
    "\n",
    "        print(\"Executing notebook header\")\n",
    "        exec_state = {\"pd\": pd}  # Initialize execution state with Pandas\n",
    "        outputs, exec_state = execute_intent_code(exec_state, nb_header)\n",
    "        \n",
    "        # Initialize the execution state with the output from the header execution\n",
    "        inputs = outputs \n",
    "\n",
    "        # Serialize the exec_state using pickle\n",
    "        # serialized_exec_state = pickle.dumps(exec_state)\n",
    "\n",
    "        # Mark intents with errors if any of the previous intents had errors\n",
    "        execute_error = False\n",
    "\n",
    "        # Check if header had errors\n",
    "        if \"error\" in outputs:\n",
    "            execute_error = True\n",
    "        \n",
    "        for i, turn in enumerate(entry.get(\"turns\", [])):\n",
    "            print(\"Executing intent:\", i)\n",
    "            intent = turn[\"turn\"][\"intent\"][\"value\"]\n",
    "            code = turn[\"turn\"][\"code\"][\"value\"]\n",
    "            \n",
    "            # Execute the code intent\n",
    "            outputs, exec_state = execute_intent_code(exec_state, code)\n",
    "\n",
    "            # Check if this intent had an error\n",
    "            if \"error\" in outputs:\n",
    "                execute_error = True\n",
    "            \n",
    "            # Append the results\n",
    "            rows.append({\n",
    "                \"nb_name\": nb_name,\n",
    "                \"work_dir\": work_dir,\n",
    "                'nb_setup_code': nb_header,\n",
    "                \"intent_number\": i,\n",
    "                \"intent\": intent,\n",
    "                \"code\": code,\n",
    "                #\"exec_state\": str(serialized_exec_state),\n",
    "                \"inputs\": str(inputs),  # Inputs for this intent\n",
    "                \"outputs\": str(outputs),  # Outputs from this intent\n",
    "                \"execute_error\": execute_error,\n",
    "                'error_msg': outputs.get('error', ''),\n",
    "                'num_intput_files': count_csvs,\n",
    "                'total_input_size': total_size_bytes,\n",
    "                'total_input_rows': total_rows,\n",
    "                'total_input_cols': total_cols\n",
    "            })\n",
    "\n",
    "            # Update inputs for the next intent\n",
    "            inputs = outputs\n",
    "\n",
    "        # Clean up memory for all notebook outputs\n",
    "        cleanup_exec(outputs)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T20:00:31.963038Z",
     "iopub.status.busy": "2025-04-02T20:00:31.962639Z",
     "iopub.status.idle": "2025-04-02T20:09:18.801773Z",
     "shell.execute_reply": "2025-04-02T20:09:18.800484Z",
     "shell.execute_reply.started": "2025-04-02T20:00:31.963004Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# import os\n",
    "# from glob import glob\n",
    "# import chardet\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pickle\n",
    "# import pprint\n",
    "# import re\n",
    "# import gc\n",
    "\n",
    "# def replace_csv_reads_with_dataframe(code, dataset_file_name, dataframe_name=\"first_n_rows\"):\n",
    "#     \"\"\"\n",
    "#     Detects and replaces instances of `pd.read_csv` in the provided code with a predefined DataFrame.\n",
    "\n",
    "#     Args:\n",
    "#         code (str): The code string to process.\n",
    "#         dataset_file_path (str): The path to the dataset file being replaced.\n",
    "#         dataframe_name (str): The name of the DataFrame to replace `pd.read_csv` calls with.\n",
    "\n",
    "#     Returns:\n",
    "#         str: The modified code with `pd.read_csv` calls replaced.\n",
    "#     \"\"\"\n",
    "#     # Step 1: Replace direct `pd.read_csv` calls with the file name\n",
    "#     # Match patterns like pd.read_csv('athlete_events.csv') or pd.read_csv(\"athlete_events.csv\")\n",
    "#     direct_read_csv_pattern = rf\"pd\\.read_csv\\(['\\\"]{re.escape(dataset_file_name)}['\\\"]\\)\"\n",
    "#     code = re.sub(direct_read_csv_pattern, dataframe_name, code)\n",
    "\n",
    "#     # Step 2: Replace `pd.read_csv` calls that use a variable\n",
    "#     # Match patterns like pd.read_csv(PATH)\n",
    "#     read_csv_variable_pattern = r\"pd\\.read_csv\\(\\s*\\w+\\s*\\)\"\n",
    "#     code = re.sub(read_csv_variable_pattern, dataframe_name, code)\n",
    "    \n",
    "#     return code\n",
    "\n",
    "# def execute_intent_code(exec_state, code, verbose=False):\n",
    "#     \"\"\"\n",
    "#     Executes the given code in the provided execution state.\n",
    "#     Returns the updated execution state and any outputs, capturing only primitive types, tuples, \n",
    "#     and DataFrames (DataFrames are stored in JSON format).\n",
    "\n",
    "#     exec_state: python exec namespace\n",
    "\n",
    "#     examples:\n",
    "\n",
    "#         for executing notebook header:\n",
    "        \n",
    "#             first_n_rows = pd.DataFrame(eval(eval(intents.iloc[0][INPUT_DATA_COL].replace('null', 'None'))['first_n_rows']))\n",
    "#             exec_state = {\"pd\": pd, \"first_n_rows\": first_n_rows}  # Initialize execution state\n",
    "#             try:\n",
    "#                 outputs, exec_state = execute_intent_code(exec_state, nb_header, verbose=False)\n",
    "#                 inputs = outputs  # Initialize inputs with the header execution outputs\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error executing notebook header for {nb_name}: {e}\")\n",
    "#                 continue  # Skip this notebook if the header fails\n",
    "\n",
    "#         for executing intent code (note exec_state would have been previously modified from previous intent code execution)\n",
    "#             # Execute original code\n",
    "#             try:\n",
    "#                 print(\"Executing original code...\")\n",
    "#                 original_outputs, exec_state = execute_intent_code(exec_state, actual_code, verbose=False)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error executing original code: {e}\")\n",
    "#                 original_outputs = {}\n",
    "        \n",
    "#     \"\"\"\n",
    "#     error_msg = None\n",
    "#     try:\n",
    "#         # Use a non-interactive backend for matplotlib to suppress plots\n",
    "#         plt.switch_backend('Agg')\n",
    "\n",
    "#         if verbose:\n",
    "#             print(\"IN STATE\")\n",
    "#             print(exec_state)\n",
    "#             print(\"CODE\")\n",
    "#             print(code)\n",
    "\n",
    "#         # Execute the code in the provided execution state\n",
    "#         exec(code, exec_state)\n",
    "\n",
    "#         # Clear any matplotlib figures created during execution\n",
    "#         plt.close('all')\n",
    "        \n",
    "#         # Capture the outputs (all variables in the execution state)\n",
    "#         outputs = {}\n",
    "#         for key, value in exec_state.items():\n",
    "#             if not key.startswith(\"__\"):\n",
    "#                 if isinstance(value, (int, float, str, bool, tuple)):\n",
    "#                     outputs[key] = value\n",
    "#                 elif isinstance(value, pd.DataFrame):\n",
    "#                     # Convert DataFrame to JSON format\n",
    "#                     outputs[key] = str(value.to_json(orient=\"records\"))\n",
    "\n",
    "#         if verbose:\n",
    "#             print(\"OUT STATE\")\n",
    "#             print(exec_state)\n",
    "#             print(\"OUTPUTS\")\n",
    "#             print(outputs)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(\"Error in executing code: \", e)\n",
    "#         outputs = {\"error\": str(e)}\n",
    "#         error_msg = str(e)\n",
    "    \n",
    "#     return outputs, exec_state, error_msg\n",
    "\n",
    "# def cleanup_exec(outputs):\n",
    "#     '''Cleans up memory after a notebook execution'''\n",
    "\n",
    "#     # Deallocate the variables\n",
    "#     for var_name in outputs.keys():\n",
    "#         if var_name in globals():\n",
    "#             del globals()[var_name]\n",
    "#         elif var_name in locals():\n",
    "#             del locals()[var_name]\n",
    "    \n",
    "#     # Force garbage collection\n",
    "#     gc.collect()\n",
    "\n",
    "# def transform_dataset(datasets_json, n_rows=10, top_n_entries=None, specific_nb=None):\n",
    "#     \"\"\"\n",
    "#     Transforms the ARCADE dataset to the desired format by reading the initial input\n",
    "#     and executing each intent one by one, processing only the top `top_n_entries` entries.\n",
    "#     \"\"\"\n",
    "#     # Load the JSON file\n",
    "#     with open(datasets_json, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     # Limit to the top `top_n_entries` if specified\n",
    "#     if top_n_entries is not None:\n",
    "#         data = data[:top_n_entries]\n",
    "    \n",
    "#     # Number of rows to extract\n",
    "#     N = n_rows\n",
    "    \n",
    "#     ARTIFACT_PATH = '/kaggle/working/arcade_nl2code/annotated_dataset/dataset/existing_tasks/artifacts'\n",
    "    \n",
    "#     # Extract intent, code pairs, and first N rows of the dataset, and execute each intent\n",
    "#     rows = []\n",
    "#     for entry in tqdm(data):\n",
    "#         nb_name = entry.get(\"notebook_name\")\n",
    "#         work_dir = entry.get(\"work_dir\")\n",
    "#         print(\"Running: \", nb_name)\n",
    "\n",
    "#         # Construct the dataset folder path\n",
    "#         dataset_folder_path = os.path.join(ARTIFACT_PATH, work_dir)\n",
    "\n",
    "#         # change to the folder containing the datasets for this notebook\n",
    "#         #os.chdir(dataset_folder_path)\n",
    "        \n",
    "#         # Find all CSV files in the folder\n",
    "#         csv_files = glob(os.path.join(dataset_folder_path, \"*.csv\"))\n",
    "        \n",
    "#         # Load the first CSV file if any exist\n",
    "#         if csv_files:\n",
    "#             dataset_file_path = csv_files[0]  # Use the first CSV file\n",
    "    \n",
    "#             # Detect the file encoding\n",
    "#             with open(dataset_file_path, \"rb\") as f:\n",
    "#                 result = chardet.detect(f.read())\n",
    "#                 encoding = result[\"encoding\"]\n",
    "    \n",
    "#             # Use the detected encoding\n",
    "#             dataset_df = pd.read_csv(dataset_file_path, encoding=encoding)\n",
    "#             first_n_rows = pd.DataFrame(dataset_df.head(N))  # Convert to DataFrame\n",
    "#         else:\n",
    "#             first_n_rows = None  # Handle missing dataset files\n",
    "        \n",
    "#         # First turn input are the imports and dataset load, so execute it first\n",
    "#         nb_header = entry.get(\"turns\", [])[0][\"input\"]\n",
    "\n",
    "#         # Replace CSV reads with the first_n_rows DataFrame\n",
    "#         if first_n_rows is not None:\n",
    "#             nb_header = replace_csv_reads_with_dataframe(nb_header, \n",
    "#                                                          os.path.basename(dataset_file_path), \n",
    "#                                                          dataframe_name=\"first_n_rows\")\n",
    "\n",
    "#         exec_state = {\"pd\": pd, \"first_n_rows\": first_n_rows}  # Add first_n_rows to exec_state\n",
    "#         outputs, exec_state, error_msg = execute_intent_code(exec_state, nb_header)\n",
    "        \n",
    "#         # Initialize the execution state with the output from the header execution\n",
    "#         inputs = outputs \n",
    "\n",
    "#         # Serialize the exec_state using pickle\n",
    "#         #serialized_exec_state = pickle.dumps(exec_state)\n",
    "\n",
    "#         # we mark an intents with erorr if any of the previous intents had errors\n",
    "#         execute_error = False\n",
    "\n",
    "#         # check if header had errors\n",
    "#         if \"error\" in outputs:\n",
    "#             execute_error = True\n",
    "        \n",
    "#         for i, turn in enumerate(entry.get(\"turns\", [])):\n",
    "#             intent = turn[\"turn\"][\"intent\"][\"value\"]\n",
    "#             code = turn[\"turn\"][\"code\"][\"value\"]\n",
    "            \n",
    "#             # Execute the code intent\n",
    "#             outputs, exec_state, error_msg = execute_intent_code(exec_state, code)\n",
    "\n",
    "#             # check if this intent had an error\n",
    "#             if \"error\" in outputs:\n",
    "#                 execute_error=True\n",
    "            \n",
    "#             # Append the results\n",
    "#             rows.append({\n",
    "#                 \"nb_name\": nb_name,\n",
    "#                 \"work_dir\": work_dir,\n",
    "#                 'nb_setup_code': nb_header,\n",
    "#                 \"intent_number\": i,\n",
    "#                 \"intent\": intent,\n",
    "#                 \"code\": code,\n",
    "#                 #\"exec_state\": str(serialized_exec_state),\n",
    "#                 \"inputs\": str(inputs),  # Inputs for this intent\n",
    "#                 \"outputs\": str(outputs),  # Outputs from this intent\n",
    "#                 \"execute_error\": execute_error,\n",
    "#                 'error_msg': error_msg\n",
    "#             })\n",
    "\n",
    "#             # Update inputs for the next intent\n",
    "#             inputs = outputs\n",
    "\n",
    "#         #clean up memory for all notebook ouputs\n",
    "#         cleanup_exec(outputs)\n",
    "\n",
    "#     # Create a DataFrame\n",
    "#     df = pd.DataFrame(rows)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# df = transform_dataset(\n",
    "#     '/kaggle/working/arcade_nl2code/annotated_dataset/dataset/existing_tasks/dataset.json',\n",
    "#     n_rows=10,\n",
    "#     top_n_entries=None,  # limit num notebooks to process\n",
    "#     #specific_nb=None #'dataset_athlete_events/notebook_1/annotated.ipynb'\n",
    "# )\n",
    "# # Display the DataFrame\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:38:45.973554Z",
     "iopub.status.busy": "2025-04-11T17:38:45.973178Z",
     "iopub.status.idle": "2025-04-11T17:40:06.257711Z",
     "shell.execute_reply": "2025-04-11T17:40:06.256450Z",
     "shell.execute_reply.started": "2025-04-11T17:38:45.973525Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform arcade new to our dataset format\n",
    "df = transform_existing_dataset(\n",
    "    datasets_json='/kaggle/working/arcade_nl2code/annotated_dataset/dataset/existing_tasks/dataset.json',\n",
    "    artifact_path='/kaggle/input/arcade-existing-v3',\n",
    "    n_rows=10,\n",
    "    top_n_entries=None  # Adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:43:27.923645Z",
     "iopub.status.busy": "2025-04-11T17:43:27.923192Z",
     "iopub.status.idle": "2025-04-11T17:43:27.940165Z",
     "shell.execute_reply": "2025-04-11T17:43:27.939092Z",
     "shell.execute_reply.started": "2025-04-11T17:43:27.923587Z"
    }
   },
   "outputs": [],
   "source": [
    "# an individual notebook\n",
    "df[df.nb_name == 'dataset_athlete_events/notebook_1/annotated.ipynb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:44:57.130343Z",
     "iopub.status.busy": "2025-04-11T17:44:57.129900Z",
     "iopub.status.idle": "2025-04-11T17:44:57.136363Z",
     "shell.execute_reply": "2025-04-11T17:44:57.135287Z",
     "shell.execute_reply.started": "2025-04-11T17:44:57.130312Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count rows with errors in 'inputs' or 'outputs' \n",
    "#df['execute_error'] = (df['inputs'].str.contains('error', case=False, na=False) | df['outputs'].str.contains('error', case=False, na=False))\n",
    "error_count = df['execute_error'].sum()\n",
    "print(f\"Number of intents with errors: {error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:44:59.847806Z",
     "iopub.status.busy": "2025-04-11T17:44:59.847344Z",
     "iopub.status.idle": "2025-04-11T17:44:59.872694Z",
     "shell.execute_reply": "2025-04-11T17:44:59.871728Z",
     "shell.execute_reply.started": "2025-04-11T17:44:59.847761Z"
    }
   },
   "outputs": [],
   "source": [
    "# top 10 notebooks with errors\n",
    "df[df['execute_error']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:47:13.315496Z",
     "iopub.status.busy": "2025-04-11T17:47:13.315134Z",
     "iopub.status.idle": "2025-04-11T17:47:13.327454Z",
     "shell.execute_reply": "2025-04-11T17:47:13.326359Z",
     "shell.execute_reply.started": "2025-04-11T17:47:13.315467Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of intents per notebook\n",
    "df.groupby('nb_name').intent_number.max().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:47:42.840512Z",
     "iopub.status.busy": "2025-04-11T17:47:42.840131Z",
     "iopub.status.idle": "2025-04-11T17:47:42.900399Z",
     "shell.execute_reply": "2025-04-11T17:47:42.899259Z",
     "shell.execute_reply.started": "2025-04-11T17:47:42.840482Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_to_pickle(df, file_path='arcade_existing_transformed.pkl'):\n",
    "    # Extract just what you need, with code as raw strings\n",
    "    extracted_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        entry = {\n",
    "            'nb_name': row['nb_name'],\n",
    "            'work_dir': row['work_dir'],\n",
    "            'intent_number': row['intent_number'],\n",
    "            'intent': row['intent'],\n",
    "            'code': row['code'],  # This preserves exact formatting\n",
    "            'nb_setup_code': row['nb_setup_code'],\n",
    "            'inputs': row['inputs'],\n",
    "            'outputs': row['outputs'],\n",
    "            'execute_error': row['execute_error']\n",
    "        }\n",
    "        extracted_data.append(entry)\n",
    "    \n",
    "    # Save using pickle to preserve exact string representation\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(extracted_data, f)\n",
    "    \n",
    "    print(\"Saved data with preserved formatting to 'arcade_existing_transformed.pkl'\")\n",
    "\n",
    "# Save the processed DataFrame to a pickle file\n",
    "save_to_pickle(df, f'/kaggle/working/arcade_existing_transformed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:48:09.689606Z",
     "iopub.status.busy": "2025-04-11T17:48:09.689230Z",
     "iopub.status.idle": "2025-04-11T17:48:09.709143Z",
     "shell.execute_reply": "2025-04-11T17:48:09.707810Z",
     "shell.execute_reply.started": "2025-04-11T17:48:09.689566Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_from_pkl(file_path):\n",
    "    # Load the pickled data (list of dictionaries)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        extracted_data = pickle.load(f)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df_loaded = pd.DataFrame(extracted_data)\n",
    "    \n",
    "    # Verify the data loaded correctly\n",
    "    print(f\"Loaded DataFrame with shape: {df_loaded.shape}\")\n",
    "    print(f\"Columns: {df_loaded.columns.tolist()}\")\n",
    "    \n",
    "    # Check a sample of the code to ensure formatting is preserved\n",
    "    if len(df_loaded) > 0:\n",
    "        print(\"\\nSample code from first row:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(df_loaded.iloc[0]['code'])\n",
    "        print(\"-\" * 50)\n",
    "    return df_loaded\n",
    "\n",
    "# test whether we can read back to dataframe\n",
    "df_loaded = load_from_pkl( f'/kaggle/working/arcade_existing_transformed.pkl')\n",
    "\n",
    "# Verify the data loaded correctly\n",
    "print(f\"Loaded DataFrame with shape: {df_loaded.shape}\")\n",
    "print(f\"Columns: {df_loaded.columns.tolist()}\")\n",
    "\n",
    "# Check a sample of the code to ensure formatting is preserved\n",
    "if len(df_loaded) > 0:\n",
    "    print(\"\\nSample code from first row:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(df_loaded.iloc[0]['code'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:48:47.258477Z",
     "iopub.status.busy": "2025-04-11T17:48:47.258045Z",
     "iopub.status.idle": "2025-04-11T17:48:47.268130Z",
     "shell.execute_reply": "2025-04-11T17:48:47.267065Z",
     "shell.execute_reply.started": "2025-04-11T17:48:47.258440Z"
    }
   },
   "outputs": [],
   "source": [
    "# check notebook with known error in header is marked with errors\n",
    "df_loaded[df_loaded.nb_name == 'dataset_chipotle/notebook_1/annotated.ipynb']['execute_error']"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7116834,
     "sourceId": 11369318,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 233237994,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
